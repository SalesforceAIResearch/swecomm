{"instance_id": "scikit-learn__scikit-learn-13497", "identified_spans": [{"file_path": "sklearn/feature_selection/mutual_info_.py", "span_ids": ["_estimate_mi"], "content": [{"span_id": "_estimate_mi", "start_line": 195, "end_line": 290, "content": ["def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n", "                 n_neighbors=3, copy=True, random_state=None):\n", "    \"\"\"Estimate mutual information between the features and the target.\n", "\n", "    Parameters\n", "    ----------\n", "    X : array_like or sparse matrix, shape (n_samples, n_features)\n", "        Feature matrix.\n", "\n", "    y : array_like, shape (n_samples,)\n", "        Target vector.\n", "\n", "    discrete_features : {'auto', bool, array_like}, default 'auto'\n", "        If bool, then determines whether to consider all features discrete\n", "        or continuous. If array, then it should be either a boolean mask\n", "        with shape (n_features,) or array with indices of discrete features.\n", "        If 'auto', it is assigned to False for dense `X` and to True for\n", "        sparse `X`.\n", "\n", "    discrete_target : bool, default False\n", "        Whether to consider `y` as a discrete variable.\n", "\n", "    n_neighbors : int, default 3\n", "        Number of neighbors to use for MI estimation for continuous variables,\n", "        see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n", "        could introduce a bias.\n", "\n", "    copy : bool, default True\n", "        Whether to make a copy of the given data. If set to False, the initial\n", "        data will be overwritten.\n", "\n", "    random_state : int, RandomState instance or None, optional, default None\n", "        The seed of the pseudo random number generator for adding small noise\n", "        to continuous variables in order to remove repeated values.  If int,\n", "        random_state is the seed used by the random number generator; If\n", "        RandomState instance, random_state is the random number generator; If\n", "        None, the random number generator is the RandomState instance used by\n", "        `np.random`.\n", "\n", "    Returns\n", "    -------\n", "    mi : ndarray, shape (n_features,)\n", "        Estimated mutual information between each feature and the target.\n", "        A negative value will be replaced by 0.\n", "\n", "    References\n", "    ----------\n", "    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n", "           information\". Phys. Rev. E 69, 2004.\n", "    .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n", "           Data Sets\". PLoS ONE 9(2), 2014.\n", "    \"\"\"\n", "    X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n", "    n_samples, n_features = X.shape\n", "\n", "    if discrete_features == 'auto':\n", "        discrete_features = issparse(X)\n", "\n", "    if isinstance(discrete_features, bool):\n", "        discrete_mask = np.empty(n_features, dtype=bool)\n", "        discrete_mask.fill(discrete_features)\n", "    else:\n", "        discrete_features = np.asarray(discrete_features)\n", "        if discrete_features.dtype != 'bool':\n", "            discrete_mask = np.zeros(n_features, dtype=bool)\n", "            discrete_mask[discrete_features] = True\n", "        else:\n", "            discrete_mask = discrete_features\n", "\n", "    continuous_mask = ~discrete_mask\n", "    if np.any(continuous_mask) and issparse(X):\n", "        raise ValueError(\"Sparse matrix `X` can't have continuous features.\")\n", "\n", "    rng = check_random_state(random_state)\n", "    if np.any(continuous_mask):\n", "        if copy:\n", "            X = X.copy()\n", "\n", "        if not discrete_target:\n", "            X[:, continuous_mask] = scale(X[:, continuous_mask],\n", "                                          with_mean=False, copy=False)\n", "\n", "        # Add small noise to continuous features as advised in Kraskov et. al.\n", "        X = X.astype(float, **_astype_copy_false(X))\n", "        means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))\n", "        X[:, continuous_mask] += 1e-10 * means * rng.randn(\n", "                n_samples, np.sum(continuous_mask))\n", "\n", "    if not discrete_target:\n", "        y = scale(y, with_mean=False)\n", "        y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)\n", "\n", "    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n", "          x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n", "\n", "    return np.array(mi)\n"]}]}]}