{"instance_id": "scikit-learn__scikit-learn-25500", "identified_spans": [{"file_path": "sklearn/_config.py", "span_ids": ["set_config"], "content": [{"span_id": "set_config", "start_line": 48, "end_line": 159, "content": ["def set_config(\n", "    assume_finite=None,\n", "    working_memory=None,\n", "    print_changed_only=None,\n", "    display=None,\n", "    pairwise_dist_chunk_size=None,\n", "    enable_cython_pairwise_dist=None,\n", "    array_api_dispatch=None,\n", "    transform_output=None,\n", "):\n", "    \"\"\"Set global scikit-learn configuration\n", "\n", "    .. versionadded:: 0.19\n", "\n", "    Parameters\n", "    ----------\n", "    assume_finite : bool, default=None\n", "        If True, validation for finiteness will be skipped,\n", "        saving time, but leading to potential crashes. If\n", "        False, validation for finiteness will be performed,\n", "        avoiding error.  Global default: False.\n", "\n", "        .. versionadded:: 0.19\n", "\n", "    working_memory : int, default=None\n", "        If set, scikit-learn will attempt to limit the size of temporary arrays\n", "        to this number of MiB (per job when parallelised), often saving both\n", "        computation time and memory on expensive operations that can be\n", "        performed in chunks. Global default: 1024.\n", "\n", "        .. versionadded:: 0.20\n", "\n", "    print_changed_only : bool, default=None\n", "        If True, only the parameters that were set to non-default\n", "        values will be printed when printing an estimator. For example,\n", "        ``print(SVC())`` while True will only print 'SVC()' while the default\n", "        behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n", "        all the non-changed parameters.\n", "\n", "        .. versionadded:: 0.21\n", "\n", "    display : {'text', 'diagram'}, default=None\n", "        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n", "        lab or notebook context. If 'text', estimators will be displayed as\n", "        text. Default is 'diagram'.\n", "\n", "        .. versionadded:: 0.23\n", "\n", "    pairwise_dist_chunk_size : int, default=None\n", "        The number of row vectors per chunk for the accelerated pairwise-\n", "        distances reduction backend. Default is 256 (suitable for most of\n", "        modern laptops' caches and architectures).\n", "\n", "        Intended for easier benchmarking and testing of scikit-learn internals.\n", "        End users are not expected to benefit from customizing this configuration\n", "        setting.\n", "\n", "        .. versionadded:: 1.1\n", "\n", "    enable_cython_pairwise_dist : bool, default=None\n", "        Use the accelerated pairwise-distances reduction backend when\n", "        possible. Global default: True.\n", "\n", "        Intended for easier benchmarking and testing of scikit-learn internals.\n", "        End users are not expected to benefit from customizing this configuration\n", "        setting.\n", "\n", "        .. versionadded:: 1.1\n", "\n", "    array_api_dispatch : bool, default=None\n", "        Use Array API dispatching when inputs follow the Array API standard.\n", "        Default is False.\n", "\n", "        See the :ref:`User Guide <array_api>` for more details.\n", "\n", "        .. versionadded:: 1.2\n", "\n", "    transform_output : str, default=None\n", "        Configure output of `transform` and `fit_transform`.\n", "\n", "        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n", "        for an example on how to use the API.\n", "\n", "        - `\"default\"`: Default output format of a transformer\n", "        - `\"pandas\"`: DataFrame output\n", "        - `None`: Transform configuration is unchanged\n", "\n", "        .. versionadded:: 1.2\n", "\n", "    See Also\n", "    --------\n", "    config_context : Context manager for global scikit-learn configuration.\n", "    get_config : Retrieve current values of the global configuration.\n", "    \"\"\"\n", "    local_config = _get_threadlocal_config()\n", "\n", "    if assume_finite is not None:\n", "        local_config[\"assume_finite\"] = assume_finite\n", "    if working_memory is not None:\n", "        local_config[\"working_memory\"] = working_memory\n", "    if print_changed_only is not None:\n", "        local_config[\"print_changed_only\"] = print_changed_only\n", "    if display is not None:\n", "        local_config[\"display\"] = display\n", "    if pairwise_dist_chunk_size is not None:\n", "        local_config[\"pairwise_dist_chunk_size\"] = pairwise_dist_chunk_size\n", "    if enable_cython_pairwise_dist is not None:\n", "        local_config[\"enable_cython_pairwise_dist\"] = enable_cython_pairwise_dist\n", "    if array_api_dispatch is not None:\n", "        local_config[\"array_api_dispatch\"] = array_api_dispatch\n", "    if transform_output is not None:\n", "        local_config[\"transform_output\"] = transform_output\n"]}]}, {"file_path": "sklearn/calibration.py", "span_ids": ["CalibratedClassifierCV", "_CalibratedClassifier.predict_proba", "_CalibratedClassifier"], "content": [{"span_id": "CalibratedClassifierCV", "start_line": 55, "end_line": 262, "content": ["class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n", "    \"\"\"Probability calibration with isotonic regression or logistic regression.\n", "\n", "    This class uses cross-validation to both estimate the parameters of a\n", "    classifier and subsequently calibrate a classifier. With default\n", "    `ensemble=True`, for each cv split it\n", "    fits a copy of the base estimator to the training subset, and calibrates it\n", "    using the testing subset. For prediction, predicted probabilities are\n", "    averaged across these individual calibrated classifiers. When\n", "    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n", "    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n", "    used for calibration. For prediction, the base estimator, trained using all\n", "    the data, is used. This is the method implemented when `probabilities=True`\n", "    for :mod:`sklearn.svm` estimators.\n", "\n", "    Already fitted classifiers can be calibrated via the parameter\n", "    `cv=\"prefit\"`. In this case, no cross-validation is used and all provided\n", "    data is used for calibration. The user has to take care manually that data\n", "    for model fitting and calibration are disjoint.\n", "\n", "    The calibration is based on the :term:`decision_function` method of the\n", "    `estimator` if it exists, else on :term:`predict_proba`.\n", "\n", "    Read more in the :ref:`User Guide <calibration>`.\n", "\n", "    Parameters\n", "    ----------\n", "    estimator : estimator instance, default=None\n", "        The classifier whose output need to be calibrated to provide more\n", "        accurate `predict_proba` outputs. The default classifier is\n", "        a :class:`~sklearn.svm.LinearSVC`.\n", "\n", "        .. versionadded:: 1.2\n", "\n", "    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n", "        The method to use for calibration. Can be 'sigmoid' which\n", "        corresponds to Platt's method (i.e. a logistic regression model) or\n", "        'isotonic' which is a non-parametric approach. It is not advised to\n", "        use isotonic calibration with too few calibration samples\n", "        ``(<<1000)`` since it tends to overfit.\n", "\n", "    cv : int, cross-validation generator, iterable or \"prefit\", \\\n", "            default=None\n", "        Determines the cross-validation splitting strategy.\n", "        Possible inputs for cv are:\n", "\n", "        - None, to use the default 5-fold cross-validation,\n", "        - integer, to specify the number of folds.\n", "        - :term:`CV splitter`,\n", "        - An iterable yielding (train, test) splits as arrays of indices.\n", "\n", "        For integer/None inputs, if ``y`` is binary or multiclass,\n", "        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n", "        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n", "        is used.\n", "\n", "        Refer to the :ref:`User Guide <cross_validation>` for the various\n", "        cross-validation strategies that can be used here.\n", "\n", "        If \"prefit\" is passed, it is assumed that `estimator` has been\n", "        fitted already and all data is used for calibration.\n", "\n", "        .. versionchanged:: 0.22\n", "            ``cv`` default value if None changed from 3-fold to 5-fold.\n", "\n", "    n_jobs : int, default=None\n", "        Number of jobs to run in parallel.\n", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n", "        ``-1`` means using all processors.\n", "\n", "        Base estimator clones are fitted in parallel across cross-validation\n", "        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n", "\n", "        See :term:`Glossary <n_jobs>` for more details.\n", "\n", "        .. versionadded:: 0.24\n", "\n", "    ensemble : bool, default=True\n", "        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n", "        Ignored if `cv='prefit'`.\n", "\n", "        If `True`, the `estimator` is fitted using training data, and\n", "        calibrated using testing data, for each `cv` fold. The final estimator\n", "        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n", "        `n_cv` is the number of cross-validation folds. The output is the\n", "        average predicted probabilities of all pairs.\n", "\n", "        If `False`, `cv` is used to compute unbiased predictions, via\n", "        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n", "        used for calibration. At prediction time, the classifier used is the\n", "        `estimator` trained on all the data.\n", "        Note that this method is also internally implemented  in\n", "        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n", "\n", "        .. versionadded:: 0.24\n", "\n", "    base_estimator : estimator instance\n", "        This parameter is deprecated. Use `estimator` instead.\n", "\n", "        .. deprecated:: 1.2\n", "           The parameter `base_estimator` is deprecated in 1.2 and will be\n", "           removed in 1.4. Use `estimator` instead.\n", "\n", "    Attributes\n", "    ----------\n", "    classes_ : ndarray of shape (n_classes,)\n", "        The class labels.\n", "\n", "    n_features_in_ : int\n", "        Number of features seen during :term:`fit`. Only defined if the\n", "        underlying estimator exposes such an attribute when fit.\n", "\n", "        .. versionadded:: 0.24\n", "\n", "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n", "        Names of features seen during :term:`fit`. Only defined if the\n", "        underlying estimator exposes such an attribute when fit.\n", "\n", "        .. versionadded:: 1.0\n", "\n", "    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"` \\\n", "            or `ensemble=False`)\n", "        The list of classifier and calibrator pairs.\n", "\n", "        - When `cv=\"prefit\"`, the fitted `estimator` and fitted\n", "          calibrator.\n", "        - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n", "          `estimator` and calibrator pairs. `n_cv` is the number of\n", "          cross-validation folds.\n", "        - When `cv` is not \"prefit\" and `ensemble=False`, the `estimator`,\n", "          fitted on all the data, and fitted calibrator.\n", "\n", "        .. versionchanged:: 0.24\n", "            Single calibrated classifier case when `ensemble=False`.\n", "\n", "    See Also\n", "    --------\n", "    calibration_curve : Compute true and predicted probabilities\n", "        for a calibration curve.\n", "\n", "    References\n", "    ----------\n", "    .. [1] Obtaining calibrated probability estimates from decision trees\n", "           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n", "\n", "    .. [2] Transforming Classifier Scores into Accurate Multiclass\n", "           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n", "\n", "    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n", "           Regularized Likelihood Methods, J. Platt, (1999)\n", "\n", "    .. [4] Predicting Good Probabilities with Supervised Learning,\n", "           A. Niculescu-Mizil & R. Caruana, ICML 2005\n", "\n", "    Examples\n", "    --------\n", "    >>> from sklearn.datasets import make_classification\n", "    >>> from sklearn.naive_bayes import GaussianNB\n", "    >>> from sklearn.calibration import CalibratedClassifierCV\n", "    >>> X, y = make_classification(n_samples=100, n_features=2,\n", "    ...                            n_redundant=0, random_state=42)\n", "    >>> base_clf = GaussianNB()\n", "    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n", "    >>> calibrated_clf.fit(X, y)\n", "    CalibratedClassifierCV(...)\n", "    >>> len(calibrated_clf.calibrated_classifiers_)\n", "    3\n", "    >>> calibrated_clf.predict_proba(X)[:5, :]\n", "    array([[0.110..., 0.889...],\n", "           [0.072..., 0.927...],\n", "           [0.928..., 0.071...],\n", "           [0.928..., 0.071...],\n", "           [0.071..., 0.928...]])\n", "    >>> from sklearn.model_selection import train_test_split\n", "    >>> X, y = make_classification(n_samples=100, n_features=2,\n", "    ...                            n_redundant=0, random_state=42)\n", "    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n", "    ...        X, y, random_state=42\n", "    ... )\n", "    >>> base_clf = GaussianNB()\n", "    >>> base_clf.fit(X_train, y_train)\n", "    GaussianNB()\n", "    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n", "    >>> calibrated_clf.fit(X_calib, y_calib)\n", "    CalibratedClassifierCV(...)\n", "    >>> len(calibrated_clf.calibrated_classifiers_)\n", "    1\n", "    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n", "    array([[0.936..., 0.063...]])\n", "    \"\"\"\n", "\n", "    _parameter_constraints: dict = {\n", "        \"estimator\": [\n", "            HasMethods([\"fit\", \"predict_proba\"]),\n", "            HasMethods([\"fit\", \"decision_function\"]),\n", "            None,\n", "        ],\n", "        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n", "        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n", "        \"n_jobs\": [Integral, None],\n", "        \"ensemble\": [\"boolean\"],\n", "        \"base_estimator\": [\n", "            HasMethods([\"fit\", \"predict_proba\"]),\n", "            HasMethods([\"fit\", \"decision_function\"]),\n", "            None,\n", "            Hidden(StrOptions({\"deprecated\"})),\n", "        ],\n", "    }\n"]}, {"span_id": "_CalibratedClassifier.predict_proba", "start_line": 732, "end_line": 781, "content": ["    def predict_proba(self, X):\n", "        \"\"\"Calculate calibrated probabilities.\n", "\n", "        Calculates classification calibrated probabilities\n", "        for each class, in a one-vs-all manner, for `X`.\n", "\n", "        Parameters\n", "        ----------\n", "        X : ndarray of shape (n_samples, n_features)\n", "            The sample data.\n", "\n", "        Returns\n", "        -------\n", "        proba : array, shape (n_samples, n_classes)\n", "            The predicted probabilities. Can be exact zeros.\n", "        \"\"\"\n", "        n_classes = len(self.classes)\n", "        pred_method, method_name = _get_prediction_method(self.estimator)\n", "        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n", "\n", "        label_encoder = LabelEncoder().fit(self.classes)\n", "        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n", "\n", "        proba = np.zeros((_num_samples(X), n_classes))\n", "        for class_idx, this_pred, calibrator in zip(\n", "            pos_class_indices, predictions.T, self.calibrators\n", "        ):\n", "            if n_classes == 2:\n", "                # When binary, `predictions` consists only of predictions for\n", "                # clf.classes_[1] but `pos_class_indices` = 0\n", "                class_idx += 1\n", "            proba[:, class_idx] = calibrator.predict(this_pred)\n", "\n", "        # Normalize the probabilities\n", "        if n_classes == 2:\n", "            proba[:, 0] = 1.0 - proba[:, 1]\n", "        else:\n", "            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n", "            # In the edge case where for each class calibrator returns a null\n", "            # probability for a given sample, use the uniform distribution\n", "            # instead.\n", "            uniform_proba = np.full_like(proba, 1 / n_classes)\n", "            proba = np.divide(\n", "                proba, denominator, out=uniform_proba, where=denominator != 0\n", "            )\n", "\n", "        # Deal with cases where the predicted probability minimally exceeds 1.0\n", "        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n", "\n", "        return proba\n"]}, {"span_id": "_CalibratedClassifier", "start_line": 703, "end_line": 724, "content": ["class _CalibratedClassifier:\n", "    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n", "\n", "    Parameters\n", "    ----------\n", "    estimator : estimator instance\n", "        Fitted classifier.\n", "\n", "    calibrators : list of fitted estimator instances\n", "        List of fitted calibrators (either 'IsotonicRegression' or\n", "        '_SigmoidCalibration'). The number of calibrators equals the number of\n", "        classes. However, if there are 2 classes, the list contains only one\n", "        fitted calibrator.\n", "\n", "    classes : array-like of shape (n_classes,)\n", "        All the prediction classes.\n", "\n", "    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n", "        The method to use for calibration. Can be 'sigmoid' which\n", "        corresponds to Platt's method or 'isotonic' which is a\n", "        non-parametric approach based on isotonic regression.\n", "    \"\"\"\n"]}]}, {"file_path": "sklearn/isotonic.py", "span_ids": ["IsotonicRegression", "IsotonicRegression.transform"], "content": [{"span_id": "IsotonicRegression", "start_line": 137, "end_line": 236, "content": ["class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n", "    \"\"\"Isotonic regression model.\n", "\n", "    Read more in the :ref:`User Guide <isotonic>`.\n", "\n", "    .. versionadded:: 0.13\n", "\n", "    Parameters\n", "    ----------\n", "    y_min : float, default=None\n", "        Lower bound on the lowest predicted value (the minimum value may\n", "        still be higher). If not set, defaults to -inf.\n", "\n", "    y_max : float, default=None\n", "        Upper bound on the highest predicted value (the maximum may still be\n", "        lower). If not set, defaults to +inf.\n", "\n", "    increasing : bool or 'auto', default=True\n", "        Determines whether the predictions should be constrained to increase\n", "        or decrease with `X`. 'auto' will decide based on the Spearman\n", "        correlation estimate's sign.\n", "\n", "    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n", "        Handles how `X` values outside of the training domain are handled\n", "        during prediction.\n", "\n", "        - 'nan', predictions will be NaN.\n", "        - 'clip', predictions will be set to the value corresponding to\n", "          the nearest train interval endpoint.\n", "        - 'raise', a `ValueError` is raised.\n", "\n", "    Attributes\n", "    ----------\n", "    X_min_ : float\n", "        Minimum value of input array `X_` for left bound.\n", "\n", "    X_max_ : float\n", "        Maximum value of input array `X_` for right bound.\n", "\n", "    X_thresholds_ : ndarray of shape (n_thresholds,)\n", "        Unique ascending `X` values used to interpolate\n", "        the y = f(X) monotonic function.\n", "\n", "        .. versionadded:: 0.24\n", "\n", "    y_thresholds_ : ndarray of shape (n_thresholds,)\n", "        De-duplicated `y` values suitable to interpolate the y = f(X)\n", "        monotonic function.\n", "\n", "        .. versionadded:: 0.24\n", "\n", "    f_ : function\n", "        The stepwise interpolating function that covers the input domain ``X``.\n", "\n", "    increasing_ : bool\n", "        Inferred value for ``increasing``.\n", "\n", "    See Also\n", "    --------\n", "    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n", "        Regression.\n", "    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n", "        is a non-parametric model accepting monotonicity constraints.\n", "    isotonic_regression : Function to solve the isotonic regression model.\n", "\n", "    Notes\n", "    -----\n", "    Ties are broken using the secondary method from de Leeuw, 1977.\n", "\n", "    References\n", "    ----------\n", "    Isotonic Median Regression: A Linear Programming Approach\n", "    Nilotpal Chakravarti\n", "    Mathematics of Operations Research\n", "    Vol. 14, No. 2 (May, 1989), pp. 303-308\n", "\n", "    Isotone Optimization in R : Pool-Adjacent-Violators\n", "    Algorithm (PAVA) and Active Set Methods\n", "    de Leeuw, Hornik, Mair\n", "    Journal of Statistical Software 2009\n", "\n", "    Correctness of Kruskal's algorithms for monotone regression with ties\n", "    de Leeuw, Psychometrica, 1977\n", "\n", "    Examples\n", "    --------\n", "    >>> from sklearn.datasets import make_regression\n", "    >>> from sklearn.isotonic import IsotonicRegression\n", "    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n", "    >>> iso_reg = IsotonicRegression().fit(X, y)\n", "    >>> iso_reg.predict([.1, .2])\n", "    array([1.8628..., 3.7256...])\n", "    \"\"\"\n", "\n", "    _parameter_constraints: dict = {\n", "        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n", "        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n", "        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n", "        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n", "    }\n"]}, {"span_id": "IsotonicRegression.transform", "start_line": 363, "end_line": 398, "content": ["    def transform(self, T):\n", "        \"\"\"Transform new data by linear interpolation.\n", "\n", "        Parameters\n", "        ----------\n", "        T : array-like of shape (n_samples,) or (n_samples, 1)\n", "            Data to transform.\n", "\n", "            .. versionchanged:: 0.24\n", "               Also accepts 2d array with 1 feature.\n", "\n", "        Returns\n", "        -------\n", "        y_pred : ndarray of shape (n_samples,)\n", "            The transformed data.\n", "        \"\"\"\n", "\n", "        if hasattr(self, \"X_thresholds_\"):\n", "            dtype = self.X_thresholds_.dtype\n", "        else:\n", "            dtype = np.float64\n", "\n", "        T = check_array(T, dtype=dtype, ensure_2d=False)\n", "\n", "        self._check_input_data_shape(T)\n", "        T = T.reshape(-1)  # use 1d view\n", "\n", "        if self.out_of_bounds == \"clip\":\n", "            T = np.clip(T, self.X_min_, self.X_max_)\n", "\n", "        res = self.f_(T)\n", "\n", "        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n", "        res = res.astype(T.dtype)\n", "\n", "        return res\n"]}]}]}