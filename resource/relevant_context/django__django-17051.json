{"instance_id": "django__django-17051", "identified_spans": [{"file_path": "django/db/backends/postgresql/operations.py", "span_ids": ["DatabaseOperations.on_conflict_suffix_sql"], "content": [{"span_id": "DatabaseOperations.on_conflict_suffix_sql", "start_line": 398, "end_line": 416, "content": ["    def on_conflict_suffix_sql(self, fields, on_conflict, update_fields, unique_fields):\n", "        if on_conflict == OnConflict.IGNORE:\n", "            return \"ON CONFLICT DO NOTHING\"\n", "        if on_conflict == OnConflict.UPDATE:\n", "            return \"ON CONFLICT(%s) DO UPDATE SET %s\" % (\n", "                \", \".join(map(self.quote_name, unique_fields)),\n", "                \", \".join(\n", "                    [\n", "                        f\"{field} = EXCLUDED.{field}\"\n", "                        for field in map(self.quote_name, update_fields)\n", "                    ]\n", "                ),\n", "            )\n", "        return super().on_conflict_suffix_sql(\n", "            fields,\n", "            on_conflict,\n", "            update_fields,\n", "            unique_fields,\n", "        )\n"]}]}, {"file_path": "django/db/models/query.py", "span_ids": ["QuerySet.bulk_create", "QuerySet._batched_insert", "QuerySet._insert"], "content": [{"span_id": "QuerySet.bulk_create", "start_line": 726, "end_line": 824, "content": ["    def bulk_create(\n", "        self,\n", "        objs,\n", "        batch_size=None,\n", "        ignore_conflicts=False,\n", "        update_conflicts=False,\n", "        update_fields=None,\n", "        unique_fields=None,\n", "    ):\n", "        \"\"\"\n", "        Insert each of the instances into the database. Do *not* call\n", "        save() on each of the instances, do not send any pre/post_save\n", "        signals, and do not set the primary key attribute if it is an\n", "        autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n", "        Multi-table models are not supported.\n", "        \"\"\"\n", "        # When you bulk insert you don't get the primary keys back (if it's an\n", "        # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n", "        # you can't insert into the child tables which references this. There\n", "        # are two workarounds:\n", "        # 1) This could be implemented if you didn't have an autoincrement pk\n", "        # 2) You could do it by doing O(n) normal inserts into the parent\n", "        #    tables to get the primary keys back and then doing a single bulk\n", "        #    insert into the childmost table.\n", "        # We currently set the primary keys on the objects when using\n", "        # PostgreSQL via the RETURNING ID clause. It should be possible for\n", "        # Oracle as well, but the semantics for extracting the primary keys is\n", "        # trickier so it's not done yet.\n", "        if batch_size is not None and batch_size <= 0:\n", "            raise ValueError(\"Batch size must be a positive integer.\")\n", "        # Check that the parents share the same concrete model with the our\n", "        # model to detect the inheritance pattern ConcreteGrandParent ->\n", "        # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n", "        # would not identify that case as involving multiple tables.\n", "        for parent in self.model._meta.get_parent_list():\n", "            if parent._meta.concrete_model is not self.model._meta.concrete_model:\n", "                raise ValueError(\"Can't bulk create a multi-table inherited model\")\n", "        if not objs:\n", "            return objs\n", "        opts = self.model._meta\n", "        if unique_fields:\n", "            # Primary key is allowed in unique_fields.\n", "            unique_fields = [\n", "                self.model._meta.get_field(opts.pk.name if name == \"pk\" else name)\n", "                for name in unique_fields\n", "            ]\n", "        if update_fields:\n", "            update_fields = [self.model._meta.get_field(name) for name in update_fields]\n", "        on_conflict = self._check_bulk_create_options(\n", "            ignore_conflicts,\n", "            update_conflicts,\n", "            update_fields,\n", "            unique_fields,\n", "        )\n", "        self._for_write = True\n", "        fields = opts.concrete_fields\n", "        objs = list(objs)\n", "        self._prepare_for_bulk_create(objs)\n", "        with transaction.atomic(using=self.db, savepoint=False):\n", "            objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n", "            if objs_with_pk:\n", "                returned_columns = self._batched_insert(\n", "                    objs_with_pk,\n", "                    fields,\n", "                    batch_size,\n", "                    on_conflict=on_conflict,\n", "                    update_fields=update_fields,\n", "                    unique_fields=unique_fields,\n", "                )\n", "                for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n", "                    for result, field in zip(results, opts.db_returning_fields):\n", "                        if field != opts.pk:\n", "                            setattr(obj_with_pk, field.attname, result)\n", "                for obj_with_pk in objs_with_pk:\n", "                    obj_with_pk._state.adding = False\n", "                    obj_with_pk._state.db = self.db\n", "            if objs_without_pk:\n", "                fields = [f for f in fields if not isinstance(f, AutoField)]\n", "                returned_columns = self._batched_insert(\n", "                    objs_without_pk,\n", "                    fields,\n", "                    batch_size,\n", "                    on_conflict=on_conflict,\n", "                    update_fields=update_fields,\n", "                    unique_fields=unique_fields,\n", "                )\n", "                connection = connections[self.db]\n", "                if (\n", "                    connection.features.can_return_rows_from_bulk_insert\n", "                    and on_conflict is None\n", "                ):\n", "                    assert len(returned_columns) == len(objs_without_pk)\n", "                for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n", "                    for result, field in zip(results, opts.db_returning_fields):\n", "                        setattr(obj_without_pk, field.attname, result)\n", "                    obj_without_pk._state.adding = False\n", "                    obj_without_pk._state.db = self.db\n", "\n", "        return objs\n"]}, {"span_id": "QuerySet._batched_insert", "start_line": 1821, "end_line": 1858, "content": ["    def _batched_insert(\n", "        self,\n", "        objs,\n", "        fields,\n", "        batch_size,\n", "        on_conflict=None,\n", "        update_fields=None,\n", "        unique_fields=None,\n", "    ):\n", "        \"\"\"\n", "        Helper method for bulk_create() to insert objs one batch at a time.\n", "        \"\"\"\n", "        connection = connections[self.db]\n", "        ops = connection.ops\n", "        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n", "        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n", "        inserted_rows = []\n", "        bulk_return = connection.features.can_return_rows_from_bulk_insert\n", "        for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:\n", "            if bulk_return and on_conflict is None:\n", "                inserted_rows.extend(\n", "                    self._insert(\n", "                        item,\n", "                        fields=fields,\n", "                        using=self.db,\n", "                        returning_fields=self.model._meta.db_returning_fields,\n", "                    )\n", "                )\n", "            else:\n", "                self._insert(\n", "                    item,\n", "                    fields=fields,\n", "                    using=self.db,\n", "                    on_conflict=on_conflict,\n", "                    update_fields=update_fields,\n", "                    unique_fields=unique_fields,\n", "                )\n", "        return inserted_rows\n"]}, {"span_id": "QuerySet._insert", "start_line": 1791, "end_line": 1816, "content": ["    def _insert(\n", "        self,\n", "        objs,\n", "        fields,\n", "        returning_fields=None,\n", "        raw=False,\n", "        using=None,\n", "        on_conflict=None,\n", "        update_fields=None,\n", "        unique_fields=None,\n", "    ):\n", "        \"\"\"\n", "        Insert a new record for the given model. This provides an interface to\n", "        the InsertQuery class and is how Model.save() is implemented.\n", "        \"\"\"\n", "        self._for_write = True\n", "        if using is None:\n", "            using = self.db\n", "        query = sql.InsertQuery(\n", "            self.model,\n", "            on_conflict=on_conflict,\n", "            update_fields=update_fields,\n", "            unique_fields=unique_fields,\n", "        )\n", "        query.insert_values(fields, objs, raw=raw)\n", "        return query.get_compiler(using=using).execute_sql(returning_fields)\n"]}]}, {"file_path": "django/db/models/sql/compiler.py", "span_ids": ["SQLInsertCompiler.as_sql", "SQLInsertCompiler.execute_sql"], "content": [{"span_id": "SQLInsertCompiler.as_sql", "start_line": 1732, "end_line": 1810, "content": ["    def as_sql(self):\n", "        # We don't need quote_name_unless_alias() here, since these are all\n", "        # going to be column names (so we can avoid the extra overhead).\n", "        qn = self.connection.ops.quote_name\n", "        opts = self.query.get_meta()\n", "        insert_statement = self.connection.ops.insert_statement(\n", "            on_conflict=self.query.on_conflict,\n", "        )\n", "        result = [\"%s %s\" % (insert_statement, qn(opts.db_table))]\n", "        fields = self.query.fields or [opts.pk]\n", "        result.append(\"(%s)\" % \", \".join(qn(f.column) for f in fields))\n", "\n", "        if self.query.fields:\n", "            value_rows = [\n", "                [\n", "                    self.prepare_value(field, self.pre_save_val(field, obj))\n", "                    for field in fields\n", "                ]\n", "                for obj in self.query.objs\n", "            ]\n", "        else:\n", "            # An empty object.\n", "            value_rows = [\n", "                [self.connection.ops.pk_default_value()] for _ in self.query.objs\n", "            ]\n", "            fields = [None]\n", "\n", "        # Currently the backends just accept values when generating bulk\n", "        # queries and generate their own placeholders. Doing that isn't\n", "        # necessary and it should be possible to use placeholders and\n", "        # expressions in bulk inserts too.\n", "        can_bulk = (\n", "            not self.returning_fields and self.connection.features.has_bulk_insert\n", "        )\n", "\n", "        placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n", "\n", "        on_conflict_suffix_sql = self.connection.ops.on_conflict_suffix_sql(\n", "            fields,\n", "            self.query.on_conflict,\n", "            (f.column for f in self.query.update_fields),\n", "            (f.column for f in self.query.unique_fields),\n", "        )\n", "        if (\n", "            self.returning_fields\n", "            and self.connection.features.can_return_columns_from_insert\n", "        ):\n", "            if self.connection.features.can_return_rows_from_bulk_insert:\n", "                result.append(\n", "                    self.connection.ops.bulk_insert_sql(fields, placeholder_rows)\n", "                )\n", "                params = param_rows\n", "            else:\n", "                result.append(\"VALUES (%s)\" % \", \".join(placeholder_rows[0]))\n", "                params = [param_rows[0]]\n", "            if on_conflict_suffix_sql:\n", "                result.append(on_conflict_suffix_sql)\n", "            # Skip empty r_sql to allow subclasses to customize behavior for\n", "            # 3rd party backends. Refs #19096.\n", "            r_sql, self.returning_params = self.connection.ops.return_insert_columns(\n", "                self.returning_fields\n", "            )\n", "            if r_sql:\n", "                result.append(r_sql)\n", "                params += [self.returning_params]\n", "            return [(\" \".join(result), tuple(chain.from_iterable(params)))]\n", "\n", "        if can_bulk:\n", "            result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n", "            if on_conflict_suffix_sql:\n", "                result.append(on_conflict_suffix_sql)\n", "            return [(\" \".join(result), tuple(p for ps in param_rows for p in ps))]\n", "        else:\n", "            if on_conflict_suffix_sql:\n", "                result.append(on_conflict_suffix_sql)\n", "            return [\n", "                (\" \".join(result + [\"VALUES (%s)\" % \", \".join(p)]), vals)\n", "                for p, vals in zip(placeholder_rows, param_rows)\n", "            ]\n"]}, {"span_id": "SQLInsertCompiler.execute_sql", "start_line": 1812, "end_line": 1852, "content": ["    def execute_sql(self, returning_fields=None):\n", "        assert not (\n", "            returning_fields\n", "            and len(self.query.objs) != 1\n", "            and not self.connection.features.can_return_rows_from_bulk_insert\n", "        )\n", "        opts = self.query.get_meta()\n", "        self.returning_fields = returning_fields\n", "        with self.connection.cursor() as cursor:\n", "            for sql, params in self.as_sql():\n", "                cursor.execute(sql, params)\n", "            if not self.returning_fields:\n", "                return []\n", "            if (\n", "                self.connection.features.can_return_rows_from_bulk_insert\n", "                and len(self.query.objs) > 1\n", "            ):\n", "                rows = self.connection.ops.fetch_returned_insert_rows(cursor)\n", "            elif self.connection.features.can_return_columns_from_insert:\n", "                assert len(self.query.objs) == 1\n", "                rows = [\n", "                    self.connection.ops.fetch_returned_insert_columns(\n", "                        cursor,\n", "                        self.returning_params,\n", "                    )\n", "                ]\n", "            else:\n", "                rows = [\n", "                    (\n", "                        self.connection.ops.last_insert_id(\n", "                            cursor,\n", "                            opts.db_table,\n", "                            opts.pk.column,\n", "                        ),\n", "                    )\n", "                ]\n", "        cols = [field.get_col(opts.db_table) for field in self.returning_fields]\n", "        converters = self.get_converters(cols)\n", "        if converters:\n", "            rows = list(self.apply_converters(rows, converters))\n", "        return rows\n"]}]}]}