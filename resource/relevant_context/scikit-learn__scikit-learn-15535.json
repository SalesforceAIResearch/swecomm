{"instance_id": "scikit-learn__scikit-learn-15535", "identified_spans": [{"file_path": "sklearn/metrics/cluster/_supervised.py", "span_ids": ["contingency_matrix", "mutual_info_score"], "content": [{"span_id": "contingency_matrix", "start_line": 79, "end_line": 133, "content": ["def contingency_matrix(labels_true, labels_pred, eps=None, sparse=False):\n", "    \"\"\"Build a contingency matrix describing the relationship between labels.\n", "\n", "    Parameters\n", "    ----------\n", "    labels_true : int array, shape = [n_samples]\n", "        Ground truth class labels to be used as a reference\n", "\n", "    labels_pred : array-like of shape (n_samples,)\n", "        Cluster labels to evaluate\n", "\n", "    eps : None or float, optional.\n", "        If a float, that value is added to all values in the contingency\n", "        matrix. This helps to stop NaN propagation.\n", "        If ``None``, nothing is adjusted.\n", "\n", "    sparse : boolean, optional.\n", "        If True, return a sparse CSR continency matrix. If ``eps is not None``,\n", "        and ``sparse is True``, will throw ValueError.\n", "\n", "        .. versionadded:: 0.18\n", "\n", "    Returns\n", "    -------\n", "    contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n", "        Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n", "        true class :math:`i` and in predicted class :math:`j`. If\n", "        ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n", "        given, the dtype will be float.\n", "        Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.\n", "    \"\"\"\n", "\n", "    if eps is not None and sparse:\n", "        raise ValueError(\"Cannot set 'eps' when sparse=True\")\n", "\n", "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n", "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n", "    n_classes = classes.shape[0]\n", "    n_clusters = clusters.shape[0]\n", "    # Using coo_matrix to accelerate simple histogram calculation,\n", "    # i.e. bins are consecutive integers\n", "    # Currently, coo_matrix is faster than histogram2d for simple cases\n", "    contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),\n", "                                 (class_idx, cluster_idx)),\n", "                                shape=(n_classes, n_clusters),\n", "                                dtype=np.int)\n", "    if sparse:\n", "        contingency = contingency.tocsr()\n", "        contingency.sum_duplicates()\n", "    else:\n", "        contingency = contingency.toarray()\n", "        if eps is not None:\n", "            # don't use += as contingency is integer\n", "            contingency = contingency + eps\n", "    return contingency\n"]}, {"span_id": "mutual_info_score", "start_line": 565, "end_line": 648, "content": ["def mutual_info_score(labels_true, labels_pred, contingency=None):\n", "    \"\"\"Mutual Information between two clusterings.\n", "\n", "    The Mutual Information is a measure of the similarity between two labels of\n", "    the same data. Where :math:`|U_i|` is the number of the samples\n", "    in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n", "    samples in cluster :math:`V_j`, the Mutual Information\n", "    between clusterings :math:`U` and :math:`V` is given as:\n", "\n", "    .. math::\n", "\n", "        MI(U,V)=\\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i\\\\cap V_j|}{N}\n", "        \\\\log\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\n", "\n", "    This metric is independent of the absolute values of the labels:\n", "    a permutation of the class or cluster label values won't change the\n", "    score value in any way.\n", "\n", "    This metric is furthermore symmetric: switching ``label_true`` with\n", "    ``label_pred`` will return the same score value. This can be useful to\n", "    measure the agreement of two independent label assignments strategies\n", "    on the same dataset when the real ground truth is not known.\n", "\n", "    Read more in the :ref:`User Guide <mutual_info_score>`.\n", "\n", "    Parameters\n", "    ----------\n", "    labels_true : int array, shape = [n_samples]\n", "        A clustering of the data into disjoint subsets.\n", "\n", "    labels_pred : int array-like of shape (n_samples,)\n", "        A clustering of the data into disjoint subsets.\n", "\n", "    contingency : {None, array, sparse matrix}, \\\n", "                  shape = [n_classes_true, n_classes_pred]\n", "        A contingency matrix given by the :func:`contingency_matrix` function.\n", "        If value is ``None``, it will be computed, otherwise the given value is\n", "        used, with ``labels_true`` and ``labels_pred`` ignored.\n", "\n", "    Returns\n", "    -------\n", "    mi : float\n", "       Mutual information, a non-negative value\n", "\n", "    Notes\n", "    -----\n", "    The logarithm used is the natural logarithm (base-e).\n", "\n", "    See also\n", "    --------\n", "    adjusted_mutual_info_score: Adjusted against chance Mutual Information\n", "    normalized_mutual_info_score: Normalized Mutual Information\n", "    \"\"\"\n", "    if contingency is None:\n", "        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n", "        contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n", "    else:\n", "        contingency = check_array(contingency,\n", "                                  accept_sparse=['csr', 'csc', 'coo'],\n", "                                  dtype=[int, np.int32, np.int64])\n", "\n", "    if isinstance(contingency, np.ndarray):\n", "        # For an array\n", "        nzx, nzy = np.nonzero(contingency)\n", "        nz_val = contingency[nzx, nzy]\n", "    elif sp.issparse(contingency):\n", "        # For a sparse matrix\n", "        nzx, nzy, nz_val = sp.find(contingency)\n", "    else:\n", "        raise ValueError(\"Unsupported type for 'contingency': %s\" %\n", "                         type(contingency))\n", "\n", "    contingency_sum = contingency.sum()\n", "    pi = np.ravel(contingency.sum(axis=1))\n", "    pj = np.ravel(contingency.sum(axis=0))\n", "    log_contingency_nm = np.log(nz_val)\n", "    contingency_nm = nz_val / contingency_sum\n", "    # Don't need to calculate the full outer product, just for non-zeroes\n", "    outer = (pi.take(nzx).astype(np.int64, copy=False)\n", "             * pj.take(nzy).astype(np.int64, copy=False))\n", "    log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n", "    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n", "          contingency_nm * log_outer)\n", "    return mi.sum()\n"]}]}, {"file_path": "sklearn/utils/validation.py", "span_ids": ["check_array"], "content": [{"span_id": "check_array", "start_line": 338, "end_line": 597, "content": ["def check_array(array, accept_sparse=False, accept_large_sparse=True,\n", "                dtype=\"numeric\", order=None, copy=False, force_all_finite=True,\n", "                ensure_2d=True, allow_nd=False, ensure_min_samples=1,\n", "                ensure_min_features=1, warn_on_dtype=None, estimator=None):\n", "\n", "    \"\"\"Input validation on an array, list, sparse matrix or similar.\n", "\n", "    By default, the input is checked to be a non-empty 2D array containing\n", "    only finite values. If the dtype of the array is object, attempt\n", "    converting to float, raising on failure.\n", "\n", "    Parameters\n", "    ----------\n", "    array : object\n", "        Input object to check / convert.\n", "\n", "    accept_sparse : string, boolean or list/tuple of strings (default=False)\n", "        String[s] representing allowed sparse matrix formats, such as 'csc',\n", "        'csr', etc. If the input is sparse but not in the allowed format,\n", "        it will be converted to the first listed format. True allows the input\n", "        to be any format. False means that a sparse matrix input will\n", "        raise an error.\n", "\n", "    accept_large_sparse : bool (default=True)\n", "        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n", "        accept_sparse, accept_large_sparse=False will cause it to be accepted\n", "        only if its indices are stored with a 32-bit dtype.\n", "\n", "        .. versionadded:: 0.20\n", "\n", "    dtype : string, type, list of types or None (default=\"numeric\")\n", "        Data type of result. If None, the dtype of the input is preserved.\n", "        If \"numeric\", dtype is preserved unless array.dtype is object.\n", "        If dtype is a list of types, conversion on the first type is only\n", "        performed if the dtype of the input is not in the list.\n", "\n", "    order : 'F', 'C' or None (default=None)\n", "        Whether an array will be forced to be fortran or c-style.\n", "        When order is None (default), then if copy=False, nothing is ensured\n", "        about the memory layout of the output array; otherwise (copy=True)\n", "        the memory layout of the returned array is kept as close as possible\n", "        to the original array.\n", "\n", "    copy : boolean (default=False)\n", "        Whether a forced copy will be triggered. If copy=False, a copy might\n", "        be triggered by a conversion.\n", "\n", "    force_all_finite : boolean or 'allow-nan', (default=True)\n", "        Whether to raise an error on np.inf and np.nan in array. The\n", "        possibilities are:\n", "\n", "        - True: Force all values of array to be finite.\n", "        - False: accept both np.inf and np.nan in array.\n", "        - 'allow-nan': accept only np.nan values in array. Values cannot\n", "          be infinite.\n", "\n", "        For object dtyped data, only np.nan is checked and not np.inf.\n", "\n", "        .. versionadded:: 0.20\n", "           ``force_all_finite`` accepts the string ``'allow-nan'``.\n", "\n", "    ensure_2d : boolean (default=True)\n", "        Whether to raise a value error if array is not 2D.\n", "\n", "    allow_nd : boolean (default=False)\n", "        Whether to allow array.ndim > 2.\n", "\n", "    ensure_min_samples : int (default=1)\n", "        Make sure that the array has a minimum number of samples in its first\n", "        axis (rows for a 2D array). Setting to 0 disables this check.\n", "\n", "    ensure_min_features : int (default=1)\n", "        Make sure that the 2D array has some minimum number of features\n", "        (columns). The default value of 1 rejects empty datasets.\n", "        This check is only enforced when the input data has effectively 2\n", "        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n", "        disables this check.\n", "\n", "    warn_on_dtype : boolean or None, optional (default=None)\n", "        Raise DataConversionWarning if the dtype of the input data structure\n", "        does not match the requested dtype, causing a memory copy.\n", "\n", "        .. deprecated:: 0.21\n", "            ``warn_on_dtype`` is deprecated in version 0.21 and will be\n", "            removed in 0.23.\n", "\n", "    estimator : str or estimator instance (default=None)\n", "        If passed, include the name of the estimator in warning messages.\n", "\n", "    Returns\n", "    -------\n", "    array_converted : object\n", "        The converted and validated array.\n", "    \"\"\"\n", "    # warn_on_dtype deprecation\n", "    if warn_on_dtype is not None:\n", "        warnings.warn(\n", "            \"'warn_on_dtype' is deprecated in version 0.21 and will be \"\n", "            \"removed in 0.23. Don't set `warn_on_dtype` to remove this \"\n", "            \"warning.\",\n", "            FutureWarning, stacklevel=2)\n", "\n", "    # store reference to original array to check if copy is needed when\n", "    # function returns\n", "    array_orig = array\n", "\n", "    # store whether originally we wanted numeric dtype\n", "    dtype_numeric = isinstance(dtype, str) and dtype == \"numeric\"\n", "\n", "    dtype_orig = getattr(array, \"dtype\", None)\n", "    if not hasattr(dtype_orig, 'kind'):\n", "        # not a data type (e.g. a column named dtype in a pandas DataFrame)\n", "        dtype_orig = None\n", "\n", "    # check if the object contains several dtypes (typically a pandas\n", "    # DataFrame), and store them. If not, store None.\n", "    dtypes_orig = None\n", "    if hasattr(array, \"dtypes\") and hasattr(array.dtypes, '__array__'):\n", "        dtypes_orig = np.array(array.dtypes)\n", "        if all(isinstance(dtype, np.dtype) for dtype in dtypes_orig):\n", "            dtype_orig = np.result_type(*array.dtypes)\n", "\n", "    if dtype_numeric:\n", "        if dtype_orig is not None and dtype_orig.kind == \"O\":\n", "            # if input is object, convert to float.\n", "            dtype = np.float64\n", "        else:\n", "            dtype = None\n", "\n", "    if isinstance(dtype, (list, tuple)):\n", "        if dtype_orig is not None and dtype_orig in dtype:\n", "            # no dtype conversion required\n", "            dtype = None\n", "        else:\n", "            # dtype conversion required. Let's select the first element of the\n", "            # list of accepted types.\n", "            dtype = dtype[0]\n", "\n", "    if force_all_finite not in (True, False, 'allow-nan'):\n", "        raise ValueError('force_all_finite should be a bool or \"allow-nan\"'\n", "                         '. Got {!r} instead'.format(force_all_finite))\n", "\n", "    if estimator is not None:\n", "        if isinstance(estimator, str):\n", "            estimator_name = estimator\n", "        else:\n", "            estimator_name = estimator.__class__.__name__\n", "    else:\n", "        estimator_name = \"Estimator\"\n", "    context = \" by %s\" % estimator_name if estimator is not None else \"\"\n", "\n", "    if sp.issparse(array):\n", "        _ensure_no_complex_data(array)\n", "        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n", "                                      dtype=dtype, copy=copy,\n", "                                      force_all_finite=force_all_finite,\n", "                                      accept_large_sparse=accept_large_sparse)\n", "    else:\n", "        # If np.array(..) gives ComplexWarning, then we convert the warning\n", "        # to an error. This is needed because specifying a non complex\n", "        # dtype to the function converts complex to real dtype,\n", "        # thereby passing the test made in the lines following the scope\n", "        # of warnings context manager.\n", "        with warnings.catch_warnings():\n", "            try:\n", "                warnings.simplefilter('error', ComplexWarning)\n", "                if dtype is not None and np.dtype(dtype).kind in 'iu':\n", "                    # Conversion float -> int should not contain NaN or\n", "                    # inf (numpy#14412). We cannot use casting='safe' because\n", "                    # then conversion float -> int would be disallowed.\n", "                    array = np.asarray(array, order=order)\n", "                    if array.dtype.kind == 'f':\n", "                        _assert_all_finite(array, allow_nan=False,\n", "                                           msg_dtype=dtype)\n", "                    array = array.astype(dtype, casting=\"unsafe\", copy=False)\n", "                else:\n", "                    array = np.asarray(array, order=order, dtype=dtype)\n", "            except ComplexWarning:\n", "                raise ValueError(\"Complex data not supported\\n\"\n", "                                 \"{}\\n\".format(array))\n", "\n", "        # It is possible that the np.array(..) gave no warning. This happens\n", "        # when no dtype conversion happened, for example dtype = None. The\n", "        # result is that np.array(..) produces an array of complex dtype\n", "        # and we need to catch and raise exception for such cases.\n", "        _ensure_no_complex_data(array)\n", "\n", "        if ensure_2d:\n", "            # If input is scalar raise error\n", "            if array.ndim == 0:\n", "                raise ValueError(\n", "                    \"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\n", "                    \"Reshape your data either using array.reshape(-1, 1) if \"\n", "                    \"your data has a single feature or array.reshape(1, -1) \"\n", "                    \"if it contains a single sample.\".format(array))\n", "            # If input is 1D raise error\n", "            if array.ndim == 1:\n", "                raise ValueError(\n", "                    \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n", "                    \"Reshape your data either using array.reshape(-1, 1) if \"\n", "                    \"your data has a single feature or array.reshape(1, -1) \"\n", "                    \"if it contains a single sample.\".format(array))\n", "\n", "        # in the future np.flexible dtypes will be handled like object dtypes\n", "        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):\n", "            warnings.warn(\n", "                \"Beginning in version 0.22, arrays of bytes/strings will be \"\n", "                \"converted to decimal numbers if dtype='numeric'. \"\n", "                \"It is recommended that you convert the array to \"\n", "                \"a float dtype before using it in scikit-learn, \"\n", "                \"for example by using \"\n", "                \"your_array = your_array.astype(np.float64).\",\n", "                FutureWarning, stacklevel=2)\n", "\n", "        # make sure we actually converted to numeric:\n", "        if dtype_numeric and array.dtype.kind == \"O\":\n", "            array = array.astype(np.float64)\n", "        if not allow_nd and array.ndim >= 3:\n", "            raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n", "                             % (array.ndim, estimator_name))\n", "\n", "        if force_all_finite:\n", "            _assert_all_finite(array,\n", "                               allow_nan=force_all_finite == 'allow-nan')\n", "\n", "    if ensure_min_samples > 0:\n", "        n_samples = _num_samples(array)\n", "        if n_samples < ensure_min_samples:\n", "            raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n", "                             \" minimum of %d is required%s.\"\n", "                             % (n_samples, array.shape, ensure_min_samples,\n", "                                context))\n", "\n", "    if ensure_min_features > 0 and array.ndim == 2:\n", "        n_features = array.shape[1]\n", "        if n_features < ensure_min_features:\n", "            raise ValueError(\"Found array with %d feature(s) (shape=%s) while\"\n", "                             \" a minimum of %d is required%s.\"\n", "                             % (n_features, array.shape, ensure_min_features,\n", "                                context))\n", "\n", "    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:\n", "        msg = (\"Data with input dtype %s was converted to %s%s.\"\n", "               % (dtype_orig, array.dtype, context))\n", "        warnings.warn(msg, DataConversionWarning, stacklevel=2)\n", "\n", "    if copy and np.may_share_memory(array, array_orig):\n", "        array = np.array(array, dtype=dtype, order=order)\n", "\n", "    if (warn_on_dtype and dtypes_orig is not None and\n", "            {array.dtype} != set(dtypes_orig)):\n", "        # if there was at the beginning some other types than the final one\n", "        # (for instance in a DataFrame that can contain several dtypes) then\n", "        # some data must have been converted\n", "        msg = (\"Data with input dtype %s were all converted to %s%s.\"\n", "               % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype,\n", "                  context))\n", "        warnings.warn(msg, DataConversionWarning, stacklevel=3)\n", "\n", "    return array\n"]}]}]}