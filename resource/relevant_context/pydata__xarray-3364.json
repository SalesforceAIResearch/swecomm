{"instance_id": "pydata__xarray-3364", "identified_spans": [{"file_path": "xarray/core/alignment.py", "span_ids": ["reindex_variables", "align"], "content": [{"span_id": "reindex_variables", "start_line": 462, "end_line": 594, "content": ["def reindex_variables(\n", "    variables: Mapping[Any, Variable],\n", "    sizes: Mapping[Any, int],\n", "    indexes: Mapping[Any, pd.Index],\n", "    indexers: Mapping,\n", "    method: Optional[str] = None,\n", "    tolerance: Any = None,\n", "    copy: bool = True,\n", "    fill_value: Optional[Any] = dtypes.NA,\n", ") -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n", "    \"\"\"Conform a dictionary of aligned variables onto a new set of variables,\n", "    filling in missing values with NaN.\n", "\n", "    Not public API.\n", "\n", "    Parameters\n", "    ----------\n", "    variables : dict-like\n", "        Dictionary of xarray.Variable objects.\n", "    sizes : dict-like\n", "        Dictionary from dimension names to integer sizes.\n", "    indexes : dict-like\n", "        Dictionary of indexes associated with variables.\n", "    indexers : dict\n", "        Dictionary with keys given by dimension names and values given by\n", "        arrays of coordinates tick labels. Any mis-matched coordinate values\n", "        will be filled in with NaN, and any mis-matched dimension names will\n", "        simply be ignored.\n", "    method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n", "        Method to use for filling index values in ``indexers`` not found in\n", "        this dataset:\n", "          * None (default): don't fill gaps\n", "          * pad / ffill: propagate last valid index value forward\n", "          * backfill / bfill: propagate next valid index value backward\n", "          * nearest: use nearest valid index value\n", "    tolerance : optional\n", "        Maximum distance between original and new labels for inexact matches.\n", "        The values of the index at the matching locations must satisfy the\n", "        equation ``abs(index[indexer] - target) <= tolerance``.\n", "    copy : bool, optional\n", "        If ``copy=True``, data in the return values is always copied. If\n", "        ``copy=False`` and reindexing is unnecessary, or can be performed\n", "        with only slice operations, then the output may share memory with\n", "        the input. In either case, new xarray objects are always returned.\n", "    fill_value : scalar, optional\n", "        Value to use for newly missing values\n", "\n", "    Returns\n", "    -------\n", "    reindexed : dict\n", "        Dict of reindexed variables.\n", "    new_indexes : dict\n", "        Dict of indexes associated with the reindexed variables.\n", "    \"\"\"\n", "    from .dataarray import DataArray\n", "\n", "    # create variables for the new dataset\n", "    reindexed: Dict[Hashable, Variable] = {}\n", "\n", "    # build up indexers for assignment along each dimension\n", "    int_indexers = {}\n", "    new_indexes = dict(indexes)\n", "    masked_dims = set()\n", "    unchanged_dims = set()\n", "\n", "    for dim, indexer in indexers.items():\n", "        if isinstance(indexer, DataArray) and indexer.dims != (dim,):\n", "            raise ValueError(\n", "                \"Indexer has dimensions {:s} that are different \"\n", "                \"from that to be indexed along {:s}\".format(str(indexer.dims), dim)\n", "            )\n", "\n", "        target = new_indexes[dim] = utils.safe_cast_to_index(indexers[dim])\n", "\n", "        if dim in indexes:\n", "            index = indexes[dim]\n", "\n", "            if not index.is_unique:\n", "                raise ValueError(\n", "                    \"cannot reindex or align along dimension %r because the \"\n", "                    \"index has duplicate values\" % dim\n", "                )\n", "\n", "            int_indexer = get_indexer_nd(index, target, method, tolerance)\n", "\n", "            # We uses negative values from get_indexer_nd to signify\n", "            # values that are missing in the index.\n", "            if (int_indexer < 0).any():\n", "                masked_dims.add(dim)\n", "            elif np.array_equal(int_indexer, np.arange(len(index))):\n", "                unchanged_dims.add(dim)\n", "\n", "            int_indexers[dim] = int_indexer\n", "\n", "        if dim in variables:\n", "            var = variables[dim]\n", "            args: tuple = (var.attrs, var.encoding)\n", "        else:\n", "            args = ()\n", "        reindexed[dim] = IndexVariable((dim,), target, *args)\n", "\n", "    for dim in sizes:\n", "        if dim not in indexes and dim in indexers:\n", "            existing_size = sizes[dim]\n", "            new_size = indexers[dim].size\n", "            if existing_size != new_size:\n", "                raise ValueError(\n", "                    \"cannot reindex or align along dimension %r without an \"\n", "                    \"index because its size %r is different from the size of \"\n", "                    \"the new index %r\" % (dim, existing_size, new_size)\n", "                )\n", "\n", "    for name, var in variables.items():\n", "        if name not in indexers:\n", "            key = tuple(\n", "                slice(None) if d in unchanged_dims else int_indexers.get(d, slice(None))\n", "                for d in var.dims\n", "            )\n", "            needs_masking = any(d in masked_dims for d in var.dims)\n", "\n", "            if needs_masking:\n", "                new_var = var._getitem_with_mask(key, fill_value=fill_value)\n", "            elif all(is_full_slice(k) for k in key):\n", "                # no reindexing necessary\n", "                # here we need to manually deal with copying data, since\n", "                # we neither created a new ndarray nor used fancy indexing\n", "                new_var = var.copy(deep=copy)\n", "            else:\n", "                new_var = var[key]\n", "\n", "            reindexed[name] = new_var\n", "\n", "    return reindexed, new_indexes\n"]}, {"span_id": "align", "start_line": 61, "end_line": 337, "content": ["def align(\n", "    *objects,\n", "    join=\"inner\",\n", "    copy=True,\n", "    indexes=None,\n", "    exclude=frozenset(),\n", "    fill_value=dtypes.NA\n", "):\n", "    \"\"\"\n", "    Given any number of Dataset and/or DataArray objects, returns new\n", "    objects with aligned indexes and dimension sizes.\n", "\n", "    Array from the aligned objects are suitable as input to mathematical\n", "    operators, because along each dimension they have the same index and size.\n", "\n", "    Missing values (if ``join != 'inner'``) are filled with ``fill_value``.\n", "    The default fill value is NaN.\n", "\n", "    Parameters\n", "    ----------\n", "    *objects : Dataset or DataArray\n", "        Objects to align.\n", "    join : {'outer', 'inner', 'left', 'right', 'exact', 'override'}, optional\n", "        Method for joining the indexes of the passed objects along each\n", "        dimension:\n", "\n", "        - 'outer': use the union of object indexes\n", "        - 'inner': use the intersection of object indexes\n", "        - 'left': use indexes from the first object with each dimension\n", "        - 'right': use indexes from the last object with each dimension\n", "        - 'exact': instead of aligning, raise `ValueError` when indexes to be\n", "          aligned are not equal\n", "        - 'override': if indexes are of same size, rewrite indexes to be\n", "          those of the first object with that dimension. Indexes for the same\n", "          dimension must have the same size in all objects.\n", "    copy : bool, optional\n", "        If ``copy=True``, data in the return values is always copied. If\n", "        ``copy=False`` and reindexing is unnecessary, or can be performed with\n", "        only slice operations, then the output may share memory with the input.\n", "        In either case, new xarray objects are always returned.\n", "    indexes : dict-like, optional\n", "        Any indexes explicitly provided with the `indexes` argument should be\n", "        used in preference to the aligned indexes.\n", "    exclude : sequence of str, optional\n", "        Dimensions that must be excluded from alignment\n", "    fill_value : scalar, optional\n", "        Value to use for newly missing values\n", "\n", "    Returns\n", "    -------\n", "    aligned : same as *objects\n", "        Tuple of objects with aligned coordinates.\n", "\n", "    Raises\n", "    ------\n", "    ValueError\n", "        If any dimensions without labels on the arguments have different sizes,\n", "        or a different size than the size of the aligned dimension labels.\n", "\n", "    Examples\n", "    --------\n", "\n", "    >>> import xarray as xr\n", "    >>> x = xr.DataArray([[25, 35], [10, 24]], dims=('lat', 'lon'),\n", "    ...              coords={'lat': [35., 40.], 'lon': [100., 120.]})\n", "    >>> y = xr.DataArray([[20, 5], [7, 13]], dims=('lat', 'lon'),\n", "    ...              coords={'lat': [35., 42.], 'lon': [100., 120.]})\n", "\n", "    >>> x\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[25, 35],\n", "           [10, 24]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> y\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[20,  5],\n", "           [ 7, 13]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y)\n", "    >>> a\n", "    <xarray.DataArray (lat: 1, lon: 2)>\n", "    array([[25, 35]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 1, lon: 2)>\n", "    array([[20,  5]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y, join='outer')\n", "    >>> a\n", "    <xarray.DataArray (lat: 3, lon: 2)>\n", "    array([[25., 35.],\n", "           [10., 24.],\n", "           [nan, nan]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 3, lon: 2)>\n", "    array([[20.,  5.],\n", "           [nan, nan],\n", "           [ 7., 13.]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y, join='outer', fill_value=-999)\n", "    >>> a\n", "    <xarray.DataArray (lat: 3, lon: 2)>\n", "    array([[  25,   35],\n", "           [  10,   24],\n", "           [-999, -999]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 3, lon: 2)>\n", "    array([[  20,    5],\n", "           [-999, -999],\n", "           [   7,   13]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y, join='left')\n", "    >>> a\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[25, 35],\n", "           [10, 24]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[20.,  5.],\n", "           [nan, nan]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y, join='right')\n", "    >>> a\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[25., 35.],\n", "           [nan, nan]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[20,  5],\n", "           [ 7, 13]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 42.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    >>> a, b = xr.align(x, y, join='exact')\n", "    Traceback (most recent call last):\n", "    ...\n", "        \"indexes along dimension {!r} are not equal\".format(dim)\n", "    ValueError: indexes along dimension 'lat' are not equal\n", "\n", "    >>> a, b = xr.align(x, y, join='override')\n", "    >>> a\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[25, 35],\n", "           [10, 24]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0\n", "    * lon      (lon) float64 100.0 120.0\n", "    >>> b\n", "    <xarray.DataArray (lat: 2, lon: 2)>\n", "    array([[20,  5],\n", "           [ 7, 13]])\n", "    Coordinates:\n", "    * lat      (lat) float64 35.0 40.0\n", "    * lon      (lon) float64 100.0 120.0\n", "\n", "    \"\"\"\n", "    if indexes is None:\n", "        indexes = {}\n", "\n", "    if not indexes and len(objects) == 1:\n", "        # fast path for the trivial case\n", "        obj, = objects\n", "        return (obj.copy(deep=copy),)\n", "\n", "    all_indexes = defaultdict(list)\n", "    unlabeled_dim_sizes = defaultdict(set)\n", "    for obj in objects:\n", "        for dim in obj.dims:\n", "            if dim not in exclude:\n", "                try:\n", "                    index = obj.indexes[dim]\n", "                except KeyError:\n", "                    unlabeled_dim_sizes[dim].add(obj.sizes[dim])\n", "                else:\n", "                    all_indexes[dim].append(index)\n", "\n", "    if join == \"override\":\n", "        objects = _override_indexes(objects, all_indexes, exclude)\n", "\n", "    # We don't reindex over dimensions with all equal indexes for two reasons:\n", "    # - It's faster for the usual case (already aligned objects).\n", "    # - It ensures it's possible to do operations that don't require alignment\n", "    #   on indexes with duplicate values (which cannot be reindexed with\n", "    #   pandas). This is useful, e.g., for overwriting such duplicate indexes.\n", "    joiner = _get_joiner(join)\n", "    joined_indexes = {}\n", "    for dim, matching_indexes in all_indexes.items():\n", "        if dim in indexes:\n", "            index = utils.safe_cast_to_index(indexes[dim])\n", "            if (\n", "                any(not index.equals(other) for other in matching_indexes)\n", "                or dim in unlabeled_dim_sizes\n", "            ):\n", "                joined_indexes[dim] = index\n", "        else:\n", "            if (\n", "                any(\n", "                    not matching_indexes[0].equals(other)\n", "                    for other in matching_indexes[1:]\n", "                )\n", "                or dim in unlabeled_dim_sizes\n", "            ):\n", "                if join == \"exact\":\n", "                    raise ValueError(\n", "                        \"indexes along dimension {!r} are not equal\".format(dim)\n", "                    )\n", "                index = joiner(matching_indexes)\n", "                joined_indexes[dim] = index\n", "            else:\n", "                index = matching_indexes[0]\n", "\n", "        if dim in unlabeled_dim_sizes:\n", "            unlabeled_sizes = unlabeled_dim_sizes[dim]\n", "            labeled_size = index.size\n", "            if len(unlabeled_sizes | {labeled_size}) > 1:\n", "                raise ValueError(\n", "                    \"arguments without labels along dimension %r cannot be \"\n", "                    \"aligned because they have different dimension size(s) %r \"\n", "                    \"than the size of the aligned dimension labels: %r\"\n", "                    % (dim, unlabeled_sizes, labeled_size)\n", "                )\n", "\n", "    for dim in unlabeled_dim_sizes:\n", "        if dim not in all_indexes:\n", "            sizes = unlabeled_dim_sizes[dim]\n", "            if len(sizes) > 1:\n", "                raise ValueError(\n", "                    \"arguments without labels along dimension %r cannot be \"\n", "                    \"aligned because they have different dimension sizes: %r\"\n", "                    % (dim, sizes)\n", "                )\n", "\n", "    result = []\n", "    for obj in objects:\n", "        valid_indexers = {k: v for k, v in joined_indexes.items() if k in obj.dims}\n", "        if not valid_indexers:\n", "            # fast path for no reindexing necessary\n", "            new_obj = obj.copy(deep=copy)\n", "        else:\n", "            new_obj = obj.reindex(copy=copy, fill_value=fill_value, **valid_indexers)\n", "        new_obj.encoding = obj.encoding\n", "        result.append(new_obj)\n", "\n", "    return tuple(result)\n"]}]}, {"file_path": "xarray/core/concat.py", "span_ids": ["concat", "_dataset_concat"], "content": [{"span_id": "concat", "start_line": 10, "end_line": 131, "content": ["def concat(\n", "    objs,\n", "    dim,\n", "    data_vars=\"all\",\n", "    coords=\"different\",\n", "    compat=\"equals\",\n", "    positions=None,\n", "    fill_value=dtypes.NA,\n", "    join=\"outer\",\n", "):\n", "    \"\"\"Concatenate xarray objects along a new or existing dimension.\n", "\n", "    Parameters\n", "    ----------\n", "    objs : sequence of Dataset and DataArray objects\n", "        xarray objects to concatenate together. Each object is expected to\n", "        consist of variables and coordinates with matching shapes except for\n", "        along the concatenated dimension.\n", "    dim : str or DataArray or pandas.Index\n", "        Name of the dimension to concatenate along. This can either be a new\n", "        dimension name, in which case it is added along axis=0, or an existing\n", "        dimension name, in which case the location of the dimension is\n", "        unchanged. If dimension is provided as a DataArray or Index, its name\n", "        is used as the dimension to concatenate along and the values are added\n", "        as a coordinate.\n", "    data_vars : {'minimal', 'different', 'all' or list of str}, optional\n", "        These data variables will be concatenated together:\n", "          * 'minimal': Only data variables in which the dimension already\n", "            appears are included.\n", "          * 'different': Data variables which are not equal (ignoring\n", "            attributes) across all datasets are also concatenated (as well as\n", "            all for which dimension already appears). Beware: this option may\n", "            load the data payload of data variables into memory if they are not\n", "            already loaded.\n", "          * 'all': All data variables will be concatenated.\n", "          * list of str: The listed data variables will be concatenated, in\n", "            addition to the 'minimal' data variables.\n", "        If objects are DataArrays, data_vars must be 'all'.\n", "    coords : {'minimal', 'different', 'all' or list of str}, optional\n", "        These coordinate variables will be concatenated together:\n", "          * 'minimal': Only coordinates in which the dimension already appears\n", "            are included.\n", "          * 'different': Coordinates which are not equal (ignoring attributes)\n", "            across all datasets are also concatenated (as well as all for which\n", "            dimension already appears). Beware: this option may load the data\n", "            payload of coordinate variables into memory if they are not already\n", "            loaded.\n", "          * 'all': All coordinate variables will be concatenated, except\n", "            those corresponding to other dimensions.\n", "          * list of str: The listed coordinate variables will be concatenated,\n", "            in addition to the 'minimal' coordinates.\n", "    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional\n", "        String indicating how to compare non-concatenated variables of the same name for\n", "        potential conflicts. This is passed down to merge.\n", "\n", "        - 'broadcast_equals': all values must be equal when variables are\n", "          broadcast against each other to ensure common dimensions.\n", "        - 'equals': all values and dimensions must be the same.\n", "        - 'identical': all values, dimensions and attributes must be the\n", "          same.\n", "        - 'no_conflicts': only values which are not null in both datasets\n", "          must be equal. The returned dataset then contains the combination\n", "          of all non-null values.\n", "        - 'override': skip comparing and pick variable from first dataset\n", "    positions : None or list of integer arrays, optional\n", "        List of integer arrays which specifies the integer positions to which\n", "        to assign each dataset along the concatenated dimension. If not\n", "        supplied, objects are concatenated in the provided order.\n", "    fill_value : scalar, optional\n", "        Value to use for newly missing values\n", "    join : {'outer', 'inner', 'left', 'right', 'exact'}, optional\n", "        String indicating how to combine differing indexes\n", "        (excluding dim) in objects\n", "\n", "        - 'outer': use the union of object indexes\n", "        - 'inner': use the intersection of object indexes\n", "        - 'left': use indexes from the first object with each dimension\n", "        - 'right': use indexes from the last object with each dimension\n", "        - 'exact': instead of aligning, raise `ValueError` when indexes to be\n", "          aligned are not equal\n", "        - 'override': if indexes are of same size, rewrite indexes to be\n", "          those of the first object with that dimension. Indexes for the same\n", "          dimension must have the same size in all objects.\n", "\n", "    indexers, mode, concat_over : deprecated\n", "\n", "    Returns\n", "    -------\n", "    concatenated : type of objs\n", "\n", "    See also\n", "    --------\n", "    merge\n", "    auto_combine\n", "    \"\"\"\n", "    # TODO: add ignore_index arguments copied from pandas.concat\n", "    # TODO: support concatenating scalar coordinates even if the concatenated\n", "    # dimension already exists\n", "    from .dataset import Dataset\n", "    from .dataarray import DataArray\n", "\n", "    try:\n", "        first_obj, objs = utils.peek_at(objs)\n", "    except StopIteration:\n", "        raise ValueError(\"must supply at least one object to concatenate\")\n", "\n", "    if compat not in _VALID_COMPAT:\n", "        raise ValueError(\n", "            \"compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'\"\n", "            % compat\n", "        )\n", "\n", "    if isinstance(first_obj, DataArray):\n", "        f = _dataarray_concat\n", "    elif isinstance(first_obj, Dataset):\n", "        f = _dataset_concat\n", "    else:\n", "        raise TypeError(\n", "            \"can only concatenate xarray Dataset and DataArray \"\n", "            \"objects, got %s\" % type(first_obj)\n", "        )\n", "    return f(objs, dim, data_vars, coords, compat, positions, fill_value, join)\n"]}, {"span_id": "_dataset_concat", "start_line": 263, "end_line": 385, "content": ["def _dataset_concat(\n", "    datasets,\n", "    dim,\n", "    data_vars,\n", "    coords,\n", "    compat,\n", "    positions,\n", "    fill_value=dtypes.NA,\n", "    join=\"outer\",\n", "):\n", "    \"\"\"\n", "    Concatenate a sequence of datasets along a new or existing dimension\n", "    \"\"\"\n", "    from .dataset import Dataset\n", "\n", "    dim, coord = _calc_concat_dim_coord(dim)\n", "    # Make sure we're working on a copy (we'll be loading variables)\n", "    datasets = [ds.copy() for ds in datasets]\n", "    datasets = align(\n", "        *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value\n", "    )\n", "\n", "    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n", "    dim_names = set(dim_coords)\n", "    unlabeled_dims = dim_names - coord_names\n", "\n", "    both_data_and_coords = coord_names & data_names\n", "    if both_data_and_coords:\n", "        raise ValueError(\n", "            \"%r is a coordinate in some datasets but not others.\" % both_data_and_coords\n", "        )\n", "    # we don't want the concat dimension in the result dataset yet\n", "    dim_coords.pop(dim, None)\n", "    dims_sizes.pop(dim, None)\n", "\n", "    # case where concat dimension is a coordinate or data_var but not a dimension\n", "    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n", "        datasets = [ds.expand_dims(dim) for ds in datasets]\n", "\n", "    # determine which variables to concatentate\n", "    concat_over, equals, concat_dim_lengths = _calc_concat_over(\n", "        datasets, dim, dim_names, data_vars, coords, compat\n", "    )\n", "\n", "    # determine which variables to merge, and then merge them according to compat\n", "    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n", "\n", "    result_vars = {}\n", "    if variables_to_merge:\n", "        to_merge = {var: [] for var in variables_to_merge}\n", "\n", "        for ds in datasets:\n", "            absent_merge_vars = variables_to_merge - set(ds.variables)\n", "            if absent_merge_vars:\n", "                raise ValueError(\n", "                    \"variables %r are present in some datasets but not others. \"\n", "                    % absent_merge_vars\n", "                )\n", "\n", "            for var in variables_to_merge:\n", "                to_merge[var].append(ds.variables[var])\n", "\n", "        for var in variables_to_merge:\n", "            result_vars[var] = unique_variable(\n", "                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n", "            )\n", "    else:\n", "        result_vars = {}\n", "    result_vars.update(dim_coords)\n", "\n", "    # assign attrs and encoding from first dataset\n", "    result_attrs = datasets[0].attrs\n", "    result_encoding = datasets[0].encoding\n", "\n", "    # check that global attributes are fixed across all datasets if necessary\n", "    for ds in datasets[1:]:\n", "        if compat == \"identical\" and not utils.dict_equiv(ds.attrs, result_attrs):\n", "            raise ValueError(\"Dataset global attributes not equal.\")\n", "\n", "    # we've already verified everything is consistent; now, calculate\n", "    # shared dimension sizes so we can expand the necessary variables\n", "    def ensure_common_dims(vars):\n", "        # ensure each variable with the given name shares the same\n", "        # dimensions and the same shape for all of them except along the\n", "        # concat dimension\n", "        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n", "        if dim not in common_dims:\n", "            common_dims = (dim,) + common_dims\n", "        for var, dim_len in zip(vars, concat_dim_lengths):\n", "            if var.dims != common_dims:\n", "                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n", "                var = var.set_dims(common_dims, common_shape)\n", "            yield var\n", "\n", "    # stack up each variable to fill-out the dataset (in order)\n", "    # n.b. this loop preserves variable order, needed for groupby.\n", "    for k in datasets[0].variables:\n", "        if k in concat_over:\n", "            try:\n", "                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n", "            except KeyError:\n", "                raise ValueError(\"%r is not present in all datasets.\" % k)\n", "            combined = concat_vars(vars, dim, positions)\n", "            assert isinstance(combined, Variable)\n", "            result_vars[k] = combined\n", "\n", "    result = Dataset(result_vars, attrs=result_attrs)\n", "    absent_coord_names = coord_names - set(result.variables)\n", "    if absent_coord_names:\n", "        raise ValueError(\n", "            \"Variables %r are coordinates in some datasets but not others.\"\n", "            % absent_coord_names\n", "        )\n", "    result = result.set_coords(coord_names)\n", "    result.encoding = result_encoding\n", "\n", "    result = result.drop(unlabeled_dims, errors=\"ignore\")\n", "\n", "    if coord is not None:\n", "        # add concat dimension last to ensure that its in the final Dataset\n", "        result[coord.name] = coord\n", "\n", "    return result\n"]}]}, {"file_path": "xarray/core/dataarray.py", "span_ids": ["DataArray.reindex"], "content": [{"span_id": "DataArray.reindex", "start_line": 1226, "end_line": 1288, "content": ["    def reindex(\n", "        self,\n", "        indexers: Mapping[Hashable, Any] = None,\n", "        method: str = None,\n", "        tolerance=None,\n", "        copy: bool = True,\n", "        fill_value=dtypes.NA,\n", "        **indexers_kwargs: Any\n", "    ) -> \"DataArray\":\n", "        \"\"\"Conform this object onto the indexes of another object, filling in\n", "        missing values with ``fill_value``. The default fill value is NaN.\n", "\n", "        Parameters\n", "        ----------\n", "        indexers : dict, optional\n", "            Dictionary with keys given by dimension names and values given by\n", "            arrays of coordinates tick labels. Any mis-matched coordinate\n", "            values will be filled in with NaN, and any mis-matched dimension\n", "            names will simply be ignored.\n", "            One of indexers or indexers_kwargs must be provided.\n", "        copy : bool, optional\n", "            If ``copy=True``, data in the return value is always copied. If\n", "            ``copy=False`` and reindexing is unnecessary, or can be performed\n", "            with only slice operations, then the output may share memory with\n", "            the input. In either case, a new xarray object is always returned.\n", "        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n", "            Method to use for filling index values in ``indexers`` not found on\n", "            this data array:\n", "\n", "            * None (default): don't fill gaps\n", "            * pad / ffill: propagate last valid index value forward\n", "            * backfill / bfill: propagate next valid index value backward\n", "            * nearest: use nearest valid index value\n", "        tolerance : optional\n", "            Maximum distance between original and new labels for inexact\n", "            matches. The values of the index at the matching locations must\n", "            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n", "        fill_value : scalar, optional\n", "            Value to use for newly missing values\n", "        **indexers_kwarg : {dim: indexer, ...}, optional\n", "            The keyword arguments form of ``indexers``.\n", "            One of indexers or indexers_kwargs must be provided.\n", "\n", "        Returns\n", "        -------\n", "        reindexed : DataArray\n", "            Another dataset array, with this array's data but replaced\n", "            coordinates.\n", "\n", "        See Also\n", "        --------\n", "        DataArray.reindex_like\n", "        align\n", "        \"\"\"\n", "        indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n", "        ds = self._to_temp_dataset().reindex(\n", "            indexers=indexers,\n", "            method=method,\n", "            tolerance=tolerance,\n", "            copy=copy,\n", "            fill_value=fill_value,\n", "        )\n", "        return self._from_temp_dataset(ds)\n"]}]}, {"file_path": "xarray/core/dataset.py", "span_ids": ["Dataset.reindex"], "content": [{"span_id": "Dataset.reindex", "start_line": 2236, "end_line": 2436, "content": ["    def reindex(\n", "        self,\n", "        indexers: Mapping[Hashable, Any] = None,\n", "        method: str = None,\n", "        tolerance: Number = None,\n", "        copy: bool = True,\n", "        fill_value: Any = dtypes.NA,\n", "        **indexers_kwargs: Any,\n", "    ) -> \"Dataset\":\n", "        \"\"\"Conform this object onto a new set of indexes, filling in\n", "        missing values with ``fill_value``. The default fill value is NaN.\n", "\n", "        Parameters\n", "        ----------\n", "        indexers : dict. optional\n", "            Dictionary with keys given by dimension names and values given by\n", "            arrays of coordinates tick labels. Any mis-matched coordinate\n", "            values will be filled in with NaN, and any mis-matched dimension\n", "            names will simply be ignored.\n", "            One of indexers or indexers_kwargs must be provided.\n", "        method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n", "            Method to use for filling index values in ``indexers`` not found in\n", "            this dataset:\n", "\n", "            * None (default): don't fill gaps\n", "            * pad / ffill: propagate last valid index value forward\n", "            * backfill / bfill: propagate next valid index value backward\n", "            * nearest: use nearest valid index value\n", "        tolerance : optional\n", "            Maximum distance between original and new labels for inexact\n", "            matches. The values of the index at the matching locations must\n", "            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n", "        copy : bool, optional\n", "            If ``copy=True``, data in the return value is always copied. If\n", "            ``copy=False`` and reindexing is unnecessary, or can be performed\n", "            with only slice operations, then the output may share memory with\n", "            the input. In either case, a new xarray object is always returned.\n", "        fill_value : scalar, optional\n", "            Value to use for newly missing values\n", "        **indexers_kwarg : {dim: indexer, ...}, optional\n", "            Keyword arguments in the same form as ``indexers``.\n", "            One of indexers or indexers_kwargs must be provided.\n", "\n", "        Returns\n", "        -------\n", "        reindexed : Dataset\n", "            Another dataset, with this dataset's data but replaced coordinates.\n", "\n", "        See Also\n", "        --------\n", "        Dataset.reindex_like\n", "        align\n", "        pandas.Index.get_indexer\n", "\n", "        Examples\n", "        --------\n", "\n", "        Create a dataset with some fictional data.\n", "\n", "        >>> import xarray as xr\n", "        >>> import pandas as pd\n", "        >>> x = xr.Dataset(\n", "        ...     {\n", "        ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n", "        ...         \"pressure\": (\"station\", 500 * np.random.rand(4))\n", "        ...     },\n", "        ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]})\n", "        >>> x\n", "        <xarray.Dataset>\n", "        Dimensions:      (station: 4)\n", "        Coordinates:\n", "        * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n", "        Data variables:\n", "            temperature  (station) float64 18.84 14.59 19.22 17.16\n", "            pressure     (station) float64 324.1 194.3 122.8 244.3\n", "        >>> x.indexes\n", "        station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n", "\n", "        Create a new index and reindex the dataset. By default values in the new index that\n", "        do not have corresponding records in the dataset are assigned `NaN`.\n", "\n", "        >>> new_index = ['boston', 'austin', 'seattle', 'lincoln']\n", "        >>> x.reindex({'station': new_index})\n", "        <xarray.Dataset>\n", "        Dimensions:      (station: 4)\n", "        Coordinates:\n", "        * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'\n", "        Data variables:\n", "            temperature  (station) float64 18.84 nan 19.22 nan\n", "            pressure     (station) float64 324.1 nan 122.8 nan\n", "\n", "        We can fill in the missing values by passing a value to the keyword `fill_value`.\n", "\n", "        >>> x.reindex({'station': new_index}, fill_value=0)\n", "        <xarray.Dataset>\n", "        Dimensions:      (station: 4)\n", "        Coordinates:\n", "        * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'\n", "        Data variables:\n", "            temperature  (station) float64 18.84 0.0 19.22 0.0\n", "            pressure     (station) float64 324.1 0.0 122.8 0.0\n", "\n", "        Because the index is not monotonically increasing or decreasing, we cannot use arguments\n", "        to the keyword method to fill the `NaN` values.\n", "\n", "        >>> x.reindex({'station': new_index}, method='nearest')\n", "        Traceback (most recent call last):\n", "        ...\n", "            raise ValueError('index must be monotonic increasing or decreasing')\n", "        ValueError: index must be monotonic increasing or decreasing\n", "\n", "        To further illustrate the filling functionality in reindex, we will create a\n", "        dataset with a monotonically increasing index (for example, a sequence of dates).\n", "\n", "        >>> x2 = xr.Dataset(\n", "        ...     {\n", "        ...         \"temperature\": (\"time\", [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12]),\n", "        ...         \"pressure\": (\"time\", 500 * np.random.rand(6))\n", "        ...     },\n", "        ...     coords={\"time\": pd.date_range('01/01/2019', periods=6, freq='D')})\n", "        >>> x2\n", "        <xarray.Dataset>\n", "        Dimensions:      (time: 6)\n", "        Coordinates:\n", "        * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n", "        Data variables:\n", "            temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n", "            pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0\n", "\n", "        Suppose we decide to expand the dataset to cover a wider date range.\n", "\n", "        >>> time_index2 = pd.date_range('12/29/2018', periods=10, freq='D')\n", "        >>> x2.reindex({'time': time_index2})\n", "        <xarray.Dataset>\n", "        Dimensions:      (time: 10)\n", "        Coordinates:\n", "        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n", "        Data variables:\n", "            temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n", "            pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan\n", "\n", "        The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n", "        are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n", "\n", "        For example, to back-propagate the last valid value to fill the `NaN` values,\n", "        pass `bfill` as an argument to the `method` keyword.\n", "\n", "        >>> x3 = x2.reindex({'time': time_index2}, method='bfill')\n", "        >>> x3\n", "        <xarray.Dataset>\n", "        Dimensions:      (time: 10)\n", "        Coordinates:\n", "        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n", "        Data variables:\n", "            temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n", "            pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan\n", "\n", "        Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n", "        will not be filled by any of the value propagation schemes.\n", "\n", "        >>> x2.where(x2.temperature.isnull(), drop=True)\n", "        <xarray.Dataset>\n", "        Dimensions:      (time: 1)\n", "        Coordinates:\n", "        * time         (time) datetime64[ns] 2019-01-03\n", "        Data variables:\n", "            temperature  (time) float64 nan\n", "            pressure     (time) float64 452.0\n", "        >>> x3.where(x3.temperature.isnull(), drop=True)\n", "        <xarray.Dataset>\n", "        Dimensions:      (time: 2)\n", "        Coordinates:\n", "        * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n", "        Data variables:\n", "            temperature  (time) float64 nan nan\n", "            pressure     (time) float64 452.0 nan\n", "\n", "        This is because filling while reindexing does not look at dataset values, but only compares\n", "        the original and desired indexes. If you do want to fill in the `NaN` values present in the\n", "        original dataset, use the :py:meth:`~Dataset.fillna()` method.\n", "\n", "        \"\"\"\n", "        indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n", "\n", "        bad_dims = [d for d in indexers if d not in self.dims]\n", "        if bad_dims:\n", "            raise ValueError(\"invalid reindex dimensions: %s\" % bad_dims)\n", "\n", "        variables, indexes = alignment.reindex_variables(\n", "            self.variables,\n", "            self.sizes,\n", "            self.indexes,\n", "            indexers,\n", "            method,\n", "            tolerance,\n", "            copy=copy,\n", "            fill_value=fill_value,\n", "        )\n", "        coord_names = set(self._coord_names)\n", "        coord_names.update(indexers)\n", "        return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n"]}]}, {"file_path": "xarray/core/variable.py", "span_ids": ["concat", "IndexVariable.concat", "Variable.concat"], "content": [{"span_id": "concat", "start_line": 2198, "end_line": 2232, "content": ["def concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n", "    \"\"\"Concatenate variables along a new or existing dimension.\n", "\n", "    Parameters\n", "    ----------\n", "    variables : iterable of Array\n", "        Arrays to stack together. Each variable is expected to have\n", "        matching dimensions and shape except for along the stacked\n", "        dimension.\n", "    dim : str or DataArray, optional\n", "        Name of the dimension to stack along. This can either be a new\n", "        dimension name, in which case it is added along axis=0, or an\n", "        existing dimension name, in which case the location of the\n", "        dimension is unchanged. Where to insert the new dimension is\n", "        determined by the first variable.\n", "    positions : None or list of integer arrays, optional\n", "        List of integer arrays which specifies the integer positions to which\n", "        to assign each dataset along the concatenated dimension. If not\n", "        supplied, objects are concatenated in the provided order.\n", "    shortcut : bool, optional\n", "        This option is used internally to speed-up groupby operations.\n", "        If `shortcut` is True, some checks of internal consistency between\n", "        arrays to concatenate are skipped.\n", "\n", "    Returns\n", "    -------\n", "    stacked : Variable\n", "        Concatenated Variable formed by stacking all the supplied variables\n", "        along the given dimension.\n", "    \"\"\"\n", "    variables = list(variables)\n", "    if all(isinstance(v, IndexVariable) for v in variables):\n", "        return IndexVariable.concat(variables, dim, positions, shortcut)\n", "    else:\n", "        return Variable.concat(variables, dim, positions, shortcut)\n"]}, {"span_id": "IndexVariable.concat", "start_line": 1989, "end_line": 2026, "content": ["    @classmethod\n", "    def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n", "        \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n", "\n", "        This exists because we want to avoid converting Index objects to NumPy\n", "        arrays, if possible.\n", "        \"\"\"\n", "        if not isinstance(dim, str):\n", "            dim, = dim.dims\n", "\n", "        variables = list(variables)\n", "        first_var = variables[0]\n", "\n", "        if any(not isinstance(v, cls) for v in variables):\n", "            raise TypeError(\n", "                \"IndexVariable.concat requires that all input \"\n", "                \"variables be IndexVariable objects\"\n", "            )\n", "\n", "        indexes = [v._data.array for v in variables]\n", "\n", "        if not indexes:\n", "            data = []\n", "        else:\n", "            data = indexes[0].append(indexes[1:])\n", "\n", "            if positions is not None:\n", "                indices = nputils.inverse_permutation(np.concatenate(positions))\n", "                data = data.take(indices)\n", "\n", "        attrs = dict(first_var.attrs)\n", "        if not shortcut:\n", "            for var in variables:\n", "                if var.dims != first_var.dims:\n", "                    raise ValueError(\"inconsistent dimensions\")\n", "                utils.remove_incompatible_items(attrs, var.attrs)\n", "\n", "        return cls(first_var.dims, data, attrs)\n"]}, {"span_id": "Variable.concat", "start_line": 1495, "end_line": 1558, "content": ["    @classmethod\n", "    def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n", "        \"\"\"Concatenate variables along a new or existing dimension.\n", "\n", "        Parameters\n", "        ----------\n", "        variables : iterable of Array\n", "            Arrays to stack together. Each variable is expected to have\n", "            matching dimensions and shape except for along the stacked\n", "            dimension.\n", "        dim : str or DataArray, optional\n", "            Name of the dimension to stack along. This can either be a new\n", "            dimension name, in which case it is added along axis=0, or an\n", "            existing dimension name, in which case the location of the\n", "            dimension is unchanged. Where to insert the new dimension is\n", "            determined by the first variable.\n", "        positions : None or list of integer arrays, optional\n", "            List of integer arrays which specifies the integer positions to\n", "            which to assign each dataset along the concatenated dimension.\n", "            If not supplied, objects are concatenated in the provided order.\n", "        shortcut : bool, optional\n", "            This option is used internally to speed-up groupby operations.\n", "            If `shortcut` is True, some checks of internal consistency between\n", "            arrays to concatenate are skipped.\n", "\n", "        Returns\n", "        -------\n", "        stacked : Variable\n", "            Concatenated Variable formed by stacking all the supplied variables\n", "            along the given dimension.\n", "        \"\"\"\n", "        if not isinstance(dim, str):\n", "            dim, = dim.dims\n", "\n", "        # can't do this lazily: we need to loop through variables at least\n", "        # twice\n", "        variables = list(variables)\n", "        first_var = variables[0]\n", "\n", "        arrays = [v.data for v in variables]\n", "\n", "        if dim in first_var.dims:\n", "            axis = first_var.get_axis_num(dim)\n", "            dims = first_var.dims\n", "            data = duck_array_ops.concatenate(arrays, axis=axis)\n", "            if positions is not None:\n", "                # TODO: deprecate this option -- we don't need it for groupby\n", "                # any more.\n", "                indices = nputils.inverse_permutation(np.concatenate(positions))\n", "                data = duck_array_ops.take(data, indices, axis=axis)\n", "        else:\n", "            axis = 0\n", "            dims = (dim,) + first_var.dims\n", "            data = duck_array_ops.stack(arrays, axis=axis)\n", "\n", "        attrs = dict(first_var.attrs)\n", "        encoding = dict(first_var.encoding)\n", "        if not shortcut:\n", "            for var in variables:\n", "                if var.dims != first_var.dims:\n", "                    raise ValueError(\"inconsistent dimensions\")\n", "                utils.remove_incompatible_items(attrs, var.attrs)\n", "\n", "        return cls(dims, data, attrs, encoding)\n"]}]}]}