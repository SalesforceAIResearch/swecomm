{"instance_id": "scikit-learn__scikit-learn-14087", "identified_spans": [{"file_path": "sklearn/linear_model/logistic.py", "span_ids": ["LogisticRegressionCV.fit"], "content": [{"span_id": "LogisticRegressionCV.fit", "start_line": 1936, "end_line": 2212, "content": ["    def fit(self, X, y, sample_weight=None):\n", "        \"\"\"Fit the model according to the given training data.\n", "\n", "        Parameters\n", "        ----------\n", "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n", "            Training vector, where n_samples is the number of samples and\n", "            n_features is the number of features.\n", "\n", "        y : array-like, shape (n_samples,)\n", "            Target vector relative to X.\n", "\n", "        sample_weight : array-like, shape (n_samples,) optional\n", "            Array of weights that are assigned to individual samples.\n", "            If not provided, then each sample is given unit weight.\n", "\n", "        Returns\n", "        -------\n", "        self : object\n", "        \"\"\"\n", "        solver = _check_solver(self.solver, self.penalty, self.dual)\n", "\n", "        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n", "            raise ValueError(\"Maximum number of iteration must be positive;\"\n", "                             \" got (max_iter=%r)\" % self.max_iter)\n", "        if not isinstance(self.tol, numbers.Number) or self.tol < 0:\n", "            raise ValueError(\"Tolerance for stopping criteria must be \"\n", "                             \"positive; got (tol=%r)\" % self.tol)\n", "        if self.penalty == 'elasticnet':\n", "            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(\n", "                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0\n", "                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):\n", "                raise ValueError(\"l1_ratios must be a list of numbers between \"\n", "                                 \"0 and 1; got (l1_ratios=%r)\" %\n", "                                 self.l1_ratios)\n", "            l1_ratios_ = self.l1_ratios\n", "        else:\n", "            if self.l1_ratios is not None:\n", "                warnings.warn(\"l1_ratios parameter is only used when penalty \"\n", "                              \"is 'elasticnet'. Got (penalty={})\".format(\n", "                                  self.penalty))\n", "\n", "            l1_ratios_ = [None]\n", "\n", "        if self.penalty == 'none':\n", "            raise ValueError(\n", "                \"penalty='none' is not useful and not supported by \"\n", "                \"LogisticRegressionCV.\"\n", "            )\n", "\n", "        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n", "                         order=\"C\",\n", "                         accept_large_sparse=solver != 'liblinear')\n", "        check_classification_targets(y)\n", "\n", "        class_weight = self.class_weight\n", "\n", "        # Encode for string labels\n", "        label_encoder = LabelEncoder().fit(y)\n", "        y = label_encoder.transform(y)\n", "        if isinstance(class_weight, dict):\n", "            class_weight = {label_encoder.transform([cls])[0]: v\n", "                            for cls, v in class_weight.items()}\n", "\n", "        # The original class labels\n", "        classes = self.classes_ = label_encoder.classes_\n", "        encoded_labels = label_encoder.transform(label_encoder.classes_)\n", "\n", "        multi_class = _check_multi_class(self.multi_class, solver,\n", "                                         len(classes))\n", "\n", "        if solver in ['sag', 'saga']:\n", "            max_squared_sum = row_norms(X, squared=True).max()\n", "        else:\n", "            max_squared_sum = None\n", "\n", "        # init cross-validation generator\n", "        cv = check_cv(self.cv, y, classifier=True)\n", "        folds = list(cv.split(X, y))\n", "\n", "        # Use the label encoded classes\n", "        n_classes = len(encoded_labels)\n", "\n", "        if n_classes < 2:\n", "            raise ValueError(\"This solver needs samples of at least 2 classes\"\n", "                             \" in the data, but the data contains only one\"\n", "                             \" class: %r\" % classes[0])\n", "\n", "        if n_classes == 2:\n", "            # OvR in case of binary problems is as good as fitting\n", "            # the higher label\n", "            n_classes = 1\n", "            encoded_labels = encoded_labels[1:]\n", "            classes = classes[1:]\n", "\n", "        # We need this hack to iterate only once over labels, in the case of\n", "        # multi_class = multinomial, without changing the value of the labels.\n", "        if multi_class == 'multinomial':\n", "            iter_encoded_labels = iter_classes = [None]\n", "        else:\n", "            iter_encoded_labels = encoded_labels\n", "            iter_classes = classes\n", "\n", "        # compute the class weights for the entire dataset y\n", "        if class_weight == \"balanced\":\n", "            class_weight = compute_class_weight(class_weight,\n", "                                                np.arange(len(self.classes_)),\n", "                                                y)\n", "            class_weight = dict(enumerate(class_weight))\n", "\n", "        path_func = delayed(_log_reg_scoring_path)\n", "\n", "        # The SAG solver releases the GIL so it's more efficient to use\n", "        # threads for this solver.\n", "        if self.solver in ['sag', 'saga']:\n", "            prefer = 'threads'\n", "        else:\n", "            prefer = 'processes'\n", "\n", "        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n", "                               **_joblib_parallel_args(prefer=prefer))(\n", "            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,\n", "                      fit_intercept=self.fit_intercept, penalty=self.penalty,\n", "                      dual=self.dual, solver=solver, tol=self.tol,\n", "                      max_iter=self.max_iter, verbose=self.verbose,\n", "                      class_weight=class_weight, scoring=self.scoring,\n", "                      multi_class=multi_class,\n", "                      intercept_scaling=self.intercept_scaling,\n", "                      random_state=self.random_state,\n", "                      max_squared_sum=max_squared_sum,\n", "                      sample_weight=sample_weight,\n", "                      l1_ratio=l1_ratio\n", "                      )\n", "            for label in iter_encoded_labels\n", "            for train, test in folds\n", "            for l1_ratio in l1_ratios_)\n", "\n", "        # _log_reg_scoring_path will output different shapes depending on the\n", "        # multi_class param, so we need to reshape the outputs accordingly.\n", "        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\n", "        # rows are equal, so we just take the first one.\n", "        # After reshaping,\n", "        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)\n", "        # - coefs_paths is of shape\n", "        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)\n", "        # - n_iter is of shape\n", "        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or\n", "        #  (1, n_folds, n_Cs . n_l1_ratios)\n", "        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n", "        self.Cs_ = Cs[0]\n", "        if multi_class == 'multinomial':\n", "            coefs_paths = np.reshape(\n", "                coefs_paths,\n", "                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)\n", "            )\n", "            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\n", "            #                                                 (1, 2, 0, 3))\n", "            coefs_paths = np.swapaxes(coefs_paths, 0, 1)\n", "            coefs_paths = np.swapaxes(coefs_paths, 0, 2)\n", "            self.n_iter_ = np.reshape(\n", "                n_iter_,\n", "                (1, len(folds), len(self.Cs_) * len(l1_ratios_))\n", "            )\n", "            # repeat same scores across all classes\n", "            scores = np.tile(scores, (n_classes, 1, 1))\n", "        else:\n", "            coefs_paths = np.reshape(\n", "                coefs_paths,\n", "                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),\n", "                 -1)\n", "            )\n", "            self.n_iter_ = np.reshape(\n", "                n_iter_,\n", "                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))\n", "            )\n", "        scores = np.reshape(scores, (n_classes, len(folds), -1))\n", "        self.scores_ = dict(zip(classes, scores))\n", "        self.coefs_paths_ = dict(zip(classes, coefs_paths))\n", "\n", "        self.C_ = list()\n", "        self.l1_ratio_ = list()\n", "        self.coef_ = np.empty((n_classes, X.shape[1]))\n", "        self.intercept_ = np.zeros(n_classes)\n", "        for index, (cls, encoded_label) in enumerate(\n", "                zip(iter_classes, iter_encoded_labels)):\n", "\n", "            if multi_class == 'ovr':\n", "                scores = self.scores_[cls]\n", "                coefs_paths = self.coefs_paths_[cls]\n", "            else:\n", "                # For multinomial, all scores are the same across classes\n", "                scores = scores[0]\n", "                # coefs_paths will keep its original shape because\n", "                # logistic_regression_path expects it this way\n", "\n", "            if self.refit:\n", "                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)\n", "                # for example, with n_cs=2 and n_l1_ratios=3\n", "                # the layout of scores is\n", "                # [c1, c2, c1, c2, c1, c2]\n", "                #   l1_1 ,  l1_2 ,  l1_3\n", "                best_index = scores.sum(axis=0).argmax()\n", "\n", "                best_index_C = best_index % len(self.Cs_)\n", "                C_ = self.Cs_[best_index_C]\n", "                self.C_.append(C_)\n", "\n", "                best_index_l1 = best_index // len(self.Cs_)\n", "                l1_ratio_ = l1_ratios_[best_index_l1]\n", "                self.l1_ratio_.append(l1_ratio_)\n", "\n", "                if multi_class == 'multinomial':\n", "                    coef_init = np.mean(coefs_paths[:, :, best_index, :],\n", "                                        axis=1)\n", "                else:\n", "                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)\n", "\n", "                # Note that y is label encoded and hence pos_class must be\n", "                # the encoded label / None (for 'multinomial')\n", "                w, _, _ = _logistic_regression_path(\n", "                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,\n", "                    fit_intercept=self.fit_intercept, coef=coef_init,\n", "                    max_iter=self.max_iter, tol=self.tol,\n", "                    penalty=self.penalty,\n", "                    class_weight=class_weight,\n", "                    multi_class=multi_class,\n", "                    verbose=max(0, self.verbose - 1),\n", "                    random_state=self.random_state,\n", "                    check_input=False, max_squared_sum=max_squared_sum,\n", "                    sample_weight=sample_weight,\n", "                    l1_ratio=l1_ratio_)\n", "                w = w[0]\n", "\n", "            else:\n", "                # Take the best scores across every fold and the average of\n", "                # all coefficients corresponding to the best scores.\n", "                best_indices = np.argmax(scores, axis=1)\n", "                if self.multi_class == 'ovr':\n", "                    w = np.mean([coefs_paths[i, best_indices[i], :]\n", "                                 for i in range(len(folds))], axis=0)\n", "                else:\n", "                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n", "                                 for i in range(len(folds))], axis=0)\n", "\n", "                best_indices_C = best_indices % len(self.Cs_)\n", "                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n", "\n", "                best_indices_l1 = best_indices // len(self.Cs_)\n", "                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n", "\n", "            if multi_class == 'multinomial':\n", "                self.C_ = np.tile(self.C_, n_classes)\n", "                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n", "                self.coef_ = w[:, :X.shape[1]]\n", "                if self.fit_intercept:\n", "                    self.intercept_ = w[:, -1]\n", "            else:\n", "                self.coef_[index] = w[: X.shape[1]]\n", "                if self.fit_intercept:\n", "                    self.intercept_[index] = w[-1]\n", "\n", "        self.C_ = np.asarray(self.C_)\n", "        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n", "        self.l1_ratios_ = np.asarray(l1_ratios_)\n", "        # if elasticnet was used, add the l1_ratios dimension to some\n", "        # attributes\n", "        if self.l1_ratios is not None:\n", "            for cls, coefs_path in self.coefs_paths_.items():\n", "                self.coefs_paths_[cls] = coefs_path.reshape(\n", "                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))\n", "            for cls, score in self.scores_.items():\n", "                self.scores_[cls] = score.reshape(\n", "                    (len(folds), self.Cs_.size, self.l1_ratios_.size))\n", "            self.n_iter_ = self.n_iter_.reshape(\n", "                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))\n", "\n", "        return self\n"]}]}]}