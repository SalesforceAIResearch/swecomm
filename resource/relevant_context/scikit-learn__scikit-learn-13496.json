{"instance_id": "scikit-learn__scikit-learn-13496", "identified_spans": [{"file_path": "sklearn/ensemble/bagging.py", "span_ids": ["BaseBagging.__init__", "BaseBagging"], "content": [{"span_id": "BaseBagging.__init__", "start_line": 191, "end_line": 216, "content": ["    @abstractmethod\n", "    def __init__(self,\n", "                 base_estimator=None,\n", "                 n_estimators=10,\n", "                 max_samples=1.0,\n", "                 max_features=1.0,\n", "                 bootstrap=True,\n", "                 bootstrap_features=False,\n", "                 oob_score=False,\n", "                 warm_start=False,\n", "                 n_jobs=None,\n", "                 random_state=None,\n", "                 verbose=0):\n", "        super().__init__(\n", "            base_estimator=base_estimator,\n", "            n_estimators=n_estimators)\n", "\n", "        self.max_samples = max_samples\n", "        self.max_features = max_features\n", "        self.bootstrap = bootstrap\n", "        self.bootstrap_features = bootstrap_features\n", "        self.oob_score = oob_score\n", "        self.warm_start = warm_start\n", "        self.n_jobs = n_jobs\n", "        self.random_state = random_state\n", "        self.verbose = verbose\n"]}, {"span_id": "BaseBagging", "start_line": 184, "end_line": 189, "content": ["class BaseBagging(BaseEnsemble, metaclass=ABCMeta):\n", "    \"\"\"Base class for Bagging meta-estimator.\n", "\n", "    Warning: This class should not be used directly. Use derived classes\n", "    instead.\n", "    \"\"\"\n"]}]}, {"file_path": "sklearn/ensemble/iforest.py", "span_ids": ["IsolationForest.__init__", "IsolationForest"], "content": [{"span_id": "IsolationForest.__init__", "start_line": 167, "end_line": 193, "content": ["    def __init__(self,\n", "                 n_estimators=100,\n", "                 max_samples=\"auto\",\n", "                 contamination=\"legacy\",\n", "                 max_features=1.,\n", "                 bootstrap=False,\n", "                 n_jobs=None,\n", "                 behaviour='old',\n", "                 random_state=None,\n", "                 verbose=0):\n", "        super().__init__(\n", "            base_estimator=ExtraTreeRegressor(\n", "                max_features=1,\n", "                splitter='random',\n", "                random_state=random_state),\n", "            # here above max_features has no links with self.max_features\n", "            bootstrap=bootstrap,\n", "            bootstrap_features=False,\n", "            n_estimators=n_estimators,\n", "            max_samples=max_samples,\n", "            max_features=max_features,\n", "            n_jobs=n_jobs,\n", "            random_state=random_state,\n", "            verbose=verbose)\n", "\n", "        self.behaviour = behaviour\n", "        self.contamination = contamination\n"]}, {"span_id": "IsolationForest", "start_line": 29, "end_line": 165, "content": ["class IsolationForest(BaseBagging, OutlierMixin):\n", "    \"\"\"Isolation Forest Algorithm\n", "\n", "    Return the anomaly score of each sample using the IsolationForest algorithm\n", "\n", "    The IsolationForest 'isolates' observations by randomly selecting a feature\n", "    and then randomly selecting a split value between the maximum and minimum\n", "    values of the selected feature.\n", "\n", "    Since recursive partitioning can be represented by a tree structure, the\n", "    number of splittings required to isolate a sample is equivalent to the path\n", "    length from the root node to the terminating node.\n", "\n", "    This path length, averaged over a forest of such random trees, is a\n", "    measure of normality and our decision function.\n", "\n", "    Random partitioning produces noticeably shorter paths for anomalies.\n", "    Hence, when a forest of random trees collectively produce shorter path\n", "    lengths for particular samples, they are highly likely to be anomalies.\n", "\n", "    Read more in the :ref:`User Guide <isolation_forest>`.\n", "\n", "    .. versionadded:: 0.18\n", "\n", "    Parameters\n", "    ----------\n", "    n_estimators : int, optional (default=100)\n", "        The number of base estimators in the ensemble.\n", "\n", "    max_samples : int or float, optional (default=\"auto\")\n", "        The number of samples to draw from X to train each base estimator.\n", "            - If int, then draw `max_samples` samples.\n", "            - If float, then draw `max_samples * X.shape[0]` samples.\n", "            - If \"auto\", then `max_samples=min(256, n_samples)`.\n", "\n", "        If max_samples is larger than the number of samples provided,\n", "        all samples will be used for all trees (no sampling).\n", "\n", "    contamination : float in (0., 0.5), optional (default=0.1)\n", "        The amount of contamination of the data set, i.e. the proportion\n", "        of outliers in the data set. Used when fitting to define the threshold\n", "        on the decision function. If 'auto', the decision function threshold is\n", "        determined as in the original paper.\n", "\n", "        .. versionchanged:: 0.20\n", "           The default value of ``contamination`` will change from 0.1 in 0.20\n", "           to ``'auto'`` in 0.22.\n", "\n", "    max_features : int or float, optional (default=1.0)\n", "        The number of features to draw from X to train each base estimator.\n", "\n", "            - If int, then draw `max_features` features.\n", "            - If float, then draw `max_features * X.shape[1]` features.\n", "\n", "    bootstrap : boolean, optional (default=False)\n", "        If True, individual trees are fit on random subsets of the training\n", "        data sampled with replacement. If False, sampling without replacement\n", "        is performed.\n", "\n", "    n_jobs : int or None, optional (default=None)\n", "        The number of jobs to run in parallel for both `fit` and `predict`.\n", "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n", "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n", "        for more details.\n", "\n", "    behaviour : str, default='old'\n", "        Behaviour of the ``decision_function`` which can be either 'old' or\n", "        'new'. Passing ``behaviour='new'`` makes the ``decision_function``\n", "        change to match other anomaly detection algorithm API which will be\n", "        the default behaviour in the future. As explained in details in the\n", "        ``offset_`` attribute documentation, the ``decision_function`` becomes\n", "        dependent on the contamination parameter, in such a way that 0 becomes\n", "        its natural threshold to detect outliers.\n", "\n", "        .. versionadded:: 0.20\n", "           ``behaviour`` is added in 0.20 for back-compatibility purpose.\n", "\n", "        .. deprecated:: 0.20\n", "           ``behaviour='old'`` is deprecated in 0.20 and will not be possible\n", "           in 0.22.\n", "\n", "        .. deprecated:: 0.22\n", "           ``behaviour`` parameter will be deprecated in 0.22 and removed in\n", "           0.24.\n", "\n", "    random_state : int, RandomState instance or None, optional (default=None)\n", "        If int, random_state is the seed used by the random number generator;\n", "        If RandomState instance, random_state is the random number generator;\n", "        If None, the random number generator is the RandomState instance used\n", "        by `np.random`.\n", "\n", "    verbose : int, optional (default=0)\n", "        Controls the verbosity of the tree building process.\n", "\n", "\n", "    Attributes\n", "    ----------\n", "    estimators_ : list of DecisionTreeClassifier\n", "        The collection of fitted sub-estimators.\n", "\n", "    estimators_samples_ : list of arrays\n", "        The subset of drawn samples (i.e., the in-bag samples) for each base\n", "        estimator.\n", "\n", "    max_samples_ : integer\n", "        The actual number of samples\n", "\n", "    offset_ : float\n", "        Offset used to define the decision function from the raw scores.\n", "        We have the relation: ``decision_function = score_samples - offset_``.\n", "        Assuming behaviour == 'new', ``offset_`` is defined as follows.\n", "        When the contamination parameter is set to \"auto\", the offset is equal\n", "        to -0.5 as the scores of inliers are close to 0 and the scores of\n", "        outliers are close to -1. When a contamination parameter different\n", "        than \"auto\" is provided, the offset is defined in such a way we obtain\n", "        the expected number of outliers (samples with decision function < 0)\n", "        in training.\n", "        Assuming the behaviour parameter is set to 'old', we always have\n", "        ``offset_ = -0.5``, making the decision function independent from the\n", "        contamination parameter.\n", "\n", "    Notes\n", "    -----\n", "    The implementation is based on an ensemble of ExtraTreeRegressor. The\n", "    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n", "    :math:`n` is the number of samples used to build the tree\n", "    (see (Liu et al., 2008) for more details).\n", "\n", "    References\n", "    ----------\n", "    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n", "           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n", "    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n", "           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n", "           Data (TKDD) 6.1 (2012): 3.\n", "\n", "    \"\"\"\n"]}]}]}