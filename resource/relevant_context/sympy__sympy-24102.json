{"instance_id": "sympy__sympy-24102", "identified_spans": [{"file_path": "sympy/parsing/mathematica.py", "span_ids": ["MathematicaParser", "MathematicaParser.parse", "parse_mathematica", "MathematicaParser._get_tokenizer", "MathematicaParser:55"], "content": [{"span_id": "MathematicaParser", "start_line": 109, "end_line": 243, "content": ["@_deco\n", "class MathematicaParser:\n", "    \"\"\"\n", "    An instance of this class converts a string of a Wolfram Mathematica\n", "    expression to a SymPy expression.\n", "\n", "    The main parser acts internally in three stages:\n", "\n", "    1. tokenizer: tokenizes the Mathematica expression and adds the missing *\n", "        operators. Handled by ``_from_mathematica_to_tokens(...)``\n", "    2. full form list: sort the list of strings output by the tokenizer into a\n", "        syntax tree of nested lists and strings, equivalent to Mathematica's\n", "        ``FullForm`` expression output. This is handled by the function\n", "        ``_from_tokens_to_fullformlist(...)``.\n", "    3. SymPy expression: the syntax tree expressed as full form list is visited\n", "        and the nodes with equivalent classes in SymPy are replaced. Unknown\n", "        syntax tree nodes are cast to SymPy ``Function`` objects. This is\n", "        handled by ``_from_fullformlist_to_sympy(...)``.\n", "\n", "    \"\"\"\n", "\n", "    # left: Mathematica, right: SymPy\n", "    CORRESPONDENCES = {\n", "        'Sqrt[x]': 'sqrt(x)',\n", "        'Exp[x]': 'exp(x)',\n", "        'Log[x]': 'log(x)',\n", "        'Log[x,y]': 'log(y,x)',\n", "        'Log2[x]': 'log(x,2)',\n", "        'Log10[x]': 'log(x,10)',\n", "        'Mod[x,y]': 'Mod(x,y)',\n", "        'Max[*x]': 'Max(*x)',\n", "        'Min[*x]': 'Min(*x)',\n", "        'Pochhammer[x,y]':'rf(x,y)',\n", "        'ArcTan[x,y]':'atan2(y,x)',\n", "        'ExpIntegralEi[x]': 'Ei(x)',\n", "        'SinIntegral[x]': 'Si(x)',\n", "        'CosIntegral[x]': 'Ci(x)',\n", "        'AiryAi[x]': 'airyai(x)',\n", "        'AiryAiPrime[x]': 'airyaiprime(x)',\n", "        'AiryBi[x]' :'airybi(x)',\n", "        'AiryBiPrime[x]' :'airybiprime(x)',\n", "        'LogIntegral[x]':' li(x)',\n", "        'PrimePi[x]': 'primepi(x)',\n", "        'Prime[x]': 'prime(x)',\n", "        'PrimeQ[x]': 'isprime(x)'\n", "    }\n", "\n", "    # trigonometric, e.t.c.\n", "    for arc, tri, h in product(('', 'Arc'), (\n", "            'Sin', 'Cos', 'Tan', 'Cot', 'Sec', 'Csc'), ('', 'h')):\n", "        fm = arc + tri + h + '[x]'\n", "        if arc:  # arc func\n", "            fs = 'a' + tri.lower() + h + '(x)'\n", "        else:    # non-arc func\n", "            fs = tri.lower() + h + '(x)'\n", "        CORRESPONDENCES.update({fm: fs})\n", "\n", "    REPLACEMENTS = {\n", "        ' ': '',\n", "        '^': '**',\n", "        '{': '[',\n", "        '}': ']',\n", "    }\n", "\n", "    RULES = {\n", "        # a single whitespace to '*'\n", "        'whitespace': (\n", "            re.compile(r'''\n", "                (?:(?<=[a-zA-Z\\d])|(?<=\\d\\.))     # a letter or a number\n", "                \\s+                               # any number of whitespaces\n", "                (?:(?=[a-zA-Z\\d])|(?=\\.\\d))       # a letter or a number\n", "                ''', re.VERBOSE),\n", "            '*'),\n", "\n", "        # add omitted '*' character\n", "        'add*_1': (\n", "            re.compile(r'''\n", "                (?:(?<=[])\\d])|(?<=\\d\\.))       # ], ) or a number\n", "                                                # ''\n", "                (?=[(a-zA-Z])                   # ( or a single letter\n", "                ''', re.VERBOSE),\n", "            '*'),\n", "\n", "        # add omitted '*' character (variable letter preceding)\n", "        'add*_2': (\n", "            re.compile(r'''\n", "                (?<=[a-zA-Z])       # a letter\n", "                \\(                  # ( as a character\n", "                (?=.)               # any characters\n", "                ''', re.VERBOSE),\n", "            '*('),\n", "\n", "        # convert 'Pi' to 'pi'\n", "        'Pi': (\n", "            re.compile(r'''\n", "                (?:\n", "                \\A|(?<=[^a-zA-Z])\n", "                )\n", "                Pi                  # 'Pi' is 3.14159... in Mathematica\n", "                (?=[^a-zA-Z])\n", "                ''', re.VERBOSE),\n", "            'pi'),\n", "    }\n", "\n", "    # Mathematica function name pattern\n", "    FM_PATTERN = re.compile(r'''\n", "                (?:\n", "                \\A|(?<=[^a-zA-Z])   # at the top or a non-letter\n", "                )\n", "                [A-Z][a-zA-Z\\d]*    # Function\n", "                (?=\\[)              # [ as a character\n", "                ''', re.VERBOSE)\n", "\n", "    # list or matrix pattern (for future usage)\n", "    ARG_MTRX_PATTERN = re.compile(r'''\n", "                \\{.*\\}\n", "                ''', re.VERBOSE)\n", "\n", "    # regex string for function argument pattern\n", "    ARGS_PATTERN_TEMPLATE = r'''\n", "                (?:\n", "                \\A|(?<=[^a-zA-Z])\n", "                )\n", "                {arguments}         # model argument like x, y,...\n", "                (?=[^a-zA-Z])\n", "                '''\n", "\n", "    # will contain transformed CORRESPONDENCES dictionary\n", "    TRANSLATIONS = {}  # type: tDict[tTuple[str, int], tDict[str, Any]]\n", "\n", "    # cache for a raw users' translation dictionary\n", "    cache_original = {}  # type: tDict[tTuple[str, int], tDict[str, Any]]\n", "\n", "    # cache for a compiled users' translation dictionary\n", "    cache_compiled = {}  # type: tDict[tTuple[str, int], tDict[str, Any]]\n"]}, {"span_id": "MathematicaParser.parse", "start_line": 533, "end_line": 537, "content": ["    def parse(self, s):\n", "        s2 = self._from_mathematica_to_tokens(s)\n", "        s3 = self._from_tokens_to_fullformlist(s2)\n", "        s4 = self._from_fullformlist_to_sympy(s3)\n", "        return s4\n"]}, {"span_id": "parse_mathematica", "start_line": 31, "end_line": 82, "content": ["def parse_mathematica(s):\n", "    \"\"\"\n", "    Translate a string containing a Wolfram Mathematica expression to a SymPy\n", "    expression.\n", "\n", "    If the translator is unable to find a suitable SymPy expression, the\n", "    ``FullForm`` of the Mathematica expression will be output, using SymPy\n", "    ``Function`` objects as nodes of the syntax tree.\n", "\n", "    Examples\n", "    ========\n", "\n", "    >>> from sympy.parsing.mathematica import parse_mathematica\n", "    >>> parse_mathematica(\"Sin[x]^2 Tan[y]\")\n", "    sin(x)**2*tan(y)\n", "    >>> e = parse_mathematica(\"F[7,5,3]\")\n", "    >>> e\n", "    F(7, 5, 3)\n", "    >>> from sympy import Function, Max, Min\n", "    >>> e.replace(Function(\"F\"), lambda *x: Max(*x)*Min(*x))\n", "    21\n", "\n", "    Both standard input form and Mathematica full form are supported:\n", "\n", "    >>> parse_mathematica(\"x*(a + b)\")\n", "    x*(a + b)\n", "    >>> parse_mathematica(\"Times[x, Plus[a, b]]\")\n", "    x*(a + b)\n", "\n", "    To get a matrix from Wolfram's code:\n", "\n", "    >>> m = parse_mathematica(\"{{a, b}, {c, d}}\")\n", "    >>> m\n", "    ((a, b), (c, d))\n", "    >>> from sympy import Matrix\n", "    >>> Matrix(m)\n", "    Matrix([\n", "    [a, b],\n", "    [c, d]])\n", "\n", "    If the translation into equivalent SymPy expressions fails, an SymPy\n", "    expression equivalent to Wolfram Mathematica's \"FullForm\" will be created:\n", "\n", "    >>> parse_mathematica(\"x_.\")\n", "    Optional(Pattern(x, Blank()))\n", "    >>> parse_mathematica(\"Plus @@ {x, y, z}\")\n", "    Apply(Plus, (x, y, z))\n", "    >>> parse_mathematica(\"f[x_, 3] := x^3 /; x > 0\")\n", "    SetDelayed(f(Pattern(x, Blank()), 3), Condition(x**3, x > 0))\n", "    \"\"\"\n", "    parser = MathematicaParser()\n", "    return parser.parse(s)\n"]}, {"span_id": "MathematicaParser._get_tokenizer", "start_line": 605, "end_line": 620, "content": ["    def _get_tokenizer(self):\n", "        if self._regex_tokenizer is not None:\n", "            # Check if the regular expression has already been compiled:\n", "            return self._regex_tokenizer\n", "        tokens = [self._literal, self._number]\n", "        tokens_escape = self._enclosure_open[:] + self._enclosure_close[:]\n", "        for typ, strat, symdict in self._mathematica_op_precedence:\n", "            for k in symdict:\n", "                tokens_escape.append(k)\n", "        tokens_escape.sort(key=lambda x: -len(x))\n", "        tokens.extend(map(re.escape, tokens_escape))\n", "        tokens.append(\",\")\n", "        tokens.append(\"\\n\")\n", "        tokenizer = re.compile(\"(\" + \"|\".join(tokens) + \")\")\n", "        self._regex_tokenizer = tokenizer\n", "        return self._regex_tokenizer\n"]}, {"span_id": "MathematicaParser:55", "start_line": 603, "end_line": 603, "content": ["    _regex_tokenizer = None\n"]}]}]}