{"instance_id": "mwaskom__seaborn-3190", "identified_spans": [{"file_path": "seaborn/_core/plot.py", "span_ids": ["Plotter._setup_scales", "Plotter", "Plot._plot"], "content": [{"span_id": "Plotter._setup_scales", "start_line": 1171, "end_line": 1293, "content": ["    def _setup_scales(\n", "        self, p: Plot,\n", "        common: PlotData,\n", "        layers: list[Layer],\n", "        variables: list[str] | None = None,\n", "    ) -> None:\n", "\n", "        if variables is None:\n", "            # Add variables that have data but not a scale, which happens\n", "            # because this method can be called multiple time, to handle\n", "            # variables added during the Stat transform.\n", "            variables = []\n", "            for layer in layers:\n", "                variables.extend(layer[\"data\"].frame.columns)\n", "                for df in layer[\"data\"].frames.values():\n", "                    variables.extend(str(v) for v in df if v not in variables)\n", "            variables = [v for v in variables if v not in self._scales]\n", "\n", "        for var in variables:\n", "\n", "            # Determine whether this is a coordinate variable\n", "            # (i.e., x/y, paired x/y, or derivative such as xmax)\n", "            m = re.match(r\"^(?P<coord>(?P<axis>x|y)\\d*).*\", var)\n", "            if m is None:\n", "                coord = axis = None\n", "            else:\n", "                coord = m[\"coord\"]\n", "                axis = m[\"axis\"]\n", "\n", "            # Get keys that handle things like x0, xmax, properly where relevant\n", "            prop_key = var if axis is None else axis\n", "            scale_key = var if coord is None else coord\n", "\n", "            if prop_key not in PROPERTIES:\n", "                continue\n", "\n", "            # Concatenate layers, using only the relevant coordinate and faceting vars,\n", "            # This is unnecessarily wasteful, as layer data will often be redundant.\n", "            # But figuring out the minimal amount we need is more complicated.\n", "            cols = [var, \"col\", \"row\"]\n", "            parts = [common.frame.filter(cols)]\n", "            for layer in layers:\n", "                parts.append(layer[\"data\"].frame.filter(cols))\n", "                for df in layer[\"data\"].frames.values():\n", "                    parts.append(df.filter(cols))\n", "            var_df = pd.concat(parts, ignore_index=True)\n", "\n", "            prop = PROPERTIES[prop_key]\n", "            scale = self._get_scale(p, scale_key, prop, var_df[var])\n", "\n", "            if scale_key not in p._variables:\n", "                # TODO this implies that the variable was added by the stat\n", "                # It allows downstream orientation inference to work properly.\n", "                # But it feels rather hacky, so ideally revisit.\n", "                scale._priority = 0  # type: ignore\n", "\n", "            if axis is None:\n", "                # We could think about having a broader concept of (un)shared properties\n", "                # In general, not something you want to do (different scales in facets)\n", "                # But could make sense e.g. with paired plots. Build later.\n", "                share_state = None\n", "                subplots = []\n", "            else:\n", "                share_state = self._subplots.subplot_spec[f\"share{axis}\"]\n", "                subplots = [view for view in self._subplots if view[axis] == coord]\n", "\n", "            # Shared categorical axes are broken on matplotlib<3.4.0.\n", "            # https://github.com/matplotlib/matplotlib/pull/18308\n", "            # This only affects us when sharing *paired* axes. This is a novel/niche\n", "            # behavior, so we will raise rather than hack together a workaround.\n", "            if axis is not None and Version(mpl.__version__) < Version(\"3.4.0\"):\n", "                paired_axis = axis in p._pair_spec.get(\"structure\", {})\n", "                cat_scale = isinstance(scale, Nominal)\n", "                ok_dim = {\"x\": \"col\", \"y\": \"row\"}[axis]\n", "                shared_axes = share_state not in [False, \"none\", ok_dim]\n", "                if paired_axis and cat_scale and shared_axes:\n", "                    err = \"Sharing paired categorical axes requires matplotlib>=3.4.0\"\n", "                    raise RuntimeError(err)\n", "\n", "            if scale is None:\n", "                self._scales[var] = Scale._identity()\n", "            else:\n", "                try:\n", "                    self._scales[var] = scale._setup(var_df[var], prop)\n", "                except Exception as err:\n", "                    raise PlotSpecError._during(\"Scale setup\", var) from err\n", "\n", "            if axis is None or (var != coord and coord in p._variables):\n", "                # Everything below here applies only to coordinate variables\n", "                continue\n", "\n", "            # Set up an empty series to receive the transformed values.\n", "            # We need this to handle piecemeal transforms of categories -> floats.\n", "            transformed_data = []\n", "            for layer in layers:\n", "                index = layer[\"data\"].frame.index\n", "                empty_series = pd.Series(dtype=float, index=index, name=var)\n", "                transformed_data.append(empty_series)\n", "\n", "            for view in subplots:\n", "\n", "                axis_obj = getattr(view[\"ax\"], f\"{axis}axis\")\n", "                seed_values = self._get_subplot_data(var_df, var, view, share_state)\n", "                view_scale = scale._setup(seed_values, prop, axis=axis_obj)\n", "                set_scale_obj(view[\"ax\"], axis, view_scale._matplotlib_scale)\n", "\n", "                for layer, new_series in zip(layers, transformed_data):\n", "                    layer_df = layer[\"data\"].frame\n", "                    if var not in layer_df:\n", "                        continue\n", "\n", "                    idx = self._get_subplot_index(layer_df, view)\n", "                    try:\n", "                        new_series.loc[idx] = view_scale(layer_df.loc[idx, var])\n", "                    except Exception as err:\n", "                        spec_error = PlotSpecError._during(\"Scaling operation\", var)\n", "                        raise spec_error from err\n", "\n", "            # Now the transformed data series are complete, set update the layer data\n", "            for layer, new_series in zip(layers, transformed_data):\n", "                layer_df = layer[\"data\"].frame\n", "                if var in layer_df:\n", "                    layer_df[var] = new_series\n"]}, {"span_id": "Plotter", "start_line": 863, "end_line": 873, "content": ["class Plotter:\n", "    \"\"\"\n", "    Engine for compiling a :class:`Plot` spec into a Matplotlib figure.\n", "\n", "    This class is not intended to be instantiated directly by users.\n", "\n", "    \"\"\"\n", "    # TODO decide if we ever want these (Plot.plot(debug=True))?\n", "    _data: PlotData\n", "    _layers: list[Layer]\n", "    _figure: Figure\n"]}, {"span_id": "Plot._plot", "start_line": 823, "end_line": 857, "content": ["    def _plot(self, pyplot: bool = False) -> Plotter:\n", "\n", "        # TODO if we have _target object, pyplot should be determined by whether it\n", "        # is hooked into the pyplot state machine (how do we check?)\n", "\n", "        plotter = Plotter(pyplot=pyplot, theme=self._theme_with_defaults())\n", "\n", "        # Process the variable assignments and initialize the figure\n", "        common, layers = plotter._extract_data(self)\n", "        plotter._setup_figure(self, common, layers)\n", "\n", "        # Process the scale spec for coordinate variables and transform their data\n", "        coord_vars = [v for v in self._variables if re.match(r\"^x|y\", v)]\n", "        plotter._setup_scales(self, common, layers, coord_vars)\n", "\n", "        # Apply statistical transform(s)\n", "        plotter._compute_stats(self, layers)\n", "\n", "        # Process scale spec for semantic variables and coordinates computed by stat\n", "        plotter._setup_scales(self, common, layers)\n", "\n", "        # TODO Remove these after updating other methods\n", "        # ---- Maybe have debug= param that attaches these when True?\n", "        plotter._data = common\n", "        plotter._layers = layers\n", "\n", "        # Process the data for each layer and add matplotlib artists\n", "        for layer in layers:\n", "            plotter._plot_layer(self, layer)\n", "\n", "        # Add various figure decorations\n", "        plotter._make_legend(self)\n", "        plotter._finalize_figure(self)\n", "\n", "        return plotter\n"]}]}, {"file_path": "seaborn/_core/properties.py", "span_ids": ["Color._get_categorical_mapping", "Color.infer_scale", "Color", "Color.get_mapping"], "content": [{"span_id": "Color._get_categorical_mapping", "start_line": 609, "end_line": 649, "content": ["    def _get_categorical_mapping(self, scale, data):\n", "        \"\"\"Define mapping as lookup in list of discrete color values.\"\"\"\n", "        levels = categorical_order(data, scale.order)\n", "        n = len(levels)\n", "        values = scale.values\n", "\n", "        if isinstance(values, dict):\n", "            self._check_dict_entries(levels, values)\n", "            # TODO where to ensure that dict values have consistent representation?\n", "            colors = [values[x] for x in levels]\n", "        elif isinstance(values, list):\n", "            colors = self._check_list_length(levels, scale.values)\n", "        elif isinstance(values, tuple):\n", "            colors = blend_palette(values, n)\n", "        elif isinstance(values, str):\n", "            colors = color_palette(values, n)\n", "        elif values is None:\n", "            if n <= len(get_color_cycle()):\n", "                # Use current (global) default palette\n", "                colors = color_palette(n_colors=n)\n", "            else:\n", "                colors = color_palette(\"husl\", n)\n", "        else:\n", "            scale_class = scale.__class__.__name__\n", "            msg = \" \".join([\n", "                f\"Scale values for {self.variable} with a {scale_class} mapping\",\n", "                f\"must be string, list, tuple, or dict; not {type(scale.values)}.\"\n", "            ])\n", "            raise TypeError(msg)\n", "\n", "        # If color specified here has alpha channel, it will override alpha property\n", "        colors = self._standardize_color_sequence(colors)\n", "\n", "        def mapping(x):\n", "            ixs = np.asarray(x, np.intp)\n", "            use = np.isfinite(x)\n", "            out = np.full((len(ixs), colors.shape[1]), np.nan)\n", "            out[use] = np.take(colors, ixs[use], axis=0)\n", "            return out\n", "\n", "        return mapping\n"]}, {"span_id": "Color.infer_scale", "start_line": 567, "end_line": 607, "content": ["    def infer_scale(self, arg: Any, data: Series) -> Scale:\n", "        # TODO when inferring Continuous without data, verify type\n", "\n", "        # TODO need to rethink the variable type system\n", "        # (e.g. boolean, ordered categories as Ordinal, etc)..\n", "        var_type = variable_type(data, boolean_type=\"categorical\")\n", "\n", "        if isinstance(arg, (dict, list)):\n", "            return Nominal(arg)\n", "\n", "        if isinstance(arg, tuple):\n", "            if var_type == \"categorical\":\n", "                # TODO It seems reasonable to allow a gradient mapping for nominal\n", "                # scale but it also feels \"technically\" wrong. Should this infer\n", "                # Ordinal with categorical data and, if so, verify orderedness?\n", "                return Nominal(arg)\n", "            return Continuous(arg)\n", "\n", "        if callable(arg):\n", "            return Continuous(arg)\n", "\n", "        # TODO Do we accept str like \"log\", \"pow\", etc. for semantics?\n", "\n", "        # TODO what about\n", "        # - Temporal? (i.e. datetime)\n", "        # - Boolean?\n", "\n", "        if not isinstance(arg, str):\n", "            msg = \" \".join([\n", "                f\"A single scale argument for {self.variable} variables must be\",\n", "                f\"a string, dict, tuple, list, or callable, not {type(arg)}.\"\n", "            ])\n", "            raise TypeError(msg)\n", "\n", "        if arg in QUAL_PALETTES:\n", "            return Nominal(arg)\n", "        elif var_type == \"numeric\":\n", "            return Continuous(arg)\n", "        # TODO implement scales for date variables and any others.\n", "        else:\n", "            return Nominal(arg)\n"]}, {"span_id": "Color", "start_line": 539, "end_line": 542, "content": ["class Color(Property):\n", "    \"\"\"Color, as RGB(A), scalable with nominal palettes or continuous gradients.\"\"\"\n", "    legend = True\n", "    normed = True\n"]}, {"span_id": "Color.get_mapping", "start_line": 651, "end_line": 690, "content": ["    def get_mapping(\n", "        self, scale: Scale, data: Series\n", "    ) -> Callable[[ArrayLike], ArrayLike]:\n", "        \"\"\"Return a function that maps from data domain to color values.\"\"\"\n", "        # TODO what is best way to do this conditional?\n", "        # Should it be class-based or should classes have behavioral attributes?\n", "        if isinstance(scale, Nominal):\n", "            return self._get_categorical_mapping(scale, data)\n", "\n", "        if scale.values is None:\n", "            # TODO Rethink best default continuous color gradient\n", "            mapping = color_palette(\"ch:\", as_cmap=True)\n", "        elif isinstance(scale.values, tuple):\n", "            # TODO blend_palette will strip alpha, but we should support\n", "            # interpolation on all four channels\n", "            mapping = blend_palette(scale.values, as_cmap=True)\n", "        elif isinstance(scale.values, str):\n", "            # TODO for matplotlib colormaps this will clip extremes, which is\n", "            # different from what using the named colormap directly would do\n", "            # This may or may not be desireable.\n", "            mapping = color_palette(scale.values, as_cmap=True)\n", "        elif callable(scale.values):\n", "            mapping = scale.values\n", "        else:\n", "            scale_class = scale.__class__.__name__\n", "            msg = \" \".join([\n", "                f\"Scale values for {self.variable} with a {scale_class} mapping\",\n", "                f\"must be string, tuple, or callable; not {type(scale.values)}.\"\n", "            ])\n", "            raise TypeError(msg)\n", "\n", "        def _mapping(x):\n", "            # Remove alpha channel so it does not override alpha property downstream\n", "            # TODO this will need to be more flexible to support RGBA tuples (see above)\n", "            invalid = ~np.isfinite(x)\n", "            out = mapping(x)[:, :3]\n", "            out[invalid] = np.nan\n", "            return out\n", "\n", "        return _mapping\n"]}]}, {"file_path": "seaborn/_core/scales.py", "span_ids": ["ContinuousBase._setup"], "content": [{"span_id": "ContinuousBase._setup", "start_line": 322, "end_line": 392, "content": ["    def _setup(\n", "        self, data: Series, prop: Property, axis: Axis | None = None,\n", "    ) -> Scale:\n", "\n", "        new = copy(self)\n", "        if new._tick_params is None:\n", "            new = new.tick()\n", "        if new._label_params is None:\n", "            new = new.label()\n", "\n", "        forward, inverse = new._get_transform()\n", "\n", "        mpl_scale = new._get_scale(str(data.name), forward, inverse)\n", "\n", "        if axis is None:\n", "            axis = PseudoAxis(mpl_scale)\n", "            axis.update_units(data)\n", "\n", "        mpl_scale.set_default_locators_and_formatters(axis)\n", "        new._matplotlib_scale = mpl_scale\n", "\n", "        normalize: Optional[Callable[[ArrayLike], ArrayLike]]\n", "        if prop.normed:\n", "            if new.norm is None:\n", "                vmin, vmax = data.min(), data.max()\n", "            else:\n", "                vmin, vmax = new.norm\n", "            vmin, vmax = axis.convert_units((vmin, vmax))\n", "            a = forward(vmin)\n", "            b = forward(vmax) - forward(vmin)\n", "\n", "            def normalize(x):\n", "                return (x - a) / b\n", "\n", "        else:\n", "            normalize = vmin = vmax = None\n", "\n", "        new._pipeline = [\n", "            axis.convert_units,\n", "            forward,\n", "            normalize,\n", "            prop.get_mapping(new, data)\n", "        ]\n", "\n", "        def spacer(x):\n", "            x = x.dropna().unique()\n", "            if len(x) < 2:\n", "                return np.nan\n", "            return np.min(np.diff(np.sort(x)))\n", "        new._spacer = spacer\n", "\n", "        # TODO How to allow disabling of legend for all uses of property?\n", "        # Could add a Scale parameter, or perhaps Scale.suppress()?\n", "        # Are there other useful parameters that would be in Scale.legend()\n", "        # besides allowing Scale.legend(False)?\n", "        if prop.legend:\n", "            axis.set_view_interval(vmin, vmax)\n", "            locs = axis.major.locator()\n", "            locs = locs[(vmin <= locs) & (locs <= vmax)]\n", "            # Avoid having an offset / scientific notation in a legend\n", "            # as we don't represent that anywhere so it ends up incorrect.\n", "            # This could become an option (e.g. Continuous.label(offset=True))\n", "            # in which case we would need to figure out how to show it.\n", "            if hasattr(axis.major.formatter, \"set_useOffset\"):\n", "                axis.major.formatter.set_useOffset(False)\n", "            if hasattr(axis.major.formatter, \"set_scientific\"):\n", "                axis.major.formatter.set_scientific(False)\n", "            labels = axis.major.formatter.format_ticks(locs)\n", "            new._legend = list(locs), list(labels)\n", "\n", "        return new\n"]}]}]}