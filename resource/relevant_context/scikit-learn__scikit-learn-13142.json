{"instance_id": "scikit-learn__scikit-learn-13142", "identified_spans": [{"file_path": "sklearn/mixture/base.py", "span_ids": ["BaseMixture", "BaseMixture.fit_predict", "BaseMixture.predict", "BaseMixture.__init__"], "content": [{"span_id": "BaseMixture", "start_line": 64, "end_line": 69, "content": ["class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n", "    \"\"\"Base class for mixture models.\n", "\n", "    This abstract class specifies an interface for all mixture classes and\n", "    provides basic common methods for mixture models.\n", "    \"\"\"\n"]}, {"span_id": "BaseMixture.fit_predict", "start_line": 194, "end_line": 276, "content": ["    def fit_predict(self, X, y=None):\n", "        \"\"\"Estimate model parameters using X and predict the labels for X.\n", "\n", "        The method fits the model n_init times and sets the parameters with\n", "        which the model has the largest likelihood or lower bound. Within each\n", "        trial, the method iterates between E-step and M-step for `max_iter`\n", "        times until the change of likelihood or lower bound is less than\n", "        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n", "        predicts the most probable label for the input data points.\n", "\n", "        .. versionadded:: 0.20\n", "\n", "        Parameters\n", "        ----------\n", "        X : array-like, shape (n_samples, n_features)\n", "            List of n_features-dimensional data points. Each row\n", "            corresponds to a single data point.\n", "\n", "        Returns\n", "        -------\n", "        labels : array, shape (n_samples,)\n", "            Component labels.\n", "        \"\"\"\n", "        X = _check_X(X, self.n_components, ensure_min_samples=2)\n", "        self._check_initial_parameters(X)\n", "\n", "        # if we enable warm_start, we will have a unique initialisation\n", "        do_init = not(self.warm_start and hasattr(self, 'converged_'))\n", "        n_init = self.n_init if do_init else 1\n", "\n", "        max_lower_bound = -np.infty\n", "        self.converged_ = False\n", "\n", "        random_state = check_random_state(self.random_state)\n", "\n", "        n_samples, _ = X.shape\n", "        for init in range(n_init):\n", "            self._print_verbose_msg_init_beg(init)\n", "\n", "            if do_init:\n", "                self._initialize_parameters(X, random_state)\n", "\n", "            lower_bound = (-np.infty if do_init else self.lower_bound_)\n", "\n", "            for n_iter in range(1, self.max_iter + 1):\n", "                prev_lower_bound = lower_bound\n", "\n", "                log_prob_norm, log_resp = self._e_step(X)\n", "                self._m_step(X, log_resp)\n", "                lower_bound = self._compute_lower_bound(\n", "                    log_resp, log_prob_norm)\n", "\n", "                change = lower_bound - prev_lower_bound\n", "                self._print_verbose_msg_iter_end(n_iter, change)\n", "\n", "                if abs(change) < self.tol:\n", "                    self.converged_ = True\n", "                    break\n", "\n", "            self._print_verbose_msg_init_end(lower_bound)\n", "\n", "            if lower_bound > max_lower_bound:\n", "                max_lower_bound = lower_bound\n", "                best_params = self._get_parameters()\n", "                best_n_iter = n_iter\n", "\n", "        # Always do a final e-step to guarantee that the labels returned by\n", "        # fit_predict(X) are always consistent with fit(X).predict(X)\n", "        # for any value of max_iter and tol (and any random_state).\n", "        _, log_resp = self._e_step(X)\n", "\n", "        if not self.converged_:\n", "            warnings.warn('Initialization %d did not converge. '\n", "                          'Try different init parameters, '\n", "                          'or increase max_iter, tol '\n", "                          'or check for degenerate data.'\n", "                          % (init + 1), ConvergenceWarning)\n", "\n", "        self._set_parameters(best_params)\n", "        self.n_iter_ = best_n_iter\n", "        self.lower_bound_ = max_lower_bound\n", "\n", "        return log_resp.argmax(axis=1)\n"]}, {"span_id": "BaseMixture.predict", "start_line": 358, "end_line": 374, "content": ["    def predict(self, X):\n", "        \"\"\"Predict the labels for the data samples in X using trained model.\n", "\n", "        Parameters\n", "        ----------\n", "        X : array-like, shape (n_samples, n_features)\n", "            List of n_features-dimensional data points. Each row\n", "            corresponds to a single data point.\n", "\n", "        Returns\n", "        -------\n", "        labels : array, shape (n_samples,)\n", "            Component labels.\n", "        \"\"\"\n", "        self._check_is_fitted()\n", "        X = _check_X(X, None, self.means_.shape[1])\n", "        return self._estimate_weighted_log_prob(X).argmax(axis=1)\n"]}, {"span_id": "BaseMixture.__init__", "start_line": 71, "end_line": 83, "content": ["    def __init__(self, n_components, tol, reg_covar,\n", "                 max_iter, n_init, init_params, random_state, warm_start,\n", "                 verbose, verbose_interval):\n", "        self.n_components = n_components\n", "        self.tol = tol\n", "        self.reg_covar = reg_covar\n", "        self.max_iter = max_iter\n", "        self.n_init = n_init\n", "        self.init_params = init_params\n", "        self.random_state = random_state\n", "        self.warm_start = warm_start\n", "        self.verbose = verbose\n", "        self.verbose_interval = verbose_interval\n"]}]}, {"file_path": "sklearn/mixture/gaussian_mixture.py", "span_ids": ["GaussianMixture.__init__", "GaussianMixture"], "content": [{"span_id": "GaussianMixture.__init__", "start_line": 588, "end_line": 602, "content": ["    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n", "                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n", "                 weights_init=None, means_init=None, precisions_init=None,\n", "                 random_state=None, warm_start=False,\n", "                 verbose=0, verbose_interval=10):\n", "        super().__init__(\n", "            n_components=n_components, tol=tol, reg_covar=reg_covar,\n", "            max_iter=max_iter, n_init=n_init, init_params=init_params,\n", "            random_state=random_state, warm_start=warm_start,\n", "            verbose=verbose, verbose_interval=verbose_interval)\n", "\n", "        self.covariance_type = covariance_type\n", "        self.weights_init = weights_init\n", "        self.means_init = means_init\n", "        self.precisions_init = precisions_init\n"]}, {"span_id": "GaussianMixture", "start_line": 434, "end_line": 586, "content": ["class GaussianMixture(BaseMixture):\n", "    \"\"\"Gaussian Mixture.\n", "\n", "    Representation of a Gaussian mixture model probability distribution.\n", "    This class allows to estimate the parameters of a Gaussian mixture\n", "    distribution.\n", "\n", "    Read more in the :ref:`User Guide <gmm>`.\n", "\n", "    .. versionadded:: 0.18\n", "\n", "    Parameters\n", "    ----------\n", "    n_components : int, defaults to 1.\n", "        The number of mixture components.\n", "\n", "    covariance_type : {'full' (default), 'tied', 'diag', 'spherical'}\n", "        String describing the type of covariance parameters to use.\n", "        Must be one of:\n", "\n", "        'full'\n", "            each component has its own general covariance matrix\n", "        'tied'\n", "            all components share the same general covariance matrix\n", "        'diag'\n", "            each component has its own diagonal covariance matrix\n", "        'spherical'\n", "            each component has its own single variance\n", "\n", "    tol : float, defaults to 1e-3.\n", "        The convergence threshold. EM iterations will stop when the\n", "        lower bound average gain is below this threshold.\n", "\n", "    reg_covar : float, defaults to 1e-6.\n", "        Non-negative regularization added to the diagonal of covariance.\n", "        Allows to assure that the covariance matrices are all positive.\n", "\n", "    max_iter : int, defaults to 100.\n", "        The number of EM iterations to perform.\n", "\n", "    n_init : int, defaults to 1.\n", "        The number of initializations to perform. The best results are kept.\n", "\n", "    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.\n", "        The method used to initialize the weights, the means and the\n", "        precisions.\n", "        Must be one of::\n", "\n", "            'kmeans' : responsibilities are initialized using kmeans.\n", "            'random' : responsibilities are initialized randomly.\n", "\n", "    weights_init : array-like, shape (n_components, ), optional\n", "        The user-provided initial weights, defaults to None.\n", "        If it None, weights are initialized using the `init_params` method.\n", "\n", "    means_init : array-like, shape (n_components, n_features), optional\n", "        The user-provided initial means, defaults to None,\n", "        If it None, means are initialized using the `init_params` method.\n", "\n", "    precisions_init : array-like, optional.\n", "        The user-provided initial precisions (inverse of the covariance\n", "        matrices), defaults to None.\n", "        If it None, precisions are initialized using the 'init_params' method.\n", "        The shape depends on 'covariance_type'::\n", "\n", "            (n_components,)                        if 'spherical',\n", "            (n_features, n_features)               if 'tied',\n", "            (n_components, n_features)             if 'diag',\n", "            (n_components, n_features, n_features) if 'full'\n", "\n", "    random_state : int, RandomState instance or None, optional (default=None)\n", "        If int, random_state is the seed used by the random number generator;\n", "        If RandomState instance, random_state is the random number generator;\n", "        If None, the random number generator is the RandomState instance used\n", "        by `np.random`.\n", "\n", "    warm_start : bool, default to False.\n", "        If 'warm_start' is True, the solution of the last fitting is used as\n", "        initialization for the next call of fit(). This can speed up\n", "        convergence when fit is called several times on similar problems.\n", "        In that case, 'n_init' is ignored and only a single initialization\n", "        occurs upon the first call.\n", "        See :term:`the Glossary <warm_start>`.\n", "\n", "    verbose : int, default to 0.\n", "        Enable verbose output. If 1 then it prints the current\n", "        initialization and each iteration step. If greater than 1 then\n", "        it prints also the log probability and the time needed\n", "        for each step.\n", "\n", "    verbose_interval : int, default to 10.\n", "        Number of iteration done before the next print.\n", "\n", "    Attributes\n", "    ----------\n", "    weights_ : array-like, shape (n_components,)\n", "        The weights of each mixture components.\n", "\n", "    means_ : array-like, shape (n_components, n_features)\n", "        The mean of each mixture component.\n", "\n", "    covariances_ : array-like\n", "        The covariance of each mixture component.\n", "        The shape depends on `covariance_type`::\n", "\n", "            (n_components,)                        if 'spherical',\n", "            (n_features, n_features)               if 'tied',\n", "            (n_components, n_features)             if 'diag',\n", "            (n_components, n_features, n_features) if 'full'\n", "\n", "    precisions_ : array-like\n", "        The precision matrices for each component in the mixture. A precision\n", "        matrix is the inverse of a covariance matrix. A covariance matrix is\n", "        symmetric positive definite so the mixture of Gaussian can be\n", "        equivalently parameterized by the precision matrices. Storing the\n", "        precision matrices instead of the covariance matrices makes it more\n", "        efficient to compute the log-likelihood of new samples at test time.\n", "        The shape depends on `covariance_type`::\n", "\n", "            (n_components,)                        if 'spherical',\n", "            (n_features, n_features)               if 'tied',\n", "            (n_components, n_features)             if 'diag',\n", "            (n_components, n_features, n_features) if 'full'\n", "\n", "    precisions_cholesky_ : array-like\n", "        The cholesky decomposition of the precision matrices of each mixture\n", "        component. A precision matrix is the inverse of a covariance matrix.\n", "        A covariance matrix is symmetric positive definite so the mixture of\n", "        Gaussian can be equivalently parameterized by the precision matrices.\n", "        Storing the precision matrices instead of the covariance matrices makes\n", "        it more efficient to compute the log-likelihood of new samples at test\n", "        time. The shape depends on `covariance_type`::\n", "\n", "            (n_components,)                        if 'spherical',\n", "            (n_features, n_features)               if 'tied',\n", "            (n_components, n_features)             if 'diag',\n", "            (n_components, n_features, n_features) if 'full'\n", "\n", "    converged_ : bool\n", "        True when convergence was reached in fit(), False otherwise.\n", "\n", "    n_iter_ : int\n", "        Number of step used by the best fit of EM to reach the convergence.\n", "\n", "    lower_bound_ : float\n", "        Lower bound value on the log-likelihood (of the training data with\n", "        respect to the model) of the best fit of EM.\n", "\n", "    See Also\n", "    --------\n", "    BayesianGaussianMixture : Gaussian mixture model fit with a variational\n", "        inference.\n", "    \"\"\"\n"]}]}]}