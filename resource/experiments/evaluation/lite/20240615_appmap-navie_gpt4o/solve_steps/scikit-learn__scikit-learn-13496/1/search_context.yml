- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    # NOTE: Setting the `warm_start` construction parameter to `True` disables
    # support for parallelized ensembles but is necessary for tracking the OOB
    # error trajectory during training.
    ensemble_clfs = [
        ("RandomForestClassifier, max_features='sqrt'",
            RandomForestClassifier(n_estimators=100,
                                   warm_start=True, oob_score=True,
                                   max_features="sqrt",
                                   random_state=RANDOM_STATE)),
        ("RandomForestClassifier, max_features='log2'",
            RandomForestClassifier(n_estimators=100,
                                   warm_start=True, max_features='log2',
                                   oob_score=True,
                                   random_state=RANDOM_STATE)),
        ("RandomForestClassifier, max_features=None",
            RandomForestClassifier(n_estimators=100,
                                   warm_start=True, max_features=None,
                                   oob_score=True,
  location: examples/ensemble/plot_ensemble_oob.py:43-60
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    RandomForestClassifier(n_estimators=100,
                                   warm_start=True, max_features=None,
                                   oob_score=True,
                                   random_state=RANDOM_STATE))
    ]
  location: examples/ensemble/plot_ensemble_oob.py:58-62
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    import numpy as np
    import matplotlib.pyplot as plt

    from sklearn.datasets import make_classification
    from sklearn.ensemble import ExtraTreesClassifier

    # Build a classification task using 3 informative features
    X, y = make_classification(n_samples=1000,
                               n_features=10,
                               n_informative=3,
                               n_redundant=0,
                               n_repeated=0,
                               n_classes=2,
                               random_state=0,
                               shuffle=False)

    # Build a forest and compute the feature importances
    forest = ExtraTreesClassifier(n_estimators=250,
                                  random_state=0)

    forest.fit(X, y)
    importances = forest.feature_importances_
    std = np.std([tree.feature_importances_ for tree in forest.estimators_],
                 axis=0)
    indices = np.argsort(importances)[::-1]

    # Print the feature ranking
    print("Feature ranking:")
  location: examples/ensemble/plot_forest_importances.py:15-42
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    """

    # Author: Tim Head <betatim@gmail.com>
    #
    # License: BSD 3 clause

    import numpy as np
    np.random.seed(10)

    import matplotlib.pyplot as plt

    from sklearn.datasets import make_classification
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,
                                  GradientBoostingClassifier)
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import roc_curve
    from sklearn.pipeline import make_pipeline

    n_estimator = 10
    X, y = make_classification(n_samples=80000)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
  location: examples/ensemble/plot_feature_transformation.py:22-44
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    plt.figure(2)
    plt.xlim(0, 0.2)
    plt.ylim(0.8, 1)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
    plt.plot(fpr_rf, tpr_rf, label='RF')
    plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
    plt.plot(fpr_grd, tpr_grd, label='GBT')
    plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve (zoomed in at top left)')
    plt.legend(loc='best')
    plt.show()
  location: examples/ensemble/plot_feature_transformation.py:107-120
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    The figure shows the cumulative sum of the negative OOB improvements
    as a function of the boosting iteration. As you can see, it tracks the test
    loss for the first hundred iterations but then diverges in a
    pessimistic way.
    The figure also shows the performance of 3-fold cross validation which
    usually gives a better estimate of the test loss
    but is computationally more demanding.
    """
    print(__doc__)

    # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
    #
    # License: BSD 3 clause

    import numpy as np
    import matplotlib.pyplot as plt

    from sklearn import ensemble
    from sklearn.model_selection import KFold
    from sklearn.model_selection import train_test_split

    from scipy.special import expit

    # Generate data (adapted from G. Ridgeway's gbm example)
    n_samples = 1000
    random_state = np.random.RandomState(13)
    x1 = random_state.uniform(size=n_samples)
    x2 = random_state.uniform(size=n_samples)
    x3 = random_state.randint(0, 4, size=n_samples)
  location: examples/ensemble/plot_gradient_boosting_oob.py:18-46
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    y_pred_grd_lm = grd_lm.predict_proba(
        grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]
    fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)

    # The gradient boosted model by itself
    y_pred_grd = grd.predict_proba(X_test)[:, 1]
    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)

    # The random forest model by itself
    y_pred_rf = rf.predict_proba(X_test)[:, 1]
    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)

    plt.figure(1)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
    plt.plot(fpr_rf, tpr_rf, label='RF')
    plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
    plt.plot(fpr_grd, tpr_grd, label='GBT')
    plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve')
    plt.legend(loc='best')
    plt.show()
  location: examples/ensemble/plot_feature_transformation.py:82-105
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    =========================================

    Feature importances with forests of trees

    =========================================


    This examples shows the use of forests of trees to evaluate the importance
    of

    features on an artificial classification task. The red bars are the feature

    importances of the forest, along with their inter-trees variability.


    As expected, the plot suggests that 3 features are informative, while the

    remaining are not.

    """

    print(__doc__)


    import numpy as np

    import matplotlib.pyplot as plt


    from sklearn.datasets import make_classification

    from sklearn.ensemble import ExtraTreesClassifier
  location: examples/ensemble/plot_forest_importances.py:1-19
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    ======================================

    Gradient Boosting Out-of-Bag estimates

    ======================================


    Out-of-bag (OOB) estimates can be a useful heuristic to estimate

    the "optimal" number of boosting iterations.

    OOB estimates are almost identical to cross-validation estimates but

    they can be computed on-the-fly without the need for repeated model

    fitting.

    OOB estimates are only available for Stochastic Gradient Boosting

    (i.e. ``subsample < 1.0``), the estimates are derived from the improvement

    in loss based on the examples not included in the bootstrap sample

    (the so-called out-of-bag examples).

    The OOB estimator is a pessimistic estimator of the true

    test loss, but remains a fairly good approximation for a small number of
    trees.
  location: examples/ensemble/plot_gradient_boosting_oob.py:1-16
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    It is worth noting that RandomForests and ExtraTrees can be fitted in
    parallel

    on many cores as each tree is built independently of the others. AdaBoost's

    samples are built sequentially and so do not use multiple cores.

    """

    print(__doc__)


    import numpy as np

    import matplotlib.pyplot as plt

    from matplotlib.colors import ListedColormap


    from sklearn.datasets import load_iris

    from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,
                                  AdaBoostClassifier)
    from sklearn.tree import DecisionTreeClassifier


    # Parameters

    n_classes = 3

    n_estimators = 30

    cmap = plt.cm.RdYlBu

    plot_step = 0.02  # fine step width for decision surface contours

    plot_step_coarser = 0.5  # step widths for coarse classifier guesses

    RANDOM_SEED = 13  # fix the seed on each iteration


    # Load data

    iris = load_iris()


    plot_idx = 1
  location: examples/ensemble/plot_forest_iris.py:39-65
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    ==========================================

    IsolationForest example

    ==========================================


    An example using :class:`sklearn.ensemble.IsolationForest` for anomaly

    detection.


    The IsolationForest 'isolates' observations by randomly selecting a feature

    and then randomly selecting a split value between the maximum and minimum

    values of the selected feature.


    Since recursive partitioning can be represented by a tree structure, the

    number of splittings required to isolate a sample is equivalent to the path

    length from the root node to the terminating node.


    This path length, averaged over a forest of such random trees, is a measure

    of normality and our decision function.


    Random partitioning produces noticeable shorter paths for anomalies.

    Hence, when a forest of random trees collectively produce shorter path
    lengths

    for particular samples, they are highly likely to be anomalies.


    """

    print(__doc__)
  location: examples/ensemble/plot_isolation_forest.py:1-25
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    ======================================

    Decision Tree Regression with AdaBoost

    ======================================


    A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D

    sinusoidal dataset with a small amount of Gaussian noise.

    299 boosts (300 decision trees) is compared with a single decision tree

    regressor. As the number of boosts is increased the regressor can fit more

    detail.


    .. [1] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.


    """

    print(__doc__)


    # Author: Noel Dawe <noel.dawe@gmail.com>

    #

    # License: BSD 3 clause


    # importing necessary libraries

    import numpy as np

    import matplotlib.pyplot as plt

    from sklearn.tree import DecisionTreeRegressor

    from sklearn.ensemble import AdaBoostRegressor


    # Create the dataset

    rng = np.random.RandomState(1)

    X = np.linspace(0, 6, 100)[:, np.newaxis]

    y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1,
    X.shape[0])


    # Fit regression model

    regr_1 = DecisionTreeRegressor(max_depth=4)
  location: examples/ensemble/plot_adaboost_regression.py:1-33
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    .. [1] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical
           Learning Ed. 2", p592-593, Springer, 2009.

    """
    import matplotlib.pyplot as plt

    from collections import OrderedDict
    from sklearn.datasets import make_classification
    from sklearn.ensemble import RandomForestClassifier

    # Author: Kian Ho <hui.kian.ho@gmail.com>
    #         Gilles Louppe <g.louppe@gmail.com>
    #         Andreas Mueller <amueller@ais.uni-bonn.de>
    #
    # License: BSD 3 Clause

    print(__doc__)

    RANDOM_STATE = 123

    # Generate a binary classification dataset.
    X, y = make_classification(n_samples=500, n_features=25,
                               n_clusters_per_class=1, n_informative=15,
                               random_state=RANDOM_STATE)
  location: examples/ensemble/plot_ensemble_oob.py:18-41
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    """
    print(__doc__)

    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.ensemble import IsolationForest

    rng = np.random.RandomState(42)

    # Generate train data
    X = 0.3 * rng.randn(100, 2)
    X_train = np.r_[X + 2, X - 2]
    # Generate some regular novel observations
    X = 0.3 * rng.randn(20, 2)
    X_test = np.r_[X + 2, X - 2]
    # Generate some abnormal novel observations
    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))

    # fit the model
    clf = IsolationForest(behaviour='new', max_samples=100,
                          random_state=rng, contamination='auto')
    clf.fit(X_train)
    y_pred_train = clf.predict(X_train)
    y_pred_test = clf.predict(X_test)
    y_pred_outliers = clf.predict(X_outliers)

    # plot the line, the samples, and the nearest vectors to the plane
    xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.title("IsolationForest")
    plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)
  location: examples/ensemble/plot_isolation_forest.py:24-56
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    estimators_ : list of estimators
            The collection of fitted base estimators.
        """
        # overwrite _required_parameters from MetaEstimatorMixin
        _required_parameters = []

        @abstractmethod
        def __init__(self, base_estimator, n_estimators=10,
                     estimator_params=tuple()):
            # Set parameters
            self.base_estimator = base_estimator
            self.n_estimators = n_estimators
            self.estimator_params = estimator_params

            # Don't instantiate estimators now! Parameters of base_estimator might
            # still change. Eg., when grid-searching with the nested object syntax.
            # self.estimators_ needs to be filled by the derived classes in fit.
  location: sklearn/ensemble/base.py:83-99
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    def _partition_estimators(n_estimators, n_jobs):
        """Private function used to partition estimators between jobs."""
        # Compute the number of jobs
        n_jobs = min(effective_n_jobs(n_jobs), n_estimators)

        # Partition estimators between jobs
        n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,
                                       dtype=np.int)
        n_estimators_per_job[:n_estimators % n_jobs] += 1
        starts = np.cumsum(n_estimators_per_job)

        return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()
  location: sklearn/ensemble/base.py:151-162
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    * cross-validation splitters
            * ``scipy.stats`` rvs
        """
        random_state = check_random_state(random_state)
        to_set = {}
        for key in sorted(estimator.get_params(deep=True)):
            if key == 'random_state' or key.endswith('__random_state'):
                to_set[key] = random_state.randint(MAX_RAND_SEED)

        if to_set:
            estimator.set_params(**to_set)
  location: sklearn/ensemble/base.py:47-57
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    """
    The :mod:`sklearn.ensemble` module includes ensemble-based methods for
    classification, regression and anomaly detection.
    """

    from .base import BaseEnsemble
    from .forest import RandomForestClassifier
    from .forest import RandomForestRegressor
    from .forest import RandomTreesEmbedding
    from .forest import ExtraTreesClassifier
    from .forest import ExtraTreesRegressor
    from .bagging import BaggingClassifier
    from .bagging import BaggingRegressor
    from .iforest import IsolationForest
    from .weight_boosting import AdaBoostClassifier
    from .weight_boosting import AdaBoostRegressor
    from .gradient_boosting import GradientBoostingClassifier
    from .gradient_boosting import GradientBoostingRegressor
    from .voting_classifier import VotingClassifier

    from . import bagging
    from . import forest
    from . import weight_boosting
    from . import gradient_boosting
    from . import partial_dependence
  location: sklearn/ensemble/__init__.py:1-25
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    def _make_estimator(self, append=True, random_state=None):
            """Make and configure a copy of the `base_estimator_` attribute.

            Warning: This method should be used to properly instantiate new
            sub-estimators.
            """
            estimator = clone(self.base_estimator_)
            estimator.set_params(**{p: getattr(self, p)
                                    for p in self.estimator_params})

            if random_state is not None:
                _set_random_states(estimator, random_state)

            if append:
                self.estimators_.append(estimator)

            return estimator

        def __len__(self):
            """Returns the number of estimators in the ensemble."""
            return len(self.estimators_)

        def __getitem__(self, index):
            """Returns the index'th estimator in the ensemble."""
            return self.estimators_[index]

        def __iter__(self):
            """Returns iterator over estimators in the ensemble."""
            return iter(self.estimators_)
  location: sklearn/ensemble/base.py:120-148
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    def _validate_estimator(self, default=None):
            """Check the estimator and the n_estimator attribute, set the
            `base_estimator_` attribute."""
            if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):
                raise ValueError("n_estimators must be an integer, "
                                 "got {0}.".format(type(self.n_estimators)))

            if self.n_estimators <= 0:
                raise ValueError("n_estimators must be greater than zero, "
                                 "got {0}.".format(self.n_estimators))

            if self.base_estimator is not None:
                self.base_estimator_ = self.base_estimator
            else:
                self.base_estimator_ = default

            if self.base_estimator_ is None:
                raise ValueError("base_estimator cannot be None")

        def _make_estimator(self, append=True, random_state=None):
            """Make and configure a copy of the `base_estimator_` attribute.
  location: sklearn/ensemble/base.py:101-121
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    =============================

    OOB Errors for Random Forests

    =============================


    The ``RandomForestClassifier`` is trained using *bootstrap aggregation*,
    where

    each new tree is fit from a bootstrap sample of the training observations

    :math:`z_i = (x_i, y_i)`. The *out-of-bag* (OOB) error is the average error
    for

    each :math:`z_i` calculated using predictions from the trees that do not

    contain :math:`z_i` in their respective bootstrap sample. This allows the

    ``RandomForestClassifier`` to be fit and validated whilst being trained
    [1]_.


    The example below demonstrates how the OOB error can be measured at the

    addition of each new tree during training. The resulting plot allows a

    practitioner to approximate a suitable value of ``n_estimators`` at which
    the

    error stabilizes.


    .. [1] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical
           Learning Ed. 2", p592-593, Springer, 2009.

    """

    import matplotlib.pyplot as plt
  location: examples/ensemble/plot_ensemble_oob.py:1-22
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    In descending order of quality, when trained (outside of this example) on
    all

    4 features using 30 estimators and scored using 10 fold cross validation,

    we see::

        ExtraTreesClassifier()  # 0.95 score
        RandomForestClassifier()  # 0.94 score
        AdaBoost(DecisionTree(max_depth=3))  # 0.94 score
        DecisionTree(max_depth=None)  # 0.94 score

    Increasing `max_depth` for AdaBoost lowers the standard deviation of

    the scores (but the average score does not improve).


    See the console's output for further details about each model.


    In this example you might try to:


    1) vary the ``max_depth`` for the ``DecisionTreeClassifier`` and
       ``AdaBoostClassifier``, perhaps try ``max_depth=3`` for the
       ``DecisionTreeClassifier`` or ``max_depth=None`` for ``AdaBoostClassifier``
    2) vary ``n_estimators``
  location: examples/ensemble/plot_forest_iris.py:18-37
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    p = expit(np.sin(3 * x1) - 4 * x2 + x3)
    y = random_state.binomial(1, p, size=n_samples)

    X = np.c_[x1, x2, x3]

    X = X.astype(np.float32)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
                                                        random_state=9)

    # Fit classifier with out-of-bag estimates
    params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,
              'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
    clf = ensemble.GradientBoostingClassifier(**params)

    clf.fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    print("Accuracy: {:.4f}".format(acc))

    n_estimators = params['n_estimators']
    x = np.arange(n_estimators) + 1
  location: examples/ensemble/plot_gradient_boosting_oob.py:48-67
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    def cv_estimate(n_splits=None):
        cv = KFold(n_splits=n_splits)
        cv_clf = ensemble.GradientBoostingClassifier(**params)
        val_scores = np.zeros((n_estimators,), dtype=np.float64)
        for train, test in cv.split(X_train, y_train):
            cv_clf.fit(X_train[train], y_train[train])
            val_scores += heldout_score(cv_clf, X_train[test], y_train[test])
        val_scores /= n_splits
        return val_scores


    # Estimate best n_estimator using cross-validation
    cv_score = cv_estimate(3)

    # Compute best n_estimator for test data
    test_score = heldout_score(clf, X_test, y_test)

    # negative cumulative sum of oob improvements
    cumsum = -np.cumsum(clf.oob_improvement_)

    # min loss according to OOB
    oob_best_iter = x[np.argmin(cumsum)]

    # min loss according to test (normalize such that first loss is 0)
    test_score -= test_score[0]
    test_best_iter = x[np.argmin(test_score)]
  location: examples/ensemble/plot_gradient_boosting_oob.py:78-103
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    # Supervised transformation based on random forests
    rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
    rf_enc = OneHotEncoder(categories='auto')
    rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
    rf.fit(X_train, y_train)
    rf_enc.fit(rf.apply(X_train))
    rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)

    y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]
    fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)

    # Supervised transformation based on gradient boosted trees
    grd = GradientBoostingClassifier(n_estimators=n_estimator)
    grd_enc = OneHotEncoder(categories='auto')
    grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
    grd.fit(X_train, y_train)
    grd_enc.fit(grd.apply(X_train)[:, :, 0])
    grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)

    y_pred_grd_lm = grd_lm.predict_proba(
        grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]
    fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)
  location: examples/ensemble/plot_feature_transformation.py:63-84
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    n_estimator = 10
    X, y = make_classification(n_samples=80000)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

    # It is important to train the ensemble of trees on a different subset
    # of the training data than the linear regression model to avoid
    # overfitting, in particular if the total number of leaves is
    # similar to the number of training samples
    X_train, X_train_lr, y_train, y_train_lr = train_test_split(
        X_train, y_train, test_size=0.5)

    # Unsupervised transformation based on totally random trees
    rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
                              random_state=0)

    rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
    pipeline = make_pipeline(rt, rt_lm)
    pipeline.fit(X_train, y_train)
    y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)
  location: examples/ensemble/plot_feature_transformation.py:42-61
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    def heldout_score(clf, X_test, y_test):
        """compute deviance scores on ``X_test`` and ``y_test``. """
        score = np.zeros((n_estimators,), dtype=np.float64)
        for i, y_pred in enumerate(clf.staged_decision_function(X_test)):
            score[i] = clf.loss_(y_test, y_pred)
        return score
  location: examples/ensemble/plot_gradient_boosting_oob.py:70-75
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: >-
    """

    ====================================================================

    Plot the decision surfaces of ensembles of trees on the iris dataset

    ====================================================================


    Plot the decision surfaces of forests of randomized trees trained on pairs
    of

    features of the iris dataset.


    This plot compares the decision surfaces learned by a decision tree
    classifier

    (first column), by a random forest classifier (second column), by an extra-

    trees classifier (third column) and by an AdaBoost classifier (fourth
    column).


    In the first row, the classifiers are built using the sepal width and

    the sepal length features only, on the second row using the petal length and

    sepal length only, and on the third row using the petal width and the

    petal length only.


    In descending order of quality, when trained (outside of this example) on
    all

    4 features using 30 estimators and scored using 10 fold cross validation,

    we see::
  location: examples/ensemble/plot_forest_iris.py:1-20
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    # min loss according to test (normalize such that first loss is 0)
    test_score -= test_score[0]
    test_best_iter = x[np.argmin(test_score)]

    # min loss according to cv (normalize such that first loss is 0)
    cv_score -= cv_score[0]
    cv_best_iter = x[np.argmin(cv_score)]

    # color brew for the three curves
    oob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))
    test_color = list(map(lambda x: x / 256.0, (127, 201, 127)))
    cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))

    # plot curves and vertical lines for best iterations
    plt.plot(x, cumsum, label='OOB loss', color=oob_color)
    plt.plot(x, test_score, label='Test loss', color=test_color)
    plt.plot(x, cv_score, label='CV loss', color=cv_color)
    plt.axvline(x=oob_best_iter, color=oob_color)
    plt.axvline(x=test_best_iter, color=test_color)
    plt.axvline(x=cv_best_iter, color=cv_color)
  location: examples/ensemble/plot_gradient_boosting_oob.py:101-120
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    plt.title("IsolationForest")
    plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)

    b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',
                     s=20, edgecolor='k')
    b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',
                     s=20, edgecolor='k')
    c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
                    s=20, edgecolor='k')
    plt.axis('tight')
    plt.xlim((-5, 5))
    plt.ylim((-5, 5))
    plt.legend([b1, b2, c],
               ["training observations",
                "new regular observations", "new abnormal observations"],
               loc="upper left")
    plt.show()
  location: examples/ensemble/plot_isolation_forest.py:55-71
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    # add three vertical lines to xticks
    xticks = plt.xticks()
    xticks_pos = np.array(xticks[0].tolist() +
                          [oob_best_iter, cv_best_iter, test_best_iter])
    xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +
                            ['OOB', 'CV', 'Test'])
    ind = np.argsort(xticks_pos)
    xticks_pos = xticks_pos[ind]
    xticks_label = xticks_label[ind]
    plt.xticks(xticks_pos, xticks_label)

    plt.legend(loc='upper right')
    plt.ylabel('normalized loss')
    plt.xlabel('number of iterations')

    plt.show()
  location: examples/ensemble/plot_gradient_boosting_oob.py:122-137
- directory: /home/runner/work/_temp/tmpeu8jblwm/scikit-learn__scikit-learn__0.21-2
  type: code-snippet
  content: |-
    import numpy
    from numpy.distutils.misc_util import Configuration


    def configuration(parent_package="", top_path=None):
        config = Configuration("ensemble", parent_package, top_path)
        config.add_extension("_gradient_boosting",
                             sources=["_gradient_boosting.pyx"],
                             include_dirs=[numpy.get_include()])

        config.add_subpackage("tests")

        return config

    if __name__ == "__main__":
        from numpy.distutils.core import setup
        setup(**configuration().todict())
  location: sklearn/ensemble/setup.py:1-17
