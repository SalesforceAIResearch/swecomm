diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index ecae2566..c28fa275 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -2,7 +2,7 @@ import pandas as pd
 
 from . import dtypes, utils
 from .alignment import align
-from .merge import _VALID_COMPAT, unique_variable
+from .merge import _VALID_COMPAT, MergeError, unique_variable
 from .variable import IndexVariable, Variable, as_variable
 from .variable import concat as concat_vars
 
@@ -158,6 +158,7 @@ def _calc_concat_dim_coord(dim):
 def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
     """
     Determine which dataset variables need to be concatenated in the result,
+    and handle missing variables by filling them with `NaN`.
     """
     # Return values
     concat_over = set()
@@ -170,14 +171,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
         concat_over_existing_dim = False
 
     concat_dim_lengths = []
+    all_vars = set()
     for ds in datasets:
         if concat_over_existing_dim:
             if dim not in ds.dims:
                 if dim in ds:
                     ds = ds.set_coords(dim)
         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)
+        all_vars.update(ds.variables.keys())
         concat_dim_lengths.append(ds.dims.get(dim, 1))
 
+    # Identify missing variables and fill with NaN
+    for ds in datasets:
+        missing_vars = all_vars - ds.variables.keys()
+        for var in missing_vars:
+            ds[var] = ds[dim] * float('nan')
+
     def process_subset_opt(opt, subset):
         if isinstance(opt, str):
             if opt == "different":
@@ -271,7 +280,8 @@ def _dataset_concat(
     join="outer",
 ):
     """
-    Concatenate a sequence of datasets along a new or existing dimension
+    Concatenate a sequence of datasets along a new or existing dimension,
+    handling missing variables by filling them with `NaN`.
     """
     from .dataset import Dataset
 
@@ -307,6 +317,104 @@ def _dataset_concat(
     # determine which variables to merge, and then merge them according to compat
     variables_to_merge = (coord_names | data_names) - concat_over - dim_names
 
+    result_vars = {}
+    if variables_to_merge:
+        to_merge = {var: [] for var in variables_to_merge}
+
+        for ds in datasets:
+            # Identify missing variables and fill with NaN
+            for var in variables_to_merge:
+                if var not in ds.variables:
+                    ds[var] = ds[dim] * float('nan')
+                to_merge[var].append(ds.variables[var])
+
+        for var in variables_to_merge:
+            result_vars[var] = unique_variable(
+                var, to_merge[var], compat=compat, equals=equals.get(var, None)
+            )
+    else:
+        result_vars = {}
+    result_vars.update(dim_coords)
+
+    # assign attrs and encoding from first dataset
+    result_attrs = datasets[0].attrs
+    result_encoding = datasets[0].encoding
+
+    # check that global attributes are fixed across all datasets if necessary
+    for ds in datasets[1:]:
+        if compat == "identical" and not utils.dict_equiv(ds.attrs, result_attrs):
+            raise ValueError("Dataset global attributes not equal.")
+
+    # we've already verified everything is consistent; now, calculate
+    # shared dimension sizes so we can expand the necessary variables
+    def ensure_common_dims(vars):
+        # ensure each variable with the given name shares the same
+        # dimensions and the same shape for all of them except along the
+        # concat dimension
+        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))
+        if dim not in common_dims:
+            common_dims = (dim,) + common_dims
+        for var, dim_len in zip(vars, concat_dim_lengths):
+            if var.dims != common_dims:
+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)
+                var = var.set_dims(common_dims, common_shape)
+            yield var
+
+    # stack up each variable to fill-out the dataset (in order)
+    # n.b. this loop preserves variable order, needed for groupby.
+    for k in datasets[0].variables:
+        if k in concat_over:
+            try:
+                vars = ensure_common_dims([ds.variables[k] for ds in datasets])
+            except KeyError:
+                raise ValueError("%r is not present in all datasets." % k)
+            combined = concat_vars(vars, dim, positions)
+            assert isinstance(combined, Variable)
+            result_vars[k] = combined
+
+    result = Dataset(result_vars, attrs=result_attrs)
+    absent_coord_names = coord_names - set(result.variables)
+    if absent_coord_names:
+        raise ValueError(
+            "Variables %r are coordinates in some datasets but not others."
+            % absent_coord_names
+        )
+    result = result.set_coords(coord_names)
+    result.encoding = result_encoding
+
+    result = result.drop(unlabeled_dims, errors="ignore")
+
+    if coord is not None:
+        # add concat dimension last to ensure that its in the final Dataset
+        result[coord.name] = coord
+
+    return result
+
+    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)
+    dim_names = set(dim_coords)
+    unlabeled_dims = dim_names - coord_names
+
+    both_data_and_coords = coord_names & data_names
+    if both_data_and_coords:
+        raise ValueError(
+            "%r is a coordinate in some datasets but not others." % both_data_and_coords
+        )
+    # we don't want the concat dimension in the result dataset yet
+    dim_coords.pop(dim, None)
+    dims_sizes.pop(dim, None)
+
+    # case where concat dimension is a coordinate or data_var but not a dimension
+    if (dim in coord_names or dim in data_names) and dim not in dim_names:
+        datasets = [ds.expand_dims(dim) for ds in datasets]
+
+    # determine which variables to concatentate
+    concat_over, equals, concat_dim_lengths = _calc_concat_over(
+        datasets, dim, dim_names, data_vars, coords, compat
+    )
+
+    # determine which variables to merge, and then merge them according to compat
+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names
+
     result_vars = {}
     if variables_to_merge:
         to_merge = {var: [] for var in variables_to_merge}
@@ -408,7 +516,7 @@ def _dataarray_concat(
             name = arr.name
         elif name != arr.name:
             if compat == "identical":
-                raise ValueError("array names not identical")
+                raise MergeError("array names not identical")
             else:
                 arr = arr.rename(name)
         datasets.append(arr._to_temp_dataset())
