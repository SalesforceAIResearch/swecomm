diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index a9f66740f..84b49848d 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -67,6 +67,11 @@ def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):
 class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
     """Base class for mixture models.
 
+    Attributes
+    ----------
+    labels_ : array, shape (n_samples,), optional
+        Labels of each point after fitting.
+
     This abstract class specifies an interface for all mixture classes and
     provides basic common methods for mixture models.
     """
@@ -74,6 +79,7 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
     def __init__(self, n_components, tol, reg_covar,
                  max_iter, n_init, init_params, random_state, warm_start,
                  verbose, verbose_interval):
+        self.labels_ = None
         self.n_components = n_components
         self.tol = tol
         self.reg_covar = reg_covar
@@ -191,6 +197,8 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
         X = _check_X(X, self.n_components, ensure_min_samples=2)
         self._check_initial_parameters(X)
 
+        self.labels_ = None
+
         # if we enable warm_start, we will have a unique initialisation
         do_init = not(self.warm_start and hasattr(self, 'converged_'))
         n_init = self.n_init if do_init else 1
@@ -239,6 +247,7 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
 
         self._set_parameters(best_params)
         self.n_iter_ = best_n_iter
+        self.labels_ = self.predict(X)
 
         return self
 
@@ -409,6 +418,24 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):
 
         return (X, y)
 
+    def fit_predict(self, X, y=None):
+        """Fit the model using X and predict the labels for X.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            List of n_features-dimensional data points. Each row
+            corresponds to a single data point.
+
+        Returns
+        -------
+        labels : array, shape (n_samples,)
+            Component labels for each data point.
+        """
+        self.fit(X, y)
+        self.labels_ = self.predict(X)
+        return self.labels_
+
     def _estimate_weighted_log_prob(self, X):
         """Estimate the weighted log-probabilities, log P(X | Z) + log weights.
 
