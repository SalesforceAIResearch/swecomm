diff --git a/doc/whats_new/.backup.v0.21.rst b/doc/whats_new/.backup.v0.21.rst
new file mode 100644
index 000000000..55c7e0c58
--- /dev/null
+++ b/doc/whats_new/.backup.v0.21.rst
@@ -0,0 +1,342 @@
+.. include:: _contributors.rst
+
+.. currentmodule:: sklearn
+
+.. _changes_0_21:
+
+Version 0.21.0
+==============
+
+**In development**
+
+Changed models
+--------------
+
+The following estimators and functions, when fit with the same data and
+parameters, may produce different models from the previous version. This often
+occurs due to changes in the modelling logic (bug fixes or enhancements), or in
+random sampling procedures.
+
+- :class:`linear_model.BayesianRidge` |Fix|
+- Decision trees and derived ensembles when both `max_depth` and
+  `max_leaf_nodes` are set. |Fix|
+- :class:`linear_model.LogisticRegression` and
+  :class:`linear_model.LogisticRegressionCV` with 'saga' solver. |Fix|
+- :class:`ensemble.GradientBoostingClassifier` for multiclass
+  classification. |Fix|
+
+Details are listed in the changelog below.
+
+(While we are trying to better inform users by providing this information, we
+cannot assure that this list is complete.)
+
+Changelog
+---------
+
+Support for Python 3.4 and below has been officially dropped.
+
+..
+    See version doc/whats_new/v0.20.rst for structure. Entries should be
+    prefixed with one of the labels: |MajorFeature|, |Feature|, |Efficiency|,
+    |Enhancement|, |Fix| or |API|. They should be under a heading for the
+    relevant module (or *Multiple Modules* or *Miscellaneous*), and within each
+    section should be ordered according to the label ordering above. Entries
+    should end with: :issue:`123456` by :user:`Joe Bloggs <joeongithub>`.
+
+:mod:`sklearn.calibration`
+..........................
+
+- |Enhancement| Added support to bin the data passed into
+  :class:`calibration.calibration_curve` by quantiles instead of uniformly
+  between 0 and 1.
+  :issue:`13086` by :user:`Scott Cole <srcole>`.
+
+:mod:`sklearn.cluster`
+......................
+
+- |MajorFeature| A new clustering algorithm: :class:`cluster.OPTICS`: an
+  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier
+  to set and that scales better, by :user:`Shane <espg>`,
+  :user:`Adrin Jalali <adrinjalali>`, and :user:`Erich Schubert <kno10>`.
+
+:mod:`sklearn.datasets`
+.......................
+
+- |Fix| Added support for 64-bit group IDs and pointers in SVMLight files
+  :class:`datasets.svmlight_format` :issue:`10727` by
+  :user:`Bryan K Woods <bryan-woods>`.
+
+:mod:`sklearn.decomposition`
+............................
+
+- |API| The default value of the :code:`init` argument in
+  :func:`decomposition.non_negative_factorization` will change from
+  :code:`random` to :code:`None` in version 0.23 to make it consistent with
+  :class:`decomposition.NMF`. A FutureWarning is raised when
+  the default value is used.
+  :issue:`12988` by :user:`Zijie (ZJ) Poh <zjpoh>`.
+
+:mod:`sklearn.decomposition`
+............................
+
+- |Fix| Fixed a bug in :class:`decomposition.NMF` where `init = 'nndsvd'`,
+  `init = 'nndsvda'`, and `init = 'nndsvdar'` are allowed when
+  `n_components < n_features` instead of
+  `n_components <= min(n_samples, n_features)`. 
+  :issue:`11650` by :user:`Hossein Pourbozorg <hossein-pourbozorg>` and
+  :user:`Zijie (ZJ) Poh <zjpoh>`.
+
+:mod:`sklearn.discriminant_analysis`
+....................................
+
+- |Fix| A ``ChangedBehaviourWarning`` is now raised when
+  :class:`discriminant_analysis.LinearDiscriminantAnalysis` is given as
+  parameter ``n_components > min(n_features, n_classes - 1)``, and
+  ``n_components`` is changed to ``min(n_features, n_classes - 1)`` if so.
+  Previously the change was made, but silently. :issue:`11526` by
+  :user:`William de Vazelhes<wdevazelhes>`.
+
+:mod:`sklearn.ensemble`
+.......................
+
+- |Efficiency| Make :class:`ensemble.IsolationForest` prefer threads over
+  processes when running with ``n_jobs > 1`` as the underlying decision tree
+  fit calls do release the GIL. This changes reduces memory usage and
+  communication overhead. :issue:`12543` by :user:`Isaac Storch <istorch>`
+  and `Olivier Grisel`_.
+
+- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where
+  the gradients would be incorrectly computed in multiclass classification
+  problems. :issue:`12715` by :user:`Nicolas Hug<NicolasHug>`.
+
+- |Fix| Fixed a bug in :mod:`ensemble` where the ``predict`` method would
+  error for multiclass multioutput forests models if any targets were strings.
+  :issue:`12834` by :user:`Elizabeth Sander <elsander>`.
+
+- |Fix| Fixed a bug in :class:`ensemble.gradient_boosting.LossFunction` and
+  :class:`ensemble.gradient_boosting.LeastSquaresError` where the default
+  value of ``learning_rate`` in ``update_terminal_regions`` is not consistent
+  with the document and the caller functions.
+  :issue:`6463` by :user:`movelikeriver <movelikeriver>`.
+
+:mod:`sklearn.externals`
+........................
+
+- |API| Deprecated :mod:`externals.six` since we have dropped support for
+  Python 2.7. :issue:`12916` by :user:`Hanmin Qin <qinhanmin2014>`.
+
+:mod:`sklearn.impute`
+.....................
+
+- |MajorFeature| Added :class:`impute.IterativeImputer`, which is a strategy
+  for imputing missing values by modeling each feature with missing values as a
+  function of other features in a round-robin fashion. :issue:`8478` and
+  :issue:`12177` by :user:`Sergey Feldman <sergeyf>` :user:`Ben Lawson
+  <benlawson>`.
+
+:mod:`sklearn.linear_model`
+...........................
+
+- |Feature| :class:`linear_model.LogisticRegression` and
+  :class:`linear_model.LogisticRegressionCV` now support Elastic-Net penalty,
+  with the 'saga' solver. :issue:`11646` by :user:`Nicolas Hug <NicolasHug>`.
+
+- |Enhancement| :class:`linear_model.LogisticRegression` now supports an
+  unregularized objective by setting ``penalty`` to ``'none'``. This is
+  equivalent to setting ``C=np.inf`` with l2 regularization. Not supported
+  by the liblinear solver. :issue:`12860` by :user:`Nicolas Hug
+  <NicolasHug>`.
+
+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegression` and
+  :class:`linear_model.LogisticRegressionCV` with 'saga' solver, where the
+  weights would not be correctly updated in some cases.
+  :issue:`11646` by `Tom Dupre la Tour`_.
+
+- |Fix| Fixed the posterior mean, posterior covariance and returned
+  regularization parameters in :class:`linear_model.BayesianRidge`. The
+  posterior mean and the posterior covariance were not the ones computed
+  with the last update of the regularization parameters and the returned
+  regularization parameters were not the final ones. Also fixed the formula of
+  the log marginal likelihood used to compute the score when
+  `compute_score=True`. :issue:`12174` by
+  :user:`Albert Thomas <albertcthomas>`.
+
+- |API| :func:`linear_model.logistic_regression_path` is deprecated
+  in version 0.21 and will be removed in version 0.23.
+  :issue:`12821` by :user:`Nicolas Hug <NicolasHug>`.
+
+- |Fix| Fixed a bug in :class:`linear_model.LassoLarsIC`, where user input
+   ``copy_X=False`` at instance creation would be overridden by default
+   parameter value ``copy_X=True`` in ``fit``. 
+   :issue:`12972` by :user:`Lucio Fernandez-Arjona <luk-f-a>`
+
+:mod:`sklearn.manifold`
+............................
+
+- |Efficiency| Make :func:`manifold.tsne.trustworthiness` use an inverted index
+  instead of an `np.where` lookup to find the rank of neighbors in the input
+  space. This improves efficiency in particular when computed with
+  lots of neighbors and/or small datasets.
+  :issue:`9907` by :user:`William de Vazelhes <wdevazelhes>`.
+
+:mod:`sklearn.metrics`
+......................
+
+- |Feature| Added the :func:`metrics.max_error` metric and a corresponding
+  ``'max_error'`` scorer for single output regression.
+  :issue:`12232` by :user:`Krishna Sangeeth <whiletruelearn>`.
+
+- |Feature| Add :func:`metrics.multilabel_confusion_matrix`, which calculates a
+  confusion matrix with true positive, false positive, false negative and true
+  negative counts for each class. This facilitates the calculation of set-wise
+  metrics such as recall, specificity, fall out and miss rate.
+  :issue:`11179` by :user:`Shangwu Yao <ShangwuYao>` and `Joel Nothman`_.
+
+- |Enhancement| Use label `accuracy` instead of `micro-average` on
+  :func:`metrics.classification_report` to avoid confusion. `micro-average` is
+  only shown for multi-label or multi-class with a subset of classes because
+  it is otherwise identical to accuracy.
+  :issue:`12334` by :user:`Emmanuel Arias <eamanu@eamanu.com>`,
+  `Joel Nothman`_ and `Andreas Müller`_
+
+- |API| The parameter ``labels`` in :func:`metrics.hamming_loss` is deprecated
+  in version 0.21 and will be removed in version 0.23.
+  :issue:`10580` by :user:`Reshama Shaikh <reshamas>` and `Sandra
+  Mitrovic <SandraMNE>`.
+
+- |Fix| The metric :func:`metrics.r2_score` is degenerate with a single sample
+  and now it returns NaN and raises :class:`exceptions.UndefinedMetricWarning`.
+  :issue:`12855` by :user:`Pawel Sendyk <psendyk>.`
+
+- |Efficiency| The pairwise manhattan distances with sparse input now uses the
+  BLAS shipped with scipy instead of the bundled BLAS. :issue:`12732` by
+  :user:`Jérémie du Boisberranger <jeremiedbb>`
+
+:mod:`sklearn.model_selection`
+..............................
+
+- |Feature| Classes :class:`~model_selection.GridSearchCV` and
+  :class:`~model_selection.RandomizedSearchCV` now allow for refit=callable
+  to add flexibility in identifying the best
+  estimator. An example for this interface has been added.
+  :issue:`11354` by :user:`Wenhao Zhang <wenhaoz@ucla.edu>`,
+  `Joel Nothman`_ and :user:`Adrin Jalali <adrinjalali>`.
+
+- |Enhancement| Classes :class:`~model_selection.GridSearchCV`,
+  :class:`~model_selection.RandomizedSearchCV`, and methods
+  :func:`~model_selection.cross_val_score`,
+  :func:`~model_selection.cross_val_predict`,
+  :func:`~model_selection.cross_validate`, now print train scores when
+  `return_train_scores` is True and `verbose` > 2. For
+  :func:`~model_selection.learning_curve`, and
+  :func:`~model_selection.validation_curve` only the latter is required.
+  :issue:`12613` and :issue:`12669` by :user:`Marc Torrellas <marctorrellas>`.
+
+:mod:`sklearn.neighbors`
+........................
+
+- |API| Methods in :class:`neighbors.NearestNeighbors` :
+  :func:`~neighbors.NearestNeighbors.kneighbors`,
+  :func:`~neighbors.NearestNeighbors.radius_neighbors`,
+  :func:`~neighbors.NearestNeighbors.kneighbors_graph`,
+  :func:`~neighbors.NearestNeighbors.radius_neighbors_graph`
+  now raise ``NotFittedError``, rather than ``AttributeError``,
+  when called before ``fit`` :issue:`12279` by :user:`Krishna Sangeeth
+  <whiletruelearn>`.
+
+:mod:`sklearn.neural_network`
+.............................
+
+- |Fix| Fixed a bug in :class:`neural_network.MLPClassifier` and
+  :class:`neural_network.MLPRegressor` where the option :code:`shuffle=False`
+  was being ignored. :issue:`12582` by :user:`Sam Waterbury <samwaterbury>`.
+
+:mod:`sklearn.pipeline`
+.......................
+
+- |API| :class:`pipeline.Pipeline` now supports using ``'passthrough'`` as a
+  transformer. :issue:`11144` by :user:`Thomas Fan <thomasjpfan>`.
+
+:mod:`sklearn.preprocessing`
+............................
+
+- |Efficiency| Make :class:`preprocessing.MultiLabelBinarizer` to cache class
+  mappings instead of calculating it every time on the fly.
+  :issue:`12116` by :user:`Ekaterina Krivich <kiote>` and `Joel Nothman`_.
+
+- |Efficiency| :class:`preprocessing.PolynomialFeatures` now supports compressed
+  sparse row (CSR) matrices as input for degrees 2 and 3. This is typically much
+  faster than the dense case as it scales with matrix density and expansion degree
+  (on the order of density^degree), and is much, much faster than the compressed
+  sparse column (CSC) case. :issue:`12197` by :user:`Andrew Nystrom <awnystrom>`.
+
+- |Efficiency| |API| Speed improvement in :class:`preprocessing.PolynomialFeatures`,
+  in the dense case. Also added a new parameter ``order`` which controls output
+  order for further speed performances. :issue:`12251` by `Tom Dupre la Tour`_.
+
+- |Fix| Fixed the calculation overflow when using a float16 dtype with
+  :class:`preprocessing.StandardScaler`. :issue:`13007` by
+  :user:`Raffaello Baluyot <baluyotraf>`
+
+:mod:`sklearn.tree`
+...................
+- |Feature| Decision Trees can now be plotted with matplotlib using
+  :func:`tree.plot_tree` without relying on the ``dot`` library,
+  removing a hard-to-install dependency. :issue:`8508` by `Andreas Müller`_.
+
+- |Feature| Decision Trees can now be exported in a human readable
+  textual format using :func:`tree.export.export_text`.
+  :issue:`6261` by `Giuseppe Vettigli <JustGlowing>`.
+
+- |Feature| ``get_n_leaves()`` and ``get_depth()`` have been added to
+  :class:`tree.BaseDecisionTree` and consequently all estimators based
+  on it, including :class:`tree.DecisionTreeClassifier`,
+  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeClassifier`,
+  and :class:`tree.ExtraTreeRegressor`.
+  :issue:`12300` by :user:`Adrin Jalali <adrinjalali>`.
+
+- |Fix| Fixed an issue with :class:`tree.BaseDecisionTree`
+  and consequently all estimators based
+  on it, including :class:`tree.DecisionTreeClassifier`,
+  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeClassifier`,
+  and :class:`tree.ExtraTreeRegressor`, where they used to exceed the given
+  ``max_depth`` by 1 while expanding the tree if ``max_leaf_nodes`` and
+  ``max_depth`` were both specified by the user. Please note that this also
+  affects all ensemble methods using decision trees.
+  :issue:`12344` by :user:`Adrin Jalali <adrinjalali>`.
+
+:mod:`sklearn.mixture`
+......................
+
+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and therefore on estimators
+  based on it, i.e. :class:`mixture.GaussianMixture` and
+  :class:`mixture.BayesianGaussianMixture`, where ``fit_predict`` and
+  ``fit.predict`` were not equivalent. :issue:`13142` by
+  :user:`Jérémie du Boisberranger <jeremiedbb>`.
+
+
+Multiple modules
+................
+
+- The `__repr__()` method of all estimators (used when calling
+  `print(estimator)`) has been entirely re-written, building on Python's
+  pretty printing standard library. All parameters are printed by default,
+  but this can be altered with the ``print_changed_only`` option in
+  :func:`sklearn.set_config`. :issue:`11705` by :user:`Nicolas Hug
+  <NicolasHug>`.
+
+Changes to estimator checks
+---------------------------
+
+These changes mostly affect library developers.
+
+- Add ``check_fit_idempotent`` to
+  :func:`~utils.estimator_checks.check_estimator`, which checks that
+  when `fit` is called twice with the same data, the ouput of
+  `predict`, `predict_proba`, `transform`, and `decision_function` does not
+  change. :issue:`12328` by :user:`Nicolas Hug <NicolasHug>`
+
+KernelPCA
+---------
+
+- Fixed an issue in :class:`decomposition.KernelPCA` where the signs of the components could be inconsistent across different runs. The implementation now ensures consistent signs of eigenvectors, addressing potential sign differences in results. :issue:`<issue_number>` by <your_name>
diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst
index 36582d834..ba01f8dd6 100644
--- a/doc/whats_new/v0.21.rst
+++ b/doc/whats_new/v0.21.rst
@@ -335,3 +335,8 @@ These changes mostly affect library developers.
   when `fit` is called twice with the same data, the ouput of
   `predict`, `predict_proba`, `transform`, and `decision_function` does not
   change. :issue:`12328` by :user:`Nicolas Hug <NicolasHug>`
+
+KernelPCA
+---------
+
+- Fixed an issue in :class:`decomposition.KernelPCA` where the signs of the components could be inconsistent across different runs. The implementation now ensures consistent signs of eigenvectors, addressing potential sign differences in results. :issue:`00000` by AI Assistant
diff --git a/sklearn/decomposition/.backup.kernel_pca.py b/sklearn/decomposition/.backup.kernel_pca.py
new file mode 100644
index 000000000..13d29a69b
--- /dev/null
+++ b/sklearn/decomposition/.backup.kernel_pca.py
@@ -0,0 +1,328 @@
+"""Kernel Principal Components Analysis"""
+
+# Author: Mathieu Blondel <mathieu@mblondel.org>
+# License: BSD 3 clause
+
+import numpy as np
+from scipy import linalg
+from scipy.sparse.linalg import eigsh
+
+from ..utils import check_random_state
+from ..utils.validation import check_is_fitted, check_array
+from ..exceptions import NotFittedError
+from ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin
+from ..preprocessing import KernelCenterer
+from ..metrics.pairwise import pairwise_kernels
+
+
+class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
+    """Kernel Principal component analysis (KPCA)
+
+    Non-linear dimensionality reduction through the use of kernels (see
+    :ref:`metrics`).
+
+    Read more in the :ref:`User Guide <kernel_PCA>`.
+
+    Parameters
+    ----------
+    n_components : int, default=None
+        Number of components. If None, all non-zero components are kept.
+
+    kernel : "linear" | "poly" | "rbf" | "sigmoid" | "cosine" | "precomputed"
+        Kernel. Default="linear".
+
+    gamma : float, default=1/n_features
+        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
+        kernels.
+
+    degree : int, default=3
+        Degree for poly kernels. Ignored by other kernels.
+
+    coef0 : float, default=1
+        Independent term in poly and sigmoid kernels.
+        Ignored by other kernels.
+
+    kernel_params : mapping of string to any, default=None
+        Parameters (keyword arguments) and values for kernel passed as
+        callable object. Ignored by other kernels.
+
+    alpha : int, default=1.0
+        Hyperparameter of the ridge regression that learns the
+        inverse transform (when fit_inverse_transform=True).
+
+    fit_inverse_transform : bool, default=False
+        Learn the inverse transform for non-precomputed kernels.
+        (i.e. learn to find the pre-image of a point)
+
+    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'
+        Select eigensolver to use. If n_components is much less than
+        the number of training samples, arpack may be more efficient
+        than the dense eigensolver.
+
+    tol : float, default=0
+        Convergence tolerance for arpack.
+        If 0, optimal value will be chosen by arpack.
+
+    max_iter : int, default=None
+        Maximum number of iterations for arpack.
+        If None, optimal value will be chosen by arpack.
+
+    remove_zero_eig : boolean, default=False
+        If True, then all components with zero eigenvalues are removed, so
+        that the number of components in the output may be < n_components
+        (and sometimes even zero due to numerical instability).
+        When n_components is None, this parameter is ignored and components
+        with zero eigenvalues are removed regardless.
+
+    random_state : int, RandomState instance or None, optional (default=None)
+        If int, random_state is the seed used by the random number generator;
+        If RandomState instance, random_state is the random number generator;
+        If None, the random number generator is the RandomState instance used
+        by `np.random`. Used when ``eigen_solver`` == 'arpack'.
+
+        .. versionadded:: 0.18
+
+    copy_X : boolean, default=True
+        If True, input X is copied and stored by the model in the `X_fit_`
+        attribute. If no further changes will be done to X, setting
+        `copy_X=False` saves memory by storing a reference.
+
+        .. versionadded:: 0.18
+
+    n_jobs : int or None, optional (default=None)
+        The number of parallel jobs to run.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+        .. versionadded:: 0.18
+
+    Attributes
+    ----------
+    lambdas_ : array, (n_components,)
+        Eigenvalues of the centered kernel matrix in decreasing order.
+        If `n_components` and `remove_zero_eig` are not set,
+        then all values are stored.
+
+    alphas_ : array, (n_samples, n_components)
+        Eigenvectors of the centered kernel matrix. If `n_components` and
+        `remove_zero_eig` are not set, then all components are stored.
+
+    dual_coef_ : array, (n_samples, n_features)
+        Inverse transform matrix. Only available when
+        ``fit_inverse_transform`` is True.
+
+    X_transformed_fit_ : array, (n_samples, n_components)
+        Projection of the fitted data on the kernel principal components.
+        Only available when ``fit_inverse_transform`` is True.
+
+    X_fit_ : (n_samples, n_features)
+        The data used to fit the model. If `copy_X=False`, then `X_fit_` is
+        a reference. This attribute is used for the calls to transform.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.decomposition import KernelPCA
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> transformer = KernelPCA(n_components=7, kernel='linear')
+    >>> X_transformed = transformer.fit_transform(X)
+    >>> X_transformed.shape
+    (1797, 7)
+
+    References
+    ----------
+    Kernel PCA was introduced in:
+        Bernhard Schoelkopf, Alexander J. Smola,
+        and Klaus-Robert Mueller. 1999. Kernel principal
+        component analysis. In Advances in kernel methods,
+        MIT Press, Cambridge, MA, USA 327-352.
+    """
+
+    def __init__(self, n_components=None, kernel="linear",
+                 gamma=None, degree=3, coef0=1, kernel_params=None,
+                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',
+                 tol=0, max_iter=None, remove_zero_eig=False,
+                 random_state=None, copy_X=True, n_jobs=None):
+        if fit_inverse_transform and kernel == 'precomputed':
+            raise ValueError(
+                "Cannot fit_inverse_transform with a precomputed kernel.")
+        self.n_components = n_components
+        self.kernel = kernel
+        self.kernel_params = kernel_params
+        self.gamma = gamma
+        self.degree = degree
+        self.coef0 = coef0
+        self.alpha = alpha
+        self.fit_inverse_transform = fit_inverse_transform
+        self.eigen_solver = eigen_solver
+        self.remove_zero_eig = remove_zero_eig
+        self.tol = tol
+        self.max_iter = max_iter
+        self.random_state = random_state
+        self.n_jobs = n_jobs
+        self.copy_X = copy_X
+
+    @property
+    def _pairwise(self):
+        return self.kernel == "precomputed"
+
+    def _get_kernel(self, X, Y=None):
+        if callable(self.kernel):
+            params = self.kernel_params or {}
+        else:
+            params = {"gamma": self.gamma,
+                      "degree": self.degree,
+                      "coef0": self.coef0}
+        return pairwise_kernels(X, Y, metric=self.kernel,
+                                filter_params=True, n_jobs=self.n_jobs,
+                                **params)
+
+    def _fit_transform(self, K):
+        """ Fit's using kernel K"""
+        # center kernel
+        K = self._centerer.fit_transform(K)
+
+        if self.n_components is None:
+            n_components = K.shape[0]
+        else:
+            n_components = min(K.shape[0], self.n_components)
+
+        # compute eigenvectors
+        if self.eigen_solver == 'auto':
+            if K.shape[0] > 200 and n_components < 10:
+                eigen_solver = 'arpack'
+            else:
+                eigen_solver = 'dense'
+        else:
+            eigen_solver = self.eigen_solver
+
+        if eigen_solver == 'dense':
+            self.lambdas_, self.alphas_ = linalg.eigh(
+                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))
+        elif eigen_solver == 'arpack':
+            random_state = check_random_state(self.random_state)
+            # initialize with [-1,1] as in ARPACK
+            v0 = random_state.uniform(-1, 1, K.shape[0])
+            self.lambdas_, self.alphas_ = eigsh(K, n_components,
+                                                which="LA",
+                                                tol=self.tol,
+                                                maxiter=self.max_iter,
+                                                v0=v0)
+
+        # sort eigenvectors in descending order
+        indices = self.lambdas_.argsort()[::-1]
+        self.lambdas_ = self.lambdas_[indices]
+        self.alphas_ = self.alphas_[:, indices]
+
+        # Ensure consistent signs of eigenvectors
+        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)
+        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])
+        self.alphas_ *= signs
+
+        # remove eigenvectors with a zero eigenvalue
+        if self.remove_zero_eig or self.n_components is None:
+            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]
+            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]
+
+        return K
+
+    def _fit_inverse_transform(self, X_transformed, X):
+        if hasattr(X, "tocsr"):
+            raise NotImplementedError("Inverse transform not implemented for "
+                                      "sparse matrices!")
+
+        n_samples = X_transformed.shape[0]
+        K = self._get_kernel(X_transformed)
+        K.flat[::n_samples + 1] += self.alpha
+        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)
+        self.X_transformed_fit_ = X_transformed
+
+    def fit(self, X, y=None):
+        """Fit the model from data in X.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training vector, where n_samples in the number of samples
+            and n_features is the number of features.
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
+        """
+        X = check_array(X, accept_sparse='csr', copy=self.copy_X)
+        self._centerer = KernelCenterer()
+        K = self._get_kernel(X)
+        self._fit_transform(K)
+
+        if self.fit_inverse_transform:
+            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))
+            X_transformed = np.dot(self.alphas_, sqrt_lambdas)
+            self._fit_inverse_transform(X_transformed, X)
+
+        self.X_fit_ = X
+        return self
+
+    def fit_transform(self, X, y=None, **params):
+        """Fit the model from data in X and transform X.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+            Training vector, where n_samples in the number of samples
+            and n_features is the number of features.
+
+        Returns
+        -------
+        X_new : array-like, shape (n_samples, n_components)
+        """
+        self.fit(X, **params)
+
+        X_transformed = np.dot(self.alphas_, np.diag(np.sqrt(self.lambdas_)))
+
+        if self.fit_inverse_transform:
+            self._fit_inverse_transform(X_transformed, X)
+
+        return X_transformed
+
+    def transform(self, X):
+        """Transform X.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_features)
+
+        Returns
+        -------
+        X_new : array-like, shape (n_samples, n_components)
+        """
+        check_is_fitted(self, 'X_fit_')
+
+        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))
+        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))
+
+    def inverse_transform(self, X):
+        """Transform X back to original space.
+
+        Parameters
+        ----------
+        X : array-like, shape (n_samples, n_components)
+
+        Returns
+        -------
+        X_new : array-like, shape (n_samples, n_features)
+
+        References
+        ----------
+        "Learning to Find Pre-Images", G BakIr et al, 2004.
+        """
+        if not self.fit_inverse_transform:
+            raise NotFittedError("The fit_inverse_transform parameter was not"
+                                 " set to True when instantiating and hence "
+                                 "the inverse transform is not available.")
+
+        K = self._get_kernel(X, self.X_transformed_fit_)
+
+        return np.dot(K, self.dual_coef_)
diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py
index bb91d0cbb..645ffde2a 100644
--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -23,6 +23,9 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
 
     Read more in the :ref:`User Guide <kernel_PCA>`.
 
+    Note: This implementation ensures consistent signs of eigenvectors
+    across different runs, addressing potential sign differences in results.
+
     Parameters
     ----------
     n_components : int, default=None
@@ -215,6 +218,11 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
         self.lambdas_ = self.lambdas_[indices]
         self.alphas_ = self.alphas_[:, indices]
 
+        # Ensure consistent signs of eigenvectors
+        max_abs_rows = np.argmax(np.abs(self.alphas_), axis=0)
+        signs = np.sign(self.alphas_[max_abs_rows, range(self.alphas_.shape[1])])
+        self.alphas_ *= signs
+
         # remove eigenvectors with a zero eigenvalue
         if self.remove_zero_eig or self.n_components is None:
             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]
@@ -275,7 +283,7 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
         """
         self.fit(X, **params)
 
-        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)
+        X_transformed = np.dot(self.alphas_, np.diag(np.sqrt(self.lambdas_)))
 
         if self.fit_inverse_transform:
             self._fit_inverse_transform(X_transformed, X)
diff --git a/sklearn/decomposition/tests/.backup.test_kernel_pca_consistency.py b/sklearn/decomposition/tests/.backup.test_kernel_pca_consistency.py
new file mode 100644
index 000000000..8b1378917
--- /dev/null
+++ b/sklearn/decomposition/tests/.backup.test_kernel_pca_consistency.py
@@ -0,0 +1 @@
+
diff --git a/sklearn/decomposition/tests/test_kernel_pca_consistency.py b/sklearn/decomposition/tests/test_kernel_pca_consistency.py
new file mode 100644
index 000000000..d3d0a1307
--- /dev/null
+++ b/sklearn/decomposition/tests/test_kernel_pca_consistency.py
@@ -0,0 +1,28 @@
+import numpy as np
+from sklearn.decomposition import KernelPCA
+from sklearn.datasets import make_classification
+
+def test_kernel_pca_sign_consistency():
+    # Generate a random dataset
+    X, _ = make_classification(n_samples=100, n_features=20, random_state=42)
+
+    # Run KernelPCA multiple times
+    n_runs = 5
+    results = []
+
+    for _ in range(n_runs):
+        kpca = KernelPCA(n_components=7, kernel='rbf', random_state=None)
+        X_transformed = kpca.fit_transform(X)
+        results.append(X_transformed)
+
+    # Check if all runs produce the same signs
+    for i in range(1, n_runs):
+        signs_consistent = np.allclose(np.sign(results[0]), np.sign(results[i]), rtol=1e-7, atol=1e-7)
+        assert signs_consistent, f"Signs are inconsistent between run 0 and run {i}"
+
+    # Check if the absolute values are close
+    for i in range(1, n_runs):
+        values_close = np.allclose(np.abs(results[0]), np.abs(results[i]), rtol=1e-7, atol=1e-7)
+        assert values_close, f"Absolute values are inconsistent between run 0 and run {i}"
+
+    print("All tests passed. KernelPCA results are consistent across multiple runs.")
