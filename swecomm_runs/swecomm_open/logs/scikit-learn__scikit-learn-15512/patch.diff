diff --git a/sklearn/cluster/.backup._affinity_propagation.py b/sklearn/cluster/.backup._affinity_propagation.py
new file mode 100644
index 000000000..9e640c65c
--- /dev/null
+++ b/sklearn/cluster/.backup._affinity_propagation.py
@@ -0,0 +1,443 @@
+"""Affinity Propagation clustering algorithm."""
+
+# Author: Alexandre Gramfort alexandre.gramfort@inria.fr
+#        Gael Varoquaux gael.varoquaux@normalesup.org
+
+# License: BSD 3 clause
+
+import numpy as np
+import warnings
+
+from ..exceptions import ConvergenceWarning
+from ..base import BaseEstimator, ClusterMixin
+from ..utils import as_float_array, check_array
+from ..utils.validation import check_is_fitted
+from ..metrics import euclidean_distances
+from ..metrics import pairwise_distances_argmin
+
+
+def _equal_similarities_and_preferences(S, preference):
+    def all_equal_preferences():
+        return np.all(preference == preference.flat[0])
+
+    def all_equal_similarities():
+        # Create mask to ignore diagonal of S
+        mask = np.ones(S.shape, dtype=bool)
+        np.fill_diagonal(mask, 0)
+
+        return np.all(S[mask].flat == S[mask].flat[0])
+
+    return all_equal_preferences() and all_equal_similarities()
+
+
+def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
+                         damping=0.5, copy=True, verbose=False,
+                         return_n_iter=False):
+    """Perform Affinity Propagation Clustering of data
+
+    Read more in the :ref:`User Guide <affinity_propagation>`.
+
+    Parameters
+    ----------
+
+    S : array-like, shape (n_samples, n_samples)
+        Matrix of similarities between points
+
+    preference : array-like, shape (n_samples,) or float, optional
+        Preferences for each point - points with larger values of
+        preferences are more likely to be chosen as exemplars. The number of
+        exemplars, i.e. of clusters, is influenced by the input preferences
+        value. If the preferences are not passed as arguments, they will be
+        set to the median of the input similarities (resulting in a moderate
+        number of clusters). For a smaller amount of clusters, this can be set
+        to the minimum value of the similarities.
+
+    convergence_iter : int, optional, default: 15
+        Number of iterations with no change in the number
+        of estimated clusters that stops the convergence.
+
+    max_iter : int, optional, default: 200
+        Maximum number of iterations
+
+    damping : float, optional, default: 0.5
+        Damping factor between 0.5 and 1.
+
+    copy : boolean, optional, default: True
+        If copy is False, the affinity matrix is modified inplace by the
+        algorithm, for memory efficiency
+
+    verbose : boolean, optional, default: False
+        The verbosity level
+
+    return_n_iter : bool, default False
+        Whether or not to return the number of iterations.
+
+    Returns
+    -------
+
+    cluster_centers_indices : array, shape (n_clusters,)
+        index of clusters centers
+
+    labels : array, shape (n_samples,)
+        cluster labels for each point
+
+    n_iter : int
+        number of iterations run. Returned only if `return_n_iter` is
+        set to True.
+
+    Notes
+    -----
+    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
+    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.
+
+    When the algorithm does not converge, it returns an empty array as
+    ``cluster_center_indices`` and ``-1`` as label for each training sample.
+
+    When all training samples have equal similarities and equal preferences,
+    the assignment of cluster centers and labels depends on the preference.
+    If the preference is smaller than the similarities, a single cluster center
+    and label ``0`` for every sample will be returned. Otherwise, every
+    training sample becomes its own cluster center and is assigned a unique
+    label.
+
+    References
+    ----------
+    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages
+    Between Data Points", Science Feb. 2007
+    """
+    S = as_float_array(S, copy=copy)
+    n_samples = S.shape[0]
+
+    if S.shape[0] != S.shape[1]:
+        raise ValueError("S must be a square array (shape=%s)" % repr(S.shape))
+
+    if preference is None:
+        preference = np.median(S)
+    if damping < 0.5 or damping >= 1:
+        raise ValueError('damping must be >= 0.5 and < 1')
+
+    preference = np.array(preference)
+
+    if (n_samples == 1 or
+            _equal_similarities_and_preferences(S, preference)):
+        # It makes no sense to run the algorithm in this case, so return 1 or
+        # n_samples clusters, depending on preferences
+        warnings.warn("All samples have mutually equal similarities. "
+                      "Returning arbitrary cluster center(s).")
+        if preference.flat[0] >= S.flat[n_samples - 1]:
+            return ((np.arange(n_samples), np.arange(n_samples), 0)
+                    if return_n_iter
+                    else (np.arange(n_samples), np.arange(n_samples)))
+        else:
+            return ((np.array([0]), np.array([0] * n_samples), 0)
+                    if return_n_iter
+                    else (np.array([0]), np.array([0] * n_samples)))
+
+    random_state = np.random.RandomState(0)
+
+    # Place preference on the diagonal of S
+    S.flat[::(n_samples + 1)] = preference
+
+    A = np.zeros((n_samples, n_samples))
+    R = np.zeros((n_samples, n_samples))  # Initialize messages
+    # Intermediate results
+    tmp = np.zeros((n_samples, n_samples))
+
+    # Remove degeneracies
+    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *
+          random_state.randn(n_samples, n_samples))
+
+    # Execute parallel affinity propagation updates
+    e = np.zeros((n_samples, convergence_iter))
+
+    ind = np.arange(n_samples)
+
+    for it in range(max_iter):
+        # tmp = A + S; compute responsibilities
+        np.add(A, S, tmp)
+        I = np.argmax(tmp, axis=1)
+        Y = tmp[ind, I]  # np.max(A + S, axis=1)
+        tmp[ind, I] = -np.inf
+        Y2 = np.max(tmp, axis=1)
+
+        # tmp = Rnew
+        np.subtract(S, Y[:, None], tmp)
+        tmp[ind, I] = S[ind, I] - Y2
+
+        # Damping
+        tmp *= 1 - damping
+        R *= damping
+        R += tmp
+
+        # tmp = Rp; compute availabilities
+        np.maximum(R, 0, tmp)
+        tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]
+
+        # tmp = -Anew
+        tmp -= np.sum(tmp, axis=0)
+        dA = np.diag(tmp).copy()
+        tmp.clip(0, np.inf, tmp)
+        tmp.flat[::n_samples + 1] = dA
+
+        # Damping
+        tmp *= 1 - damping
+        A *= damping
+        A -= tmp
+
+        # Check for convergence
+        E = (np.diag(A) + np.diag(R)) > 0
+        e[:, it % convergence_iter] = E
+        K = np.sum(E, axis=0)
+
+        if it >= convergence_iter:
+            se = np.sum(e, axis=1)
+            unconverged = (np.sum((se == convergence_iter) + (se == 0))
+                           != n_samples)
+            if (not unconverged and (K > 0)) or (it == max_iter):
+                if verbose:
+                    print("Converged after %d iterations." % it)
+                break
+    else:
+        if verbose:
+            print("Did not converge")
+
+    I = np.flatnonzero(E)
+    K = I.size  # Identify exemplars
+
+    if K > 0 and it < max_iter - 1:
+        c = np.argmax(S[:, I], axis=1)
+        c[I] = np.arange(K)  # Identify clusters
+        # Refine the final set of exemplars and clusters and return results
+        for k in range(K):
+            ii = np.where(c == k)[0]
+            j = np.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))
+            I[k] = ii[j]
+
+        c = np.argmax(S[:, I], axis=1)
+        c[I] = np.arange(K)
+        labels = I[c]
+        # Reduce labels to a sorted, gapless, list
+        cluster_centers_indices = np.unique(labels)
+        labels = np.searchsorted(cluster_centers_indices, labels)
+    else:
+        warnings.warn("Affinity propagation did not converge, this model "
+                      "will not have any cluster centers.", ConvergenceWarning)
+        labels = np.array([-1] * n_samples)
+        cluster_centers_indices = np.array([], dtype=int)
+
+    if return_n_iter:
+        return cluster_centers_indices, labels, it + 1
+    else:
+        return cluster_centers_indices, labels
+
+
+###############################################################################
+
+class AffinityPropagation(ClusterMixin, BaseEstimator):
+    """Perform Affinity Propagation Clustering of data.
+
+    Read more in the :ref:`User Guide <affinity_propagation>`.
+
+    Parameters
+    ----------
+    damping : float, optional, default: 0.5
+        Damping factor (between 0.5 and 1) is the extent to
+        which the current value is maintained relative to
+        incoming values (weighted 1 - damping). This in order
+        to avoid numerical oscillations when updating these
+        values (messages).
+
+    max_iter : int, optional, default: 200
+        Maximum number of iterations.
+
+    convergence_iter : int, optional, default: 15
+        Number of iterations with no change in the number
+        of estimated clusters that stops the convergence.
+
+    copy : boolean, optional, default: True
+        Make a copy of input data.
+
+    preference : array-like, shape (n_samples,) or float, optional
+        Preferences for each point - points with larger values of
+        preferences are more likely to be chosen as exemplars. The number
+        of exemplars, ie of clusters, is influenced by the input
+        preferences value. If the preferences are not passed as arguments,
+        they will be set to the median of the input similarities.
+
+    affinity : string, optional, default=``euclidean``
+        Which affinity to use. At the moment ``precomputed`` and
+        ``euclidean`` are supported. ``euclidean`` uses the
+        negative squared euclidean distance between points.
+
+    verbose : boolean, optional, default: False
+        Whether to be verbose.
+
+
+    Attributes
+    ----------
+    cluster_centers_indices_ : array, shape (n_clusters,)
+        Indices of cluster centers
+
+    cluster_centers_ : array, shape (n_clusters, n_features)
+        Cluster centers (if affinity != ``precomputed``).
+
+    labels_ : array, shape (n_samples,)
+        Labels of each point
+
+    affinity_matrix_ : array, shape (n_samples, n_samples)
+        Stores the affinity matrix used in ``fit``.
+
+    n_iter_ : int
+        Number of iterations taken to converge.
+
+    Examples
+    --------
+    >>> from sklearn.cluster import AffinityPropagation
+    >>> import numpy as np
+    >>> X = np.array([[1, 2], [1, 4], [1, 0],
+    ...               [4, 2], [4, 4], [4, 0]])
+    >>> clustering = AffinityPropagation().fit(X)
+    >>> clustering
+    AffinityPropagation()
+    >>> clustering.labels_
+    array([0, 0, 0, 1, 1, 1])
+    >>> clustering.predict([[0, 0], [4, 4]])
+    array([0, 1])
+    >>> clustering.cluster_centers_
+    array([[1, 2],
+           [4, 2]])
+
+    Notes
+    -----
+    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py
+    <sphx_glr_auto_examples_cluster_plot_affinity_propagation.py>`.
+
+    The algorithmic complexity of affinity propagation is quadratic
+    in the number of points.
+
+    When ``fit`` does not converge, ``cluster_centers_`` becomes an empty
+    array and all training samples will be labelled as ``-1``. In addition,
+    ``predict`` will then label every sample as ``-1``.
+
+    When all training samples have equal similarities and equal preferences,
+    the assignment of cluster centers and labels depends on the preference.
+    If the preference is smaller than the similarities, ``fit`` will result in
+    a single cluster center and label ``0`` for every sample. Otherwise, every
+    training sample becomes its own cluster center and is assigned a unique
+    label.
+
+    References
+    ----------
+
+    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages
+    Between Data Points", Science Feb. 2007
+    """
+
+    def __init__(self, damping=.5, max_iter=200, convergence_iter=15,
+                 copy=True, preference=None, affinity='euclidean',
+                 verbose=False):
+
+        self.damping = damping
+        self.max_iter = max_iter
+        self.convergence_iter = convergence_iter
+        self.copy = copy
+        self.verbose = verbose
+        self.preference = preference
+        self.affinity = affinity
+
+    @property
+    def _pairwise(self):
+        return self.affinity == "precomputed"
+
+    def fit(self, X, y=None):
+        """Fit the clustering from features, or affinity matrix.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape (n_samples, n_features), or \
+            array-like, shape (n_samples, n_samples)
+            Training instances to cluster, or similarities / affinities between
+            instances if ``affinity='precomputed'``. If a sparse feature matrix
+            is provided, it will be converted into a sparse ``csr_matrix``.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        self
+
+        """
+        if self.affinity == "precomputed":
+            accept_sparse = False
+        else:
+            accept_sparse = 'csr'
+        X = check_array(X, accept_sparse=accept_sparse)
+        if self.affinity == "precomputed":
+            self.affinity_matrix_ = X
+        elif self.affinity == "euclidean":
+            self.affinity_matrix_ = -euclidean_distances(X, squared=True)
+        else:
+            raise ValueError("Affinity must be 'precomputed' or "
+                             "'euclidean'. Got %s instead"
+                             % str(self.affinity))
+
+        self.cluster_centers_indices_, self.labels_, self.n_iter_ = \
+            affinity_propagation(
+                self.affinity_matrix_, self.preference, max_iter=self.max_iter,
+                convergence_iter=self.convergence_iter, damping=self.damping,
+                copy=self.copy, verbose=self.verbose, return_n_iter=True)
+
+        if self.affinity != "precomputed":
+            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()
+
+        return self
+
+    def predict(self, X):
+        """Predict the closest cluster each sample in X belongs to.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape (n_samples, n_features)
+            New data to predict. If a sparse matrix is provided, it will be
+            converted into a sparse ``csr_matrix``.
+
+        Returns
+        -------
+        labels : ndarray, shape (n_samples,)
+            Cluster labels.
+        """
+        check_is_fitted(self)
+        if not hasattr(self, "cluster_centers_"):
+            raise ValueError("Predict method is not supported when "
+                             "affinity='precomputed'.")
+
+        if self.cluster_centers_.shape[0] > 0:
+            return pairwise_distances_argmin(X, self.cluster_centers_)
+        else:
+            warnings.warn("This model does not have any cluster centers "
+                          "because affinity propagation did not converge. "
+                          "Labeling every sample as '-1'.", ConvergenceWarning)
+            return np.array([-1] * X.shape[0])
+
+    def fit_predict(self, X, y=None):
+        """Fit the clustering from features or affinity matrix, and return
+        cluster labels.
+
+        Parameters
+        ----------
+        X : array-like or sparse matrix, shape (n_samples, n_features), or \
+            array-like, shape (n_samples, n_samples)
+            Training instances to cluster, or similarities / affinities between
+            instances if ``affinity='precomputed'``. If a sparse feature matrix
+            is provided, it will be converted into a sparse ``csr_matrix``.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        labels : ndarray, shape (n_samples,)
+            Cluster labels.
+        """
+        return super().fit_predict(X, y)
diff --git a/sklearn/cluster/_affinity_propagation.py b/sklearn/cluster/_affinity_propagation.py
index 4806afee9..50233dc1b 100644
--- a/sklearn/cluster/_affinity_propagation.py
+++ b/sklearn/cluster/_affinity_propagation.py
@@ -204,7 +204,7 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
     I = np.flatnonzero(E)
     K = I.size  # Identify exemplars
 
-    if K > 0:
+    if K > 0 and it < max_iter - 1:
         c = np.argmax(S[:, I], axis=1)
         c[I] = np.arange(K)  # Identify clusters
         # Refine the final set of exemplars and clusters and return results
@@ -223,7 +223,7 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,
         warnings.warn("Affinity propagation did not converge, this model "
                       "will not have any cluster centers.", ConvergenceWarning)
         labels = np.array([-1] * n_samples)
-        cluster_centers_indices = []
+        cluster_centers_indices = np.array([], dtype=int)
 
     if return_n_iter:
         return cluster_centers_indices, labels, it + 1
@@ -388,8 +388,11 @@ class AffinityPropagation(ClusterMixin, BaseEstimator):
                 convergence_iter=self.convergence_iter, damping=self.damping,
                 copy=self.copy, verbose=self.verbose, return_n_iter=True)
 
-        if self.affinity != "precomputed":
-            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()
+        if len(self.cluster_centers_indices_) > 0:
+            if self.affinity != "precomputed":
+                self.cluster_centers_ = X[self.cluster_centers_indices_].copy()
+        else:
+            self.cluster_centers_ = np.empty((0, X.shape[1]))
 
         return self
 
diff --git a/sklearn/cluster/tests/.backup.test_affinity_propagation.py b/sklearn/cluster/tests/.backup.test_affinity_propagation.py
new file mode 100644
index 000000000..98f0f79ee
--- /dev/null
+++ b/sklearn/cluster/tests/.backup.test_affinity_propagation.py
@@ -0,0 +1,189 @@
+"""
+Testing for Clustering methods
+
+"""
+
+import numpy as np
+import pytest
+from scipy.sparse import csr_matrix
+
+from sklearn.exceptions import ConvergenceWarning
+from sklearn.utils._testing import (
+    assert_array_equal, assert_warns,
+    assert_warns_message, assert_no_warnings)
+
+from sklearn.cluster import AffinityPropagation
+from sklearn.cluster._affinity_propagation import (
+    _equal_similarities_and_preferences
+)
+from sklearn.cluster import affinity_propagation
+from sklearn.datasets import make_blobs
+from sklearn.metrics import euclidean_distances
+
+n_clusters = 3
+centers = np.array([[1, 1], [-1, -1], [1, -1]]) + 10
+X, _ = make_blobs(n_samples=60, n_features=2, centers=centers,
+                  cluster_std=0.4, shuffle=True, random_state=0)
+
+
+def test_affinity_propagation():
+    # Affinity Propagation algorithm
+    # Compute similarities
+    S = -euclidean_distances(X, squared=True)
+    preference = np.median(S) * 10
+    # Compute Affinity Propagation
+    cluster_centers_indices, labels = affinity_propagation(
+        S, preference=preference)
+
+    n_clusters_ = len(cluster_centers_indices)
+
+    assert n_clusters == n_clusters_
+
+    af = AffinityPropagation(preference=preference, affinity="precomputed")
+    labels_precomputed = af.fit(S).labels_
+
+    af = AffinityPropagation(preference=preference, verbose=True)
+    labels = af.fit(X).labels_
+
+    assert_array_equal(labels, labels_precomputed)
+
+    cluster_centers_indices = af.cluster_centers_indices_
+
+    n_clusters_ = len(cluster_centers_indices)
+    assert np.unique(labels).size == n_clusters_
+    assert n_clusters == n_clusters_
+
+    # Test also with no copy
+    _, labels_no_copy = affinity_propagation(S, preference=preference,
+                                             copy=False)
+    assert_array_equal(labels, labels_no_copy)
+
+    # Test input validation
+    with pytest.raises(ValueError):
+        affinity_propagation(S[:, :-1])
+    with pytest.raises(ValueError):
+        affinity_propagation(S, damping=0)
+    af = AffinityPropagation(affinity="unknown")
+    with pytest.raises(ValueError):
+        af.fit(X)
+    af_2 = AffinityPropagation(affinity='precomputed')
+    with pytest.raises(TypeError):
+        af_2.fit(csr_matrix((3, 3)))
+
+def test_affinity_propagation_predict():
+    # Test AffinityPropagation.predict
+    af = AffinityPropagation(affinity="euclidean")
+    labels = af.fit_predict(X)
+    labels2 = af.predict(X)
+    assert_array_equal(labels, labels2)
+
+
+def test_affinity_propagation_predict_error():
+    # Test exception in AffinityPropagation.predict
+    # Not fitted.
+    af = AffinityPropagation(affinity="euclidean")
+    with pytest.raises(ValueError):
+        af.predict(X)
+
+    # Predict not supported when affinity="precomputed".
+    S = np.dot(X, X.T)
+    af = AffinityPropagation(affinity="precomputed")
+    af.fit(S)
+    with pytest.raises(ValueError):
+        af.predict(X)
+
+
+def test_affinity_propagation_fit_non_convergence():
+    # In case of non-convergence of affinity_propagation(), the cluster
+    # centers should be an empty array and training samples should be labelled
+    # as noise (-1)
+    X = np.array([[0, 0], [1, 1], [-2, -2]])
+
+    # Force non-convergence by allowing only a single iteration
+    af = AffinityPropagation(preference=-10, max_iter=1)
+
+    assert_warns(ConvergenceWarning, af.fit, X)
+    assert_array_equal(np.empty((0, 2)), af.cluster_centers_)
+    assert_array_equal(np.array([-1, -1, -1]), af.labels_)
+
+
+def test_affinity_propagation_equal_mutual_similarities():
+    X = np.array([[-1, 1], [1, -1]])
+    S = -euclidean_distances(X, squared=True)
+
+    # setting preference > similarity
+    cluster_center_indices, labels = assert_warns_message(
+        UserWarning, "mutually equal", affinity_propagation, S, preference=0)
+
+    # expect every sample to become an exemplar
+    assert_array_equal([0, 1], cluster_center_indices)
+    assert_array_equal([0, 1], labels)
+
+    # setting preference < similarity
+    cluster_center_indices, labels = assert_warns_message(
+        UserWarning, "mutually equal", affinity_propagation, S, preference=-10)
+
+    # expect one cluster, with arbitrary (first) sample as exemplar
+    assert_array_equal([0], cluster_center_indices)
+    assert_array_equal([0, 0], labels)
+
+    # setting different preferences
+    cluster_center_indices, labels = assert_no_warnings(
+        affinity_propagation, S, preference=[-20, -10])
+
+    # expect one cluster, with highest-preference sample as exemplar
+    assert_array_equal([1], cluster_center_indices)
+    assert_array_equal([0, 0], labels)
+
+
+def test_affinity_propagation_predict_non_convergence():
+    # In case of non-convergence of affinity_propagation(), the cluster
+    # centers should be an empty array
+    X = np.array([[0, 0], [1, 1], [-2, -2]])
+
+    # Force non-convergence by allowing only a single iteration
+    af = assert_warns(ConvergenceWarning,
+                      AffinityPropagation(preference=-10, max_iter=1).fit, X)
+
+    # At prediction time, consider new samples as noise since there are no
+    # clusters
+    to_predict = np.array([[2, 2], [3, 3], [4, 4]])
+    y = assert_warns(ConvergenceWarning, af.predict, to_predict)
+    assert_array_equal(np.array([-1, -1, -1]), y)
+
+
+def test_equal_similarities_and_preferences():
+    # Unequal distances
+    X = np.array([[0, 0], [1, 1], [-2, -2]])
+    S = -euclidean_distances(X, squared=True)
+
+    assert not _equal_similarities_and_preferences(S, np.array(0))
+    assert not _equal_similarities_and_preferences(S, np.array([0, 0]))
+    assert not _equal_similarities_and_preferences(S, np.array([0, 1]))
+
+    # Equal distances
+    X = np.array([[0, 0], [1, 1]])
+    S = -euclidean_distances(X, squared=True)
+
+    # Different preferences
+    assert not _equal_similarities_and_preferences(S, np.array([0, 1]))
+
+    # Same preferences
+    assert _equal_similarities_and_preferences(S, np.array([0, 0]))
+    assert _equal_similarities_and_preferences(S, np.array(0))
+
+
+@pytest.mark.parametrize('centers', [csr_matrix(np.zeros((1, 10))),
+                                     np.zeros((1, 10))])
+def test_affinity_propagation_convergence_warning_dense_sparse(centers):
+    """Non-regression, see #13334"""
+    rng = np.random.RandomState(42)
+    X = rng.rand(40, 10)
+    y = (4 * rng.rand(40)).astype(np.int)
+    ap = AffinityPropagation()
+    ap.fit(X, y)
+    ap.cluster_centers_ = centers
+    with pytest.warns(None) as record:
+        assert_array_equal(ap.predict(X),
+                           np.zeros(X.shape[0], dtype=int))
+    assert len(record) == 0
diff --git a/sklearn/cluster/tests/test_affinity_propagation.py b/sklearn/cluster/tests/test_affinity_propagation.py
index 98f0f79ee..2987fb7a9 100644
--- a/sklearn/cluster/tests/test_affinity_propagation.py
+++ b/sklearn/cluster/tests/test_affinity_propagation.py
@@ -187,3 +187,21 @@ def test_affinity_propagation_convergence_warning_dense_sparse(centers):
         assert_array_equal(ap.predict(X),
                            np.zeros(X.shape[0], dtype=int))
     assert len(record) == 0
+
+
+def test_affinity_propagation_non_convergence():
+    # Test that AffinityPropagation returns empty cluster centers and -1 labels
+    # when it doesn't converge
+    X = np.array([[0, 0], [1, 1], [-2, -2]])
+    af = AffinityPropagation(preference=-10, max_iter=2)
+    with pytest.warns(ConvergenceWarning):
+        af.fit(X)
+    assert_array_equal(af.cluster_centers_indices_, [])
+    assert_array_equal(af.labels_, [-1, -1, -1])
+    assert af.cluster_centers_.shape == (0, 2)
+    
+    # Test predict method when there are no cluster centers
+    X_new = np.array([[0, 1], [1, 0]])
+    with pytest.warns(ConvergenceWarning):
+        labels = af.predict(X_new)
+    assert_array_equal(labels, [-1, -1])
